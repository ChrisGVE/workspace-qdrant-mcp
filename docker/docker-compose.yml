# Production Docker Compose for Workspace Qdrant MCP
# Complete stack with Qdrant, Redis, monitoring, and security
version: '3.8'

services:
  # Main application server
  workspace-qdrant-mcp:
    build:
      context: ..
      dockerfile: docker/Dockerfile
      target: production
      args:
        BUILDKIT_INLINE_CACHE: 1
    image: workspace-qdrant-mcp:${VERSION:-latest}
    container_name: workspace-qdrant-mcp
    restart: unless-stopped
    ports:
      - "${WQM_PORT:-8000}:8000"
    environment:
      # Application configuration
      - WORKSPACE_QDRANT_HOST=0.0.0.0
      - WORKSPACE_QDRANT_PORT=8000
      - WORKSPACE_QDRANT_LOG_LEVEL=${LOG_LEVEL:-INFO}
      - WORKSPACE_QDRANT_DATA_DIR=/app/data
      - WORKSPACE_QDRANT_LOG_DIR=/app/logs
      
      # Qdrant connection
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_GRPC_PORT=6334
      - QDRANT_API_KEY=${QDRANT_API_KEY:-}
      
      # Redis connection
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-}
      
      # Security and performance
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:14268
      - PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus
      
    volumes:
      - workspace_data:/app/data
      - workspace_logs:/app/logs
      - workspace_tmp:/app/tmp
      # Configuration files
      - ./config:/app/config:ro
      # Optional: Mount workspace directory for document processing
      - ${WORKSPACE_DIR:-./workspace}:/workspace:ro
    networks:
      - workspace-network
      - monitoring
    depends_on:
      qdrant:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import requests; requests.get(\"http://localhost:8000/health\")' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - FOWNER
      - SETGID
      - SETUID
    read_only: false
    tmpfs:
      - /tmp:exec,nodev,nosuid,size=100m
    labels:
      - "com.workspace-qdrant-mcp.service=main"
      - "com.workspace-qdrant-mcp.version=${VERSION:-latest}"

  # Qdrant vector database
  qdrant:
    image: qdrant/qdrant:v1.7.3
    container_name: workspace-qdrant-qdrant
    restart: unless-stopped
    ports:
      - "${QDRANT_HTTP_PORT:-6333}:6333"
      - "${QDRANT_GRPC_PORT:-6334}:6334"
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=${QDRANT_LOG_LEVEL:-INFO}
      - QDRANT__STORAGE__SNAPSHOTS_PATH=/qdrant/snapshots
      - QDRANT__STORAGE__STORAGE_PATH=/qdrant/storage
      # Security configuration
      - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY:-}
      - QDRANT__SERVICE__ENABLE_CORS=true
      # Performance optimization
      - QDRANT__STORAGE__WAL_CAPACITY_MB=32
      - QDRANT__STORAGE__WAL_SEGMENTS_AHEAD=0
      - QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_THREADS=0
      - QDRANT__STORAGE__PERFORMANCE__MAX_OPTIMIZATION_THREADS=1
    volumes:
      - qdrant_storage:/qdrant/storage
      - qdrant_snapshots:/qdrant/snapshots
      - qdrant_logs:/qdrant/logs
    networks:
      - workspace-network
      - monitoring
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:6333/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 1G
    security_opt:
      - no-new-privileges:true
    labels:
      - "com.workspace-qdrant-mcp.service=qdrant"
      - "com.workspace-qdrant-mcp.component=database"

  # Redis for caching and session storage
  redis:
    image: redis:7.2-alpine
    container_name: workspace-qdrant-redis
    restart: unless-stopped
    ports:
      - "${REDIS_PORT:-6379}:6379"
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --auto-aof-rewrite-percentage 100
      --auto-aof-rewrite-min-size 64mb
      --maxmemory ${REDIS_MAX_MEMORY:-256mb}
      --maxmemory-policy allkeys-lru
      ${REDIS_PASSWORD:+--requirepass $REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    networks:
      - workspace-network
      - monitoring
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    security_opt:
      - no-new-privileges:true
    labels:
      - "com.workspace-qdrant-mcp.service=redis"
      - "com.workspace-qdrant-mcp.component=cache"

  # Nginx reverse proxy with SSL termination
  nginx:
    image: nginx:1.25-alpine
    container_name: workspace-qdrant-nginx
    restart: unless-stopped
    ports:
      - "${HTTP_PORT:-80}:80"
      - "${HTTPS_PORT:-443}:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_logs:/var/log/nginx
    networks:
      - workspace-network
      - monitoring
    depends_on:
      - workspace-qdrant-mcp
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 64M
    security_opt:
      - no-new-privileges:true
    labels:
      - "com.workspace-qdrant-mcp.service=nginx"
      - "com.workspace-qdrant-mcp.component=proxy"

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: workspace-qdrant-prometheus
    restart: unless-stopped
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--log.level=${PROMETHEUS_LOG_LEVEL:-info}'
    networks:
      - monitoring
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 256M
    labels:
      - "com.workspace-qdrant-mcp.service=prometheus"
      - "com.workspace-qdrant-mcp.component=monitoring"

  # Grafana for visualization
  grafana:
    image: grafana/grafana:10.2.0
    container_name: workspace-qdrant-grafana
    restart: unless-stopped
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ../monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ../monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-clock-panel
      - GF_LOG_LEVEL=${GRAFANA_LOG_LEVEL:-info}
    networks:
      - monitoring
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    labels:
      - "com.workspace-qdrant-mcp.service=grafana"
      - "com.workspace-qdrant-mcp.component=monitoring"

  # Jaeger for distributed tracing
  jaeger:
    image: jaegertracing/all-in-one:1.50
    container_name: workspace-qdrant-jaeger
    restart: unless-stopped
    ports:
      - "${JAEGER_UI_PORT:-16686}:16686"
      - "${JAEGER_COLLECTOR_PORT:-14268}:14268"
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
      - COLLECTOR_OTLP_ENABLED=true
      - LOG_LEVEL=${JAEGER_LOG_LEVEL:-info}
    volumes:
      - jaeger_data:/badger
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    labels:
      - "com.workspace-qdrant-mcp.service=jaeger"
      - "com.workspace-qdrant-mcp.component=tracing"

  # Log aggregation with Loki
  loki:
    image: grafana/loki:2.9.2
    container_name: workspace-qdrant-loki
    restart: unless-stopped
    ports:
      - "${LOKI_PORT:-3100}:3100"
    volumes:
      - loki_data:/loki
      - ../monitoring/loki:/etc/loki:ro
    command:
      - -config.file=/etc/loki/loki-config.yaml
    networks:
      - monitoring
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    labels:
      - "com.workspace-qdrant-mcp.service=loki"
      - "com.workspace-qdrant-mcp.component=logging"

volumes:
  # Application data
  workspace_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${WORKSPACE_DATA_DIR:-./data}
  workspace_logs:
    driver: local
  workspace_tmp:
    driver: local
  
  # Database volumes
  qdrant_storage:
    driver: local
  qdrant_snapshots:
    driver: local
  qdrant_logs:
    driver: local
  redis_data:
    driver: local
  
  # Monitoring volumes
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  jaeger_data:
    driver: local
  loki_data:
    driver: local
  
  # Infrastructure volumes
  nginx_logs:
    driver: local

networks:
  workspace-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16
          gateway: 172.21.0.1
    driver_opts:
      com.docker.network.bridge.name: workspace-qdrant
      com.docker.network.bridge.enable_ip_masquerade: "true"
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.bridge.host_binding_ipv4: "0.0.0.0"
  
  monitoring:
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.0.0/16
          gateway: 172.22.0.1
    driver_opts:
      com.docker.network.bridge.name: workspace-monitoring
      com.docker.network.bridge.enable_ip_masquerade: "true"