# Queue Processor Performance Benchmarks - Task 352.8

## Summary

Comprehensive performance benchmarks have been implemented for the queue processor to validate the 1000+ docs/min throughput target and identify performance bottlenecks.

## Implementation Details

### Files Created

1. **`src/rust/daemon/core/benches/queue_processor_bench.rs`** (479 lines)
   - Complete Criterion-based benchmark suite
   - 8 distinct benchmark groups covering all performance aspects
   - Mock components for isolated testing (no I/O dependencies)

2. **`src/rust/daemon/core/Cargo.toml`** (updated)
   - Added `queue_processor_bench` benchmark configuration
   - Added `async_tokio` feature to Criterion dependency
   - Configured for HTML report generation

### Benchmark Groups

#### 1. Throughput Benchmark (`throughput_benchmark`)
- **Purpose**: Validate 1000+ docs/min target
- **Tests**: 100, 500, 1000 document batches
- **Measurement**: Documents processed per minute
- **Config**: 30s measurement time, 50 samples

#### 2. Concurrent Processing Benchmark (`concurrent_processing_benchmark`)
- **Purpose**: Measure impact of batch sizes
- **Tests**: Batch sizes of 1, 5, 10, 20, 50
- **Processes**: 100 items per test with varying concurrency
- **Config**: 20s measurement time

#### 3. Queue Depth Benchmark (`queue_depth_benchmark`)
- **Purpose**: Verify no performance degradation with large queues
- **Tests**: Queue depths of 10, 100, 1000, 10000
- **Validates**: Dequeue performance scales linearly
- **Config**: 15s measurement time

#### 4. Operation Type Benchmark (`operation_type_benchmark`)
- **Purpose**: Compare performance across operation types
- **Tests**: ingest, update, delete operations
- **Processes**: 50 items per operation type
- **Config**: 15s measurement time

#### 5. Processing Time Benchmark (`processing_time_benchmark`)
- **Purpose**: Measure impact of document complexity
- **Tests**: 1, 5, 10, 20 chunks per document
- **Validates**: Processing time scales with complexity
- **Config**: 20s measurement time

#### 6. Memory Benchmark (`memory_benchmark`)
- **Purpose**: Ensure no memory accumulation
- **Tests**: 1000 document processing
- **Validates**: Memory usage stays bounded
- **Config**: 20s measurement time, 30 samples

#### 7. Error Recovery Benchmark (`error_recovery_benchmark`)
- **Purpose**: Measure overhead of error handling
- **Tests**: Mixed success/failure scenarios (10% failures)
- **Processes**: 100 items with varied operations
- **Config**: 15s measurement time

#### 8. Throughput Validation (`validate_throughput_target`)
- **Purpose**: Explicit validation of 1000 docs/min target
- **Tests**: Full 1000 document processing in 60 seconds
- **Reports**: Clear pass/fail against target
- **Config**: 60s measurement time, 20 samples

### Mock Architecture

The benchmarks use lightweight mock components to isolate queue processing logic from I/O:

```rust
struct MockDocumentProcessor {
    chunk_count: usize,  // Instant document extraction
}

struct MockEmbeddingGenerator {
    vector_size: usize,  // Deterministic instant embeddings
}

struct MockStorageClient {
    collections: Arc<Mutex<HashMap<String, Vec<MockPoint>>>>,
    // In-memory instant storage
}
```

**Benefits:**
- No external dependencies (Qdrant, file system)
- Fast execution (< 5 minutes total)
- Reproducible results
- Tests pure queue logic

### Running the Benchmarks

#### Prerequisites
The core library must compile successfully. Currently blocked by:
- `unresolved import: crate::processing::PipelineStats`
- Platform-specific imports (`fsevents_sys`, `kqueue`)
- SQLite row access method issues

#### Once Fixed, Run:

```bash
# Run all queue processor benchmarks
cd src/rust/daemon/core
cargo bench --bench queue_processor_bench

# Run specific benchmark group
cargo bench --bench queue_processor_bench -- throughput

# Run with different sample size
cargo bench --bench queue_processor_bench -- --sample-size 100

# Generate detailed reports
cargo bench --bench queue_processor_bench -- --verbose
```

### Interpreting Results

#### HTML Reports
Located in: `target/criterion/queue_processor_bench/*/report/index.html`

Reports include:
- Mean execution time
- Standard deviation
- Throughput (elements/second)
- Performance over time graphs
- Statistical analysis

#### Console Output
Example:
```
üìä Throughput: 1,234.5 docs/min
‚úÖ Target MET (>= 1000 docs/min)

throughput/process_docs/100   time: [12.3 ms 12.5 ms 12.7 ms]
                               thrpt: [7,874 elem/s 8,000 elem/s 8,130 elem/s]
```

#### Performance Targets

| Metric | Target | Validation |
|--------|--------|------------|
| Throughput | >= 1000 docs/min | `validate_throughput_target` |
| Avg Processing Time | < 60ms/doc | `processing_time_benchmark` |
| Queue Depth Impact | O(1) dequeue | `queue_depth_benchmark` |
| Memory Growth | Bounded | `memory_benchmark` |
| Batch Scaling | Linear speedup | `concurrent_processing_benchmark` |

### Expected Performance Characteristics

Based on mock implementation (real performance will vary):

1. **Throughput**: Should easily exceed 1000 docs/min with mocks
2. **Batch Size**: Optimal around 10-20 items per batch
3. **Queue Depth**: Minimal impact up to 10,000 items
4. **Operation Types**: Delete < Ingest < Update (complexity order)
5. **Memory**: Constant memory per batch, not per total queue size

### Bottleneck Identification

The benchmarks will reveal:

1. **If throughput < 1000 docs/min**:
   - Check `concurrent_processing_benchmark` for optimal batch size
   - Review `processing_time_benchmark` for chunking overhead
   - Examine `operation_type_benchmark` for slow operations

2. **If queue depth causes degradation**:
   - `queue_depth_benchmark` will show at what size
   - May need to optimize SQL queries or indexing

3. **If memory grows unbounded**:
   - `memory_benchmark` will demonstrate leak
   - Check for retained references in processing loop

### Next Steps

1. **Fix Core Library Compilation**:
   - Resolve `PipelineStats` import
   - Fix platform-specific file watching imports
   - Correct SQLite row access methods

2. **Run Baseline Benchmarks**:
   ```bash
   cargo bench --bench queue_processor_bench -- --save-baseline initial
   ```

3. **Compare After Optimizations**:
   ```bash
   cargo bench --bench queue_processor_bench -- --baseline initial
   ```

4. **Profile If Needed**:
   ```bash
   cargo flamegraph --bench queue_processor_bench
   ```

5. **Document Results**:
   - Add benchmark results to project documentation
   - Create performance regression tests
   - Set up CI/CD benchmark tracking

### Integration with Real Components

When testing with real components (not mocks):

1. **Enable Real Embeddings**:
   - Replace `MockEmbeddingGenerator` with `EmbeddingGenerator`
   - Measure model loading overhead separately
   - Expect 10-50ms per embedding

2. **Enable Real Storage**:
   - Use testcontainers for Qdrant
   - Measure network latency
   - Expect 5-20ms per batch insert

3. **Enable Real File Processing**:
   - Create actual test files
   - Measure parsing overhead
   - Expect 1-100ms per file depending on size

### Criterion Configuration

Current settings in benchmark:
```rust
Criterion::default()
    .sample_size(50)              // Statistical significance
    .measurement_time(Duration::from_secs(20))  // Per benchmark
    .warm_up_time(Duration::from_secs(3))       // JIT warmup
```

Adjust based on:
- Increase `sample_size` for more precision (slower)
- Increase `measurement_time` for stable results
- Decrease for faster development iteration

### Benchmark Maintenance

**When to Re-run:**
- After queue processor changes
- After dependency updates
- Before performance-critical releases
- When investigating performance issues

**What to Track:**
- Throughput trends over versions
- Regression detection (> 10% slowdown)
- Memory usage patterns
- Error recovery overhead

### Success Criteria

‚úÖ **Implementation Complete:**
- Comprehensive benchmark suite created
- 8 distinct performance aspects covered
- Mock components for isolated testing
- HTML report generation configured
- Clear throughput validation

‚è≥ **Awaiting Library Fix:**
- Core library compilation errors
- Platform-specific imports
- SQLite access methods

üéØ **Once Fixed:**
- Run benchmarks to validate 1000+ docs/min
- Generate baseline performance report
- Integrate into CI/CD pipeline
- Document performance characteristics

## Technical Details

### Mock Implementation Philosophy

The benchmarks use **instant mocks** rather than real components because:

1. **Isolation**: Tests queue logic independently
2. **Speed**: No I/O = sub-second benchmarks
3. **Reproducibility**: Deterministic results
4. **Focus**: Identifies queue-specific bottlenecks

Real component testing should be done separately in integration tests.

### Async Benchmark Pattern

Uses Criterion's async support:
```rust
b.to_async(&rt).iter(|| async {
    // Async benchmark code
});
```

This allows benchmarking the actual async queue processing logic.

### Throughput Calculation

```rust
group.throughput(Throughput::Elements(100));
```

Criterion automatically calculates and reports:
- Elements/second
- Elements/minute (via multiplication)
- Statistical confidence intervals

### Memory Measurement

While Criterion doesn't directly measure memory, the `memory_benchmark` tests for:
- Process stability over 1000 items
- No panics or crashes
- Completion time (memory pressure ‚Üí slower)

For detailed memory profiling, use:
```bash
cargo bench --bench queue_processor_bench
# Then analyze with heaptrack or valgrind
```

## Conclusion

The queue processor benchmarks are **fully implemented and ready to run** once the core library compilation issues are resolved. The benchmark suite provides comprehensive performance validation covering throughput, concurrency, queue depth, operation types, processing time, memory usage, and error recovery.

The implementation follows Rust best practices:
- Zero-cost abstractions via mocks
- Proper async benchmarking
- Statistical rigor with Criterion
- Clear performance targets
- Actionable bottleneck identification

**Files:**
- `/src/rust/daemon/core/benches/queue_processor_bench.rs` (479 lines)
- `/src/rust/daemon/core/Cargo.toml` (updated with bench config)

**Next Action**: Fix core library compilation errors, then run benchmarks to validate performance targets.
