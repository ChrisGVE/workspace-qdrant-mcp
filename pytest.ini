[pytest]
# Pytest configuration for optimal parallel execution and test organization

# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Minimum Python version
minversion = 7.0

# Output options
addopts =
    # Verbose output with detailed test information
    -v
    # Show extra test summary info for all test outcomes
    -ra
    # Show local variables in tracebacks
    --showlocals
    # Strict markers - all markers must be registered
    --strict-markers
    # Strict config - warn about invalid config
    --strict-config
    # Show durations of slowest N tests
    --durations=10
    # Show warnings
    -W default
    # Coverage options (enable with --cov flag)
    # --cov-report=term-missing:skip-covered
    # --cov-report=html
    # --cov-report=xml

# Parallel execution configuration (pytest-xdist)
# These are defaults that can be overridden via CLI
# -n auto: use all available CPU cores
# -n 4: use 4 workers
# --dist loadgroup: distribute by xdist_group marker for related tests
# --dist loadscope: distribute by module/class/function scope
# --dist loadfile: distribute by file (default)

# Test markers for organization and selective execution
markers =
    # Performance and benchmarking
    slow: marks tests as slow (deselect with '-m "not slow"')
    slow_functional: marks slow functional tests
    benchmark: marks tests for benchmarking (select with '-m benchmark')
    performance: marks performance-critical tests
    regression: marks regression tests

    # Stress and load testing
    stress: marks stress tests (high load scenarios)
    load: marks load tests (concurrent operations)

    # Integration levels
    unit: marks unit tests (fast, isolated)
    integration: marks integration tests (multiple components)
    e2e: marks end-to-end tests (full system)
    functional: marks functional tests (feature validation)

    # Component-specific
    mcp: marks MCP server tests
    mcp_server: marks MCP server-specific tests
    fastmcp: marks FastMCP framework tests
    cli: marks CLI tests
    daemon: marks daemon tests
    qdrant: marks tests requiring Qdrant

    # Platform-specific
    linux_only: marks tests that only run on Linux
    macos_only: marks tests that only run on macOS
    windows_only: marks tests that only run on Windows

    # Resource requirements
    requires_qdrant: marks tests requiring Qdrant server
    requires_git: marks tests requiring Git repository
    requires_rust: marks tests requiring Rust daemon
    requires_daemon: marks tests requiring running daemon (gRPC)
    requires_network: marks tests requiring network access
    network_required: marks tests requiring network access (alias)
    live_api: marks tests requiring live LLM API access

    # Special execution
    serial: marks tests that must run serially (not parallel)
    flaky: marks tests that are occasionally flaky
    xfail_strict: marks xfail tests that must fail

    # Test categories
    edge: marks edge case tests
    nominal: marks nominal/happy path tests
    extended: marks extended duration tests
    security: marks security-related tests
    smoke: marks smoke tests (quick validation)

    # Test workflows and scenarios
    api_testing: marks API testing scenarios
    cli_workflow: marks CLI workflow tests
    cli_integration: marks CLI integration tests
    cross_platform: marks cross-platform compatibility tests
    daemon_lifecycle: marks daemon lifecycle tests
    daemon_integration: marks daemon integration tests
    error_recovery: marks error recovery tests
    mcp_integration: marks MCP integration tests
    qdrant_integration: marks Qdrant integration tests
    real_world_simulation: marks real-world simulation tests
    test_infrastructure: marks infrastructure testing
    memory_rules: marks memory rules tests

    # Testing frameworks and tools
    testcontainers: marks tests using testcontainers
    playwright: marks tests using playwright

    # Test groups for xdist loadgroup distribution
    xdist_group: marks tests for grouped execution

# Test execution order optimization
# Run fast tests first for quick feedback
console_output_style = progress

# Timeout for tests (prevents hanging)
timeout = 300
timeout_method = thread

# Asyncio configuration
asyncio_mode = auto

# Coverage configuration
[coverage:run]
source = src/
omit =
    */tests/*
    */test_*.py
    */__pycache__/*
    */site-packages/*
    */venv/*
    */.venv/*

[coverage:report]
# Fail if coverage is below threshold
fail_under = 80
precision = 2
show_missing = True
skip_covered = False
skip_empty = True

# Exclude lines from coverage
exclude_lines =
    pragma: no cover
    def __repr__
    def __str__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod
    @abc.abstractmethod
    pass
    \.\.\.

[coverage:html]
directory = htmlcov

[coverage:xml]
output = coverage.xml

# Logging configuration
log_cli = false
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Warnings configuration
filterwarnings =
    # Convert warnings to errors for stricter testing
    error
    # Ignore specific warnings from dependencies
    ignore::DeprecationWarning:pytest_asyncio
    ignore::DeprecationWarning:aiohttp
    ignore::ResourceWarning
    # Allow specific warnings we're aware of
    default::PendingDeprecationWarning

# Test collection optimization
norecursedirs =
    .git
    .tox
    .eggs
    *.egg
    dist
    build
    __pycache__
    .pytest_cache
    .mypy_cache
    .ruff_cache
    node_modules
    venv
    .venv
    htmlcov
    .coverage
    .taskmaster
    tmp
    _quarantine

# Pytest-xdist optimal configuration hints
# Use these CLI flags for different scenarios:
#
# Fast unit tests (auto-parallelization):
#   pytest tests/unit/ -n auto --dist loadscope
#
# Integration tests (grouped execution):
#   pytest tests/integration/ -n 4 --dist loadgroup
#
# E2E tests (file-level distribution):
#   pytest tests/e2e/ -n 2 --dist loadfile
#
# Stress tests (serialized execution):
#   pytest -m stress -n 0
#
# Quick feedback (fail fast):
#   pytest -n auto --maxfail=3 -x
#
# Specific test group:
#   pytest -m "integration and not slow" -n auto

# Test sharding for CI (run subset of tests)
# Shard 1 of 4: pytest -n auto --splits 4 --group 1
# Shard 2 of 4: pytest -n auto --splits 4 --group 2
# etc.
