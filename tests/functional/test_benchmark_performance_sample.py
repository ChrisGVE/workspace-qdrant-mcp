"""
Sample performance benchmark tests for workspace-qdrant-mcp.

These tests demonstrate benchmark testing patterns for:
- Vector search performance baselines
- API response time benchmarks
- Memory usage analysis
- Regression detection
"""

import pytest
import numpy as np
from typing import List, Dict, Any
import time
import asyncio
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import httpx
import json
from pathlib import Path
import psutil
import os


@pytest.mark.benchmark
@pytest.mark.performance
class TestVectorSearchBenchmarks:
    """Benchmark vector search operations for performance baselines."""

    @pytest.fixture(scope="class")
    def benchmark_collection(self, qdrant_client: QdrantClient) -> str:
        """Create a benchmark collection with standardized data."""
        collection_name = "benchmark_collection"

        qdrant_client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=384, distance=Distance.COSINE)
        )

        # Insert benchmark dataset
        vectors = []
        for i in range(1000):
            vector = np.random.random(384).astype(np.float32)
            point = PointStruct(
                id=i,
                vector=vector.tolist(),
                payload={
                    "category": f"cat_{i % 10}",
                    "subcategory": f"sub_{i % 50}",
                    "index": i,
                    "score": float(i % 100) / 100.0
                }
            )
            vectors.append(point)

        # Insert in batches for better performance
        batch_size = 100
        for i in range(0, len(vectors), batch_size):
            batch = vectors[i:i + batch_size]
            qdrant_client.upsert(collection_name=collection_name, points=batch)

        yield collection_name

        # Cleanup
        qdrant_client.delete_collection(collection_name)

    def test_search_performance_baseline(self, benchmark, qdrant_client: QdrantClient, benchmark_collection: str):
        """Benchmark basic vector search performance."""
        search_vector = np.random.random(384).astype(np.float32)

        def search_operation():
            return qdrant_client.search(
                collection_name=benchmark_collection,
                query_vector=search_vector.tolist(),
                limit=10
            )

        # Benchmark the search operation
        result = benchmark(search_operation)

        # Validate results
        assert len(result) == 10
        assert all(isinstance(r.score, float) for r in result)
        assert all(r.score >= 0 for r in result)

        # Performance expectations (baseline)
        stats = benchmark.stats
        assert stats.min < 0.1, f"Search too slow: {stats.min:.3f}s"
        assert stats.mean < 0.05, f"Average search too slow: {stats.mean:.3f}s"

    def test_filtered_search_performance(self, benchmark, qdrant_client: QdrantClient, benchmark_collection: str):
        """Benchmark filtered vector search performance."""
        search_vector = np.random.random(384).astype(np.float32)

        def filtered_search():
            return qdrant_client.search(
                collection_name=benchmark_collection,
                query_vector=search_vector.tolist(),
                query_filter={
                    "must": [
                        {"key": "category", "match": {"value": "cat_1"}},
                        {"key": "score", "range": {"gte": 0.5}}
                    ]
                },
                limit=10
            )

        result = benchmark(filtered_search)

        # Validate filtered results
        for point in result:
            assert point.payload["category"] == "cat_1"
            assert point.payload["score"] >= 0.5

        # Performance expectations (should be reasonable despite filtering)
        stats = benchmark.stats
        assert stats.mean < 0.2, f"Filtered search too slow: {stats.mean:.3f}s"

    def test_batch_insert_performance(self, benchmark, qdrant_client: QdrantClient):
        """Benchmark batch insert performance."""
        collection_name = "batch_benchmark"

        qdrant_client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=384, distance=Distance.COSINE)
        )

        try:
            # Create batch of vectors
            batch_size = 100
            vectors = []
            for i in range(batch_size):
                vector = np.random.random(384).astype(np.float32)
                point = PointStruct(
                    id=i,
                    vector=vector.tolist(),
                    payload={"index": i}
                )
                vectors.append(point)

            def batch_insert():
                return qdrant_client.upsert(
                    collection_name=collection_name,
                    points=vectors
                )

            result = benchmark(batch_insert)
            assert result.status == "completed"

            # Performance expectations
            stats = benchmark.stats
            vectors_per_second = batch_size / stats.mean
            assert vectors_per_second > 500, f"Insert rate too slow: {vectors_per_second:.1f} vectors/sec"

        finally:
            qdrant_client.delete_collection(collection_name)

    @pytest.mark.parametrize("search_limit", [1, 10, 50, 100])
    def test_search_limit_scaling(self, benchmark, qdrant_client: QdrantClient, benchmark_collection: str, search_limit: int):
        """Benchmark how search performance scales with result limit."""
        search_vector = np.random.random(384).astype(np.float32)

        def search_with_limit():
            return qdrant_client.search(
                collection_name=benchmark_collection,
                query_vector=search_vector.tolist(),
                limit=search_limit
            )

        result = benchmark(search_with_limit)
        assert len(result) == min(search_limit, 1000)  # Collection has 1000 vectors

        # Tag benchmark with limit for comparison
        benchmark.extra_info["search_limit"] = search_limit


@pytest.mark.benchmark
@pytest.mark.api_testing
class TestAPIResponseBenchmarks:
    """Benchmark API response times for various operations."""

    @pytest.fixture(scope="class")
    def http_client(self) -> httpx.AsyncClient:
        """Provide HTTP client for API benchmarking."""
        return httpx.AsyncClient(
            base_url="http://localhost:8000",
            timeout=30.0
        )

    def test_mcp_initialization_benchmark(self, benchmark, http_client: httpx.AsyncClient):
        """Benchmark MCP initialization handshake performance."""
        initialize_request = {
            "jsonrpc": "2.0",
            "id": 1,
            "method": "initialize",
            "params": {
                "protocolVersion": "2024-11-05",
                "capabilities": {"roots": {"listChanged": True}},
                "clientInfo": {"name": "benchmark-client", "version": "1.0.0"}
            }
        }

        async def mcp_initialize():
            response = await http_client.post("/", json=initialize_request)
            return response.json()

        # Run async benchmark
        result = benchmark(asyncio.run, mcp_initialize())

        # Validate response
        assert "result" in result
        assert "protocolVersion" in result["result"]

        # Performance expectations
        stats = benchmark.stats
        assert stats.mean < 0.5, f"MCP init too slow: {stats.mean:.3f}s"

    def test_workspace_status_benchmark(self, benchmark, http_client: httpx.AsyncClient):
        """Benchmark workspace status API performance."""
        status_request = {
            "jsonrpc": "2.0",
            "id": 2,
            "method": "workspace_status",
            "params": {}
        }

        async def workspace_status():
            response = await http_client.post("/", json=status_request)
            return response.json()

        result = benchmark(asyncio.run, workspace_status())

        # Validate response structure
        assert "result" in result
        assert "content" in result["result"]

        # Performance expectations
        stats = benchmark.stats
        assert stats.mean < 1.0, f"Workspace status too slow: {stats.mean:.3f}s"

    @pytest.mark.parametrize("query_length", [10, 50, 100, 500])
    def test_search_query_scaling_benchmark(self, benchmark, http_client: httpx.AsyncClient, query_length: int):
        """Benchmark search performance with varying query lengths."""
        # Generate query of specified length
        query = "test search " * (query_length // 12 + 1)
        query = query[:query_length]

        search_request = {
            "jsonrpc": "2.0",
            "id": 3,
            "method": "search_workspace",
            "params": {
                "query": query,
                "limit": 10
            }
        }

        async def search_operation():
            response = await http_client.post("/", json=search_request)
            return response.json()

        # Note: This might return an error if method not implemented
        # The benchmark will still measure the response time
        result = benchmark(asyncio.run, search_operation())

        # Tag with query length for analysis
        benchmark.extra_info["query_length"] = query_length

    def test_concurrent_request_benchmark(self, benchmark, http_client: httpx.AsyncClient):
        """Benchmark concurrent API request handling."""
        status_request = {
            "jsonrpc": "2.0",
            "id": 1,
            "method": "workspace_status",
            "params": {}
        }

        async def concurrent_requests():
            # Launch 5 concurrent requests
            tasks = []
            for i in range(5):
                request = {**status_request, "id": i + 1}
                task = http_client.post("/", json=request)
                tasks.append(task)

            responses = await asyncio.gather(*tasks)
            return [r.json() for r in responses]

        results = benchmark(asyncio.run, concurrent_requests())

        # Validate all requests succeeded
        assert len(results) == 5
        for result in results:
            assert "result" in result or "error" in result  # Either success or handled error

        # Performance expectations
        stats = benchmark.stats
        assert stats.mean < 2.0, f"Concurrent requests too slow: {stats.mean:.3f}s"


@pytest.mark.benchmark
@pytest.mark.performance
class TestMemoryUsageBenchmarks:
    """Benchmark memory usage patterns."""

    def test_memory_usage_during_vector_operations(self, benchmark, qdrant_client: QdrantClient):
        """Benchmark memory usage during vector operations."""
        collection_name = "memory_benchmark"

        qdrant_client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=384, distance=Distance.COSINE)
        )

        try:
            def memory_intensive_operation():
                # Track memory before operation
                process = psutil.Process(os.getpid())
                initial_memory = process.memory_info().rss

                # Perform memory-intensive operation
                vectors = []
                for i in range(500):  # Large batch
                    vector = np.random.random(384).astype(np.float32)
                    point = PointStruct(
                        id=i,
                        vector=vector.tolist(),
                        payload={"index": i, "data": "x" * 100}  # Some payload data
                    )
                    vectors.append(point)

                # Insert vectors
                result = qdrant_client.upsert(collection_name=collection_name, points=vectors)

                # Measure memory after
                final_memory = process.memory_info().rss
                memory_delta = final_memory - initial_memory

                return {
                    "operation_result": result,
                    "memory_delta_mb": memory_delta / 1024 / 1024,
                    "initial_memory_mb": initial_memory / 1024 / 1024,
                    "final_memory_mb": final_memory / 1024 / 1024
                }

            result = benchmark(memory_intensive_operation)

            # Validate operation succeeded
            assert result["operation_result"].status == "completed"

            # Memory usage should be reasonable
            assert result["memory_delta_mb"] < 100, f"Excessive memory usage: {result['memory_delta_mb']:.1f}MB"

            # Store memory metrics in benchmark
            benchmark.extra_info.update({
                "memory_delta_mb": result["memory_delta_mb"],
                "initial_memory_mb": result["initial_memory_mb"],
                "final_memory_mb": result["final_memory_mb"]
            })

        finally:
            qdrant_client.delete_collection(collection_name)

    def test_search_memory_efficiency(self, benchmark, qdrant_client: QdrantClient, benchmark_collection: str):
        """Benchmark memory efficiency of search operations."""
        search_vector = np.random.random(384).astype(np.float32)

        def memory_tracked_search():
            process = psutil.Process(os.getpid())
            initial_memory = process.memory_info().rss

            # Perform multiple searches to stress test
            results = []
            for _ in range(10):
                result = qdrant_client.search(
                    collection_name=benchmark_collection,
                    query_vector=search_vector.tolist(),
                    limit=50
                )
                results.append(result)

            final_memory = process.memory_info().rss
            memory_delta = final_memory - initial_memory

            return {
                "search_results": len(results),
                "total_results": sum(len(r) for r in results),
                "memory_delta_mb": memory_delta / 1024 / 1024
            }

        result = benchmark(memory_tracked_search)

        # Validate searches completed
        assert result["search_results"] == 10
        assert result["total_results"] == 500  # 10 searches × 50 results

        # Memory growth should be minimal for searches
        assert result["memory_delta_mb"] < 10, f"Search memory leak: {result['memory_delta_mb']:.1f}MB"

        benchmark.extra_info["memory_delta_mb"] = result["memory_delta_mb"]


@pytest.mark.benchmark
@pytest.mark.regression
class TestPerformanceRegressionDetection:
    """Tests for detecting performance regressions."""

    def test_baseline_performance_metrics(self, benchmark, qdrant_client: QdrantClient, benchmark_collection: str):
        """Establish baseline performance metrics for regression detection."""
        search_vector = np.random.random(384).astype(np.float32)

        def comprehensive_operation():
            # Search operation
            search_start = time.time()
            search_results = qdrant_client.search(
                collection_name=benchmark_collection,
                query_vector=search_vector.tolist(),
                limit=20
            )
            search_time = time.time() - search_start

            # Count operation
            count_start = time.time()
            count_result = qdrant_client.count(collection_name=benchmark_collection)
            count_time = time.time() - count_start

            # Retrieve operation
            retrieve_start = time.time()
            retrieve_results = qdrant_client.retrieve(
                collection_name=benchmark_collection,
                ids=[0, 1, 2, 3, 4]
            )
            retrieve_time = time.time() - retrieve_start

            return {
                "search_time": search_time,
                "search_results": len(search_results),
                "count_time": count_time,
                "count_result": count_result.count,
                "retrieve_time": retrieve_time,
                "retrieve_results": len(retrieve_results)
            }

        result = benchmark(comprehensive_operation)

        # Validate operations
        assert result["search_results"] == 20
        assert result["count_result"] == 1000
        assert result["retrieve_results"] == 5

        # Store detailed metrics for regression analysis
        benchmark.extra_info.update({
            "search_time": result["search_time"],
            "count_time": result["count_time"],
            "retrieve_time": result["retrieve_time"],
            "total_operation_time": result["search_time"] + result["count_time"] + result["retrieve_time"]
        })

        # Baseline expectations (these become regression thresholds)
        assert result["search_time"] < 0.1, "Search baseline exceeded"
        assert result["count_time"] < 0.05, "Count baseline exceeded"
        assert result["retrieve_time"] < 0.05, "Retrieve baseline exceeded"

    @pytest.mark.slow_functional
    def test_sustained_performance_stability(self, benchmark, qdrant_client: QdrantClient, benchmark_collection: str):
        """Test performance stability over sustained operations."""
        search_vector = np.random.random(384).astype(np.float32)

        def sustained_operations():
            times = []

            # Perform 50 operations and track timing
            for i in range(50):
                start_time = time.time()

                qdrant_client.search(
                    collection_name=benchmark_collection,
                    query_vector=search_vector.tolist(),
                    limit=10
                )

                operation_time = time.time() - start_time
                times.append(operation_time)

            return {
                "min_time": min(times),
                "max_time": max(times),
                "avg_time": sum(times) / len(times),
                "time_variance": np.var(times),
                "operations": len(times)
            }

        result = benchmark(sustained_operations)

        # Validate sustained performance
        assert result["operations"] == 50
        assert result["max_time"] < result["avg_time"] * 3, "Performance too variable"
        assert result["time_variance"] < 0.01, "Performance too unstable"

        # Store variance metrics
        benchmark.extra_info.update({
            "min_time": result["min_time"],
            "max_time": result["max_time"],
            "avg_time": result["avg_time"],
            "time_variance": result["time_variance"]
        })


# Benchmark configuration and utilities
@pytest.fixture(autouse=True)
def benchmark_config(benchmark):
    """Configure benchmark settings."""
    # Only run 5 rounds to reduce test time
    benchmark.rounds = 5
    benchmark.min_rounds = 3
    benchmark.max_time = 10.0  # Maximum 10 seconds per benchmark
    benchmark.min_time = 0.1   # Minimum 0.1 seconds per benchmark


def pytest_benchmark_generate_json(config, benchmarks):
    """Generate custom benchmark report with performance metrics."""
    report_data = {
        "test_run": {
            "timestamp": time.time(),
            "test_count": len(benchmarks),
            "framework_version": "workspace-qdrant-mcp-benchmarks-1.0"
        },
        "benchmarks": []
    }

    for benchmark in benchmarks:
        benchmark_data = {
            "name": benchmark["name"],
            "stats": benchmark["stats"],
            "extra_info": benchmark.get("extra_info", {})
        }
        report_data["benchmarks"].append(benchmark_data)

    # Save to file for regression analysis
    report_file = Path("benchmark_report.json")
    with open(report_file, "w") as f:
        json.dump(report_data, f, indent=2)

    return report_data