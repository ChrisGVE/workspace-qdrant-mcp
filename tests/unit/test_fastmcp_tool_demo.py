"""
FastMCP Tool Demonstration Test for Task 241.1.

This test demonstrates the FastMCP in-memory testing infrastructure
working with actual MCP tools from the workspace-qdrant-mcp server.

Shows:
    - Real tool invocation using the testing infrastructure
    - Error handling and response validation
    - Performance measurement capabilities
    - Integration with existing MCP tools
"""

import pytest
from tests.utils.fastmcp_test_infrastructure import (
    FastMCPTestClient,
    MCPProtocolTester,
    fastmcp_test_environment
)


class TestFastMCPToolDemonstration:
    """Demonstrate FastMCP testing infrastructure with real tools."""

    @pytest.mark.fastmcp
    async def test_workspace_status_tool_demonstration(self, fastmcp_test_client):
        """Demonstrate workspace_status tool using FastMCP infrastructure."""
        client = fastmcp_test_client

        # Call the workspace_status tool
        result = await client.call_tool("workspace_status", {})

        # Demonstrate comprehensive result analysis
        print(f"\\nüìä FastMCP Tool Demonstration Results:")
        print(f"Tool: {result.tool_name}")
        print(f"Success: {result.success}")
        print(f"Execution Time: {result.execution_time_ms:.2f}ms")

        if result.success:
            print(f"Response Type: {type(result.response)}")
            print(f"Response Size: {len(str(result.response))} chars")

            # Validate response structure
            if isinstance(result.response, dict):
                print(f"Response Keys: {list(result.response.keys())}")

                # Check for expected workspace status fields
                status_fields = ["connected", "collections", "current_project"]
                found_fields = [field for field in status_fields if field in result.response]
                print(f"Found Status Fields: {found_fields}")

            # Check protocol compliance
            if result.protocol_compliance:
                compliance_score = sum(result.protocol_compliance.values()) / len(result.protocol_compliance)
                print(f"Protocol Compliance: {compliance_score:.1%}")
                print(f"Compliance Details: {result.protocol_compliance}")

        else:
            print(f"Error: {result.error}")
            print(f"Error Type: {result.metadata.get('exception_type', 'Unknown')}")

        # Demonstrate that the infrastructure captures detailed metadata
        assert hasattr(result, 'metadata')
        assert 'timestamp' in result.metadata, f"Missing timestamp in metadata: {result.metadata}"
        assert result.execution_time_ms >= 0

        # The tool either succeeds or fails gracefully
        if result.success:
            assert result.response is not None
        else:
            assert result.error is not None
            # Demonstrate that even failures provide useful information
            print(f"Tool failed as expected (context issues are common in testing)")

        # This demonstrates the infrastructure works regardless of tool success/failure
        print(f"\n‚úÖ Infrastructure Test Passed: Captured detailed execution data")

    @pytest.mark.fastmcp
    async def test_search_tool_demonstration(self, fastmcp_test_client):
        """Demonstrate search tool with parameters."""
        client = fastmcp_test_client

        # Test search tool with parameters
        search_params = {
            "query": "test search",
            "limit": 5
        }

        result = await client.call_tool("search_workspace_tool", search_params)

        print(f"\\nüîç Search Tool Demonstration:")
        print(f"Parameters: {search_params}")
        print(f"Success: {result.success}")
        print(f"Execution Time: {result.execution_time_ms:.2f}ms")

        if result.success:
            print(f"Response: {result.response}")

            # If response has results structure, analyze it
            if isinstance(result.response, dict):
                if "results" in result.response:
                    results_count = len(result.response["results"]) if isinstance(result.response["results"], list) else 0
                    print(f"Results Count: {results_count}")

                if "total" in result.response:
                    print(f"Total Available: {result.response['total']}")

        else:
            print(f"Search Error: {result.error}")
            # This is acceptable - the search might fail due to missing collections etc.

        # Verify infrastructure captured the call correctly
        assert result.tool_name == "search_workspace_tool"
        assert result.parameters == search_params

        # Demonstrate infrastructure works regardless of tool outcome
        print(f"Infrastructure correctly captured call details regardless of success/failure")

    @pytest.mark.fastmcp
    async def test_multiple_tool_calls_demonstration(self, fastmcp_test_client):
        """Demonstrate multiple tool calls and call history tracking."""
        client = fastmcp_test_client

        # Clear history to start fresh
        client.clear_call_history()

        # Make multiple calls
        tools_to_test = [
            ("workspace_status", {}),
            ("list_workspace_collections", {}),
            ("search_workspace_tool", {"query": "demo", "limit": 3})
        ]

        print(f"\\nüìö Multiple Tool Calls Demonstration:")

        results = []
        for tool_name, params in tools_to_test:
            result = await client.call_tool(tool_name, params)
            results.append(result)

            print(f"  {tool_name}: {'‚úÖ Success' if result.success else '‚ùå Failed'} ({result.execution_time_ms:.1f}ms)")

        # Demonstrate call history tracking
        history = client.get_call_history()
        assert len(history) == len(tools_to_test)

        print(f"\\nCall History Summary:")
        for i, call in enumerate(history):
            print(f"  {i+1}. {call.tool_name} - {'Success' if call.success else 'Failed'}")

        # Calculate performance statistics
        execution_times = [r.execution_time_ms for r in results if r.success]
        if execution_times:
            avg_time = sum(execution_times) / len(execution_times)
            max_time = max(execution_times)
            min_time = min(execution_times)

            print(f"\\nPerformance Statistics:")
            print(f"  Average: {avg_time:.2f}ms")
            print(f"  Range: {min_time:.2f}ms - {max_time:.2f}ms")

            # Demonstrate that in-memory testing is fast (even failures are fast)
            assert avg_time < 1000.0, "In-memory calls should be sub-second"

        # Demonstrate success rate calculation
        success_rate = sum(1 for r in results if r.success) / len(results)
        print(f"  Success Rate: {success_rate:.1%}")

    @pytest.mark.fastmcp
    async def test_protocol_tester_demonstration(self, mcp_protocol_tester):
        """Demonstrate the MCP protocol tester capabilities."""
        tester = mcp_protocol_tester

        print(f"\\nüß™ MCP Protocol Tester Demonstration:")

        # Test tool registration
        registration_results = await tester.test_tool_registration()
        print(f"Tool Registration: {registration_results['success_rate']:.1%} success rate")
        print(f"Tools Found: {registration_results['total_tools']}")

        # Test tool invocation
        invocation_results = await tester.test_tool_invocation()
        print(f"Tool Invocation: {invocation_results['success_rate']:.1%} success rate")
        print(f"Average Execution: {invocation_results.get('average_execution_time_ms', 0):.2f}ms")

        # Demonstrate that protocol tester provides comprehensive analysis
        assert 'success_rate' in registration_results
        assert 'test_results' in registration_results
        assert 'success_rate' in invocation_results

    @pytest.mark.fastmcp
    async def test_comprehensive_testing_demonstration(self):
        """Demonstrate comprehensive testing capabilities."""
        from workspace_qdrant_mcp.server import app

        print(f"\\nüéØ Comprehensive Testing Demonstration:")

        async with fastmcp_test_environment(app, "comprehensive-demo") as (server, client):
            # Demonstrate server capabilities
            available_tools = server.get_available_tools()
            print(f"Available Tools: {len(available_tools)}")

            if available_tools:
                print(f"Sample Tools: {available_tools[:5]}")  # Show first 5

            # Demonstrate client capabilities
            call_count = 3
            successful_calls = 0

            for i in range(call_count):
                result = await client.call_tool("workspace_status", {})
                if result.success:
                    successful_calls += 1

            success_rate = successful_calls / call_count
            print(f"Repeated Calls: {successful_calls}/{call_count} successful ({success_rate:.1%})")

            # Demonstrate call history
            history = client.get_call_history()
            print(f"Call History: {len(history)} entries")

            if history:
                avg_time = sum(call.execution_time_ms for call in history) / len(history)
                print(f"Average Response Time: {avg_time:.2f}ms")

        print(f"\\n‚úÖ FastMCP In-Memory Testing Infrastructure Demonstration Complete!")
        print(f"   - Zero-latency server-client communication")
        print(f"   - Comprehensive tool invocation testing")
        print(f"   - Detailed performance and compliance metrics")
        print(f"   - Error handling and history tracking")

        # Test passes if we reach here without errors

        print(f"‚úÖ All demonstrations completed successfully!")