"""
Parser for pytest test results.

Supports multiple pytest output formats:
- JUnit XML (--junit-xml flag)
- pytest JSON (via pytest-json-report plugin)
"""

import json
import xml.etree.ElementTree as ET
from datetime import datetime
from pathlib import Path
from typing import Any, Optional, Union
from uuid import uuid4

from ..models import (
    PerformanceMetrics,
    TestCase,
    TestResult,
    TestRun,
    TestSource,
    TestStatus,
    TestSuite,
    TestType,
)
from .base import BaseParser


class PytestParser(BaseParser):
    """Parser for pytest test results (JUnit XML or JSON)."""

    def parse(self, source: str | Path | dict) -> TestRun:
        """
        Parse pytest results into TestRun.

        Args:
            source: Path to JUnit XML file, JSON file, or dict with pytest data

        Returns:
            TestRun object with pytest results
        """
        # If dict, treat as JSON format
        if isinstance(source, dict):
            return self._parse_json(source)

        path = self._ensure_path(source)

        # Determine format from file extension
        if path.suffix == ".xml":
            return self._parse_junit_xml(path)
        elif path.suffix == ".json":
            with open(path) as f:
                data = json.load(f)
            return self._parse_json(data)
        else:
            raise ValueError(f"Unsupported pytest result format: {path.suffix}")

    def _parse_junit_xml(self, path: Path) -> TestRun:
        """Parse JUnit XML format generated by pytest --junit-xml."""
        tree = ET.parse(path)
        root = tree.getroot()

        # Extract timestamp
        timestamp_str = root.get("timestamp")
        if timestamp_str:
            # JUnit timestamp format: 2024-09-24T19:28:46.123456
            timestamp = datetime.fromisoformat(timestamp_str)
        else:
            timestamp = datetime.now()

        # Create test run
        test_run = TestRun.create(
            source=TestSource.PYTEST,
            timestamp=timestamp,
            metadata={
                "format": "junit_xml",
                "name": root.get("name", ""),
                "tests": int(root.get("tests", 0)),
                "errors": int(root.get("errors", 0)),
                "failures": int(root.get("failures", 0)),
                "skipped": int(root.get("skipped", 0)),
                "time": float(root.get("time", 0)),
            },
        )

        # Parse test suites
        testsuites = root.findall("testsuite")
        if not testsuites:
            # Sometimes the root is the testsuite
            if root.tag == "testsuite":
                testsuites = [root]

        for testsuite_elem in testsuites:
            suite = self._parse_junit_testsuite(testsuite_elem)
            test_run.add_suite(suite)

        return test_run

    def _parse_junit_testsuite(self, elem: ET.Element) -> TestSuite:
        """Parse a JUnit testsuite element."""
        suite_name = elem.get("name", "unknown")

        # Try to determine test type from suite name
        test_type = self._infer_test_type(suite_name)

        suite = TestSuite(
            suite_id=str(uuid4()),
            name=suite_name,
            test_type=test_type,
            metadata={
                "tests": int(elem.get("tests", 0)),
                "errors": int(elem.get("errors", 0)),
                "failures": int(elem.get("failures", 0)),
                "skipped": int(elem.get("skipped", 0)),
                "time": float(elem.get("time", 0)),
            },
        )

        # Parse test cases
        for testcase_elem in elem.findall("testcase"):
            test_case = self._parse_junit_testcase(testcase_elem)
            suite.add_test_case(test_case)

        return suite

    def _parse_junit_testcase(self, elem: ET.Element) -> TestCase:
        """Parse a JUnit testcase element."""
        name = elem.get("name", "unknown")
        classname = elem.get("classname", "")
        file_path = elem.get("file", "")
        line_number = elem.get("line")

        # Try to determine test type from classname or name
        test_type = self._infer_test_type(classname or name)

        # Extract markers from properties
        markers = []
        properties = elem.find("properties")
        if properties is not None:
            for prop in properties.findall("property"):
                if prop.get("name") == "markers":
                    markers = prop.get("value", "").split()

        case = TestCase(
            case_id=str(uuid4()),
            name=name,
            file_path=file_path if file_path else None,
            line_number=int(line_number) if line_number else None,
            test_type=test_type,
            markers=markers,
            metadata={"classname": classname},
        )

        # Parse test result
        duration_ms = float(elem.get("time", 0)) * 1000  # Convert seconds to ms

        # Determine status
        failure = elem.find("failure")
        error = elem.find("error")
        skipped = elem.find("skipped")

        if failure is not None:
            status = TestStatus.FAILED
            error_message = failure.get("message", "")
            error_traceback = failure.text
        elif error is not None:
            status = TestStatus.ERROR
            error_message = error.get("message", "")
            error_traceback = error.text
        elif skipped is not None:
            status = TestStatus.SKIPPED
            error_message = skipped.get("message", "")
            error_traceback = None
        else:
            status = TestStatus.PASSED
            error_message = None
            error_traceback = None

        # Check for system-out (may contain benchmark data)
        system_out = elem.find("system-out")
        perf_metrics = None
        if system_out is not None and system_out.text:
            # Try to extract benchmark metrics from output
            perf_metrics = self._extract_benchmark_metrics(system_out.text)

        result = TestResult(
            test_id=str(uuid4()),
            name=name,
            status=status,
            duration_ms=duration_ms,
            timestamp=datetime.now(),  # JUnit XML doesn't have per-test timestamps
            error_message=error_message,
            error_traceback=error_traceback,
            performance=perf_metrics,
            metadata={"classname": classname},
        )

        case.add_result(result)
        return case

    def _parse_json(self, data: dict[str, Any]) -> TestRun:
        """Parse pytest JSON format (from pytest-json-report plugin)."""
        # Extract timestamp
        created = data.get("created")
        if created:
            timestamp = datetime.fromtimestamp(created)
        else:
            timestamp = datetime.now()

        # Create test run
        summary = data.get("summary", {})
        test_run = TestRun.create(
            source=TestSource.PYTEST,
            timestamp=timestamp,
            metadata={
                "format": "pytest_json",
                "duration": data.get("duration", 0),
                "exitcode": data.get("exitcode", 0),
                "summary": summary,
                "environment": data.get("environment", {}),
            },
        )

        # Group tests by file/class into suites
        tests = data.get("tests", [])
        suite_map: dict[str, TestSuite] = {}

        for test_data in tests:
            # Determine suite name (use file or class)
            test_data.get("nodeid", "")
            file_path = test_data.get("location", [""])[0]

            # Use file as suite name
            suite_name = file_path or "unknown"

            # Get or create suite
            if suite_name not in suite_map:
                test_type = self._infer_test_type(suite_name)
                suite = TestSuite(
                    suite_id=str(uuid4()),
                    name=suite_name,
                    test_type=test_type,
                )
                suite_map[suite_name] = suite
                test_run.add_suite(suite)
            else:
                suite = suite_map[suite_name]

            # Create test case
            test_case = self._parse_json_testcase(test_data)
            suite.add_test_case(test_case)

        return test_run

    def _parse_json_testcase(self, data: dict[str, Any]) -> TestCase:
        """Parse a test case from pytest JSON."""
        name = data.get("name", "unknown")
        nodeid = data.get("nodeid", "")

        # Extract location
        location = data.get("location", ["", 0, ""])
        file_path = location[0] if location else None
        line_number = location[1] if len(location) > 1 else None

        # Extract markers/keywords
        markers = data.get("keywords", [])
        if isinstance(markers, list):
            markers = [m for m in markers if not m.startswith("test_")]
        else:
            markers = []

        # Determine test type
        test_type = self._infer_test_type(nodeid)

        case = TestCase(
            case_id=str(uuid4()),
            name=name,
            file_path=file_path,
            line_number=line_number,
            test_type=test_type,
            markers=markers,
            metadata={"nodeid": nodeid, "keywords": data.get("keywords", [])},
        )

        # Parse result
        outcome = data.get("outcome", "unknown")
        status_map = {
            "passed": TestStatus.PASSED,
            "failed": TestStatus.FAILED,
            "skipped": TestStatus.SKIPPED,
            "error": TestStatus.ERROR,
            "xfailed": TestStatus.XFAILED,
            "xpassed": TestStatus.XPASSED,
        }
        status = status_map.get(outcome, TestStatus.ERROR)

        duration_ms = data.get("duration", 0) * 1000

        # Extract error info
        call = data.get("call", {})
        error_message = None
        error_traceback = None

        if status in [TestStatus.FAILED, TestStatus.ERROR]:
            longrepr = call.get("longrepr", "")
            if longrepr:
                # Split into message and traceback
                lines = longrepr.split("\n")
                error_message = lines[-1] if lines else longrepr
                error_traceback = longrepr

        # Check for benchmark data
        perf_metrics = None
        if "benchmark" in data:
            perf_metrics = self._parse_benchmark_data(data["benchmark"])

        result = TestResult(
            test_id=str(uuid4()),
            name=name,
            status=status,
            duration_ms=duration_ms,
            timestamp=datetime.now(),
            error_message=error_message,
            error_traceback=error_traceback,
            performance=perf_metrics,
            metadata={"outcome": outcome, "call": call},
        )

        case.add_result(result)
        return case

    def _infer_test_type(self, name: str) -> TestType:
        """Infer test type from name/path."""
        name_lower = name.lower()

        if "benchmark" in name_lower:
            return TestType.BENCHMARK
        elif "performance" in name_lower:
            return TestType.PERFORMANCE
        elif "unit" in name_lower:
            return TestType.UNIT
        elif "integration" in name_lower or "integr" in name_lower:
            return TestType.INTEGRATION
        elif "e2e" in name_lower or "end_to_end" in name_lower:
            return TestType.E2E
        elif "functional" in name_lower or "func" in name_lower:
            return TestType.FUNCTIONAL
        elif "stress" in name_lower:
            return TestType.STRESS
        elif "load" in name_lower:
            return TestType.LOAD
        elif "security" in name_lower or "sec" in name_lower:
            return TestType.SECURITY
        else:
            return TestType.UNIT  # Default

    def _extract_benchmark_metrics(self, text: str) -> PerformanceMetrics | None:
        """Try to extract benchmark metrics from test output."""
        # Simple heuristic: look for common benchmark output patterns
        # This is basic - pytest-benchmark has structured output we could parse better
        if "benchmark" not in text.lower():
            return None

        # Try to find timing info
        # Format: "Min: 1.23ms, Max: 4.56ms, Mean: 2.34ms"
        import re

        perf = PerformanceMetrics()

        min_match = re.search(r"min[:\s]+([0-9.]+)\s*(ms|s)", text, re.IGNORECASE)
        if min_match:
            val = float(min_match.group(1))
            unit = min_match.group(2)
            perf.min_ms = val if unit == "ms" else val * 1000

        max_match = re.search(r"max[:\s]+([0-9.]+)\s*(ms|s)", text, re.IGNORECASE)
        if max_match:
            val = float(max_match.group(1))
            unit = max_match.group(2)
            perf.max_ms = val if unit == "ms" else val * 1000

        mean_match = re.search(r"mean[:\s]+([0-9.]+)\s*(ms|s)", text, re.IGNORECASE)
        if mean_match:
            val = float(mean_match.group(1))
            unit = mean_match.group(2)
            perf.avg_ms = val if unit == "ms" else val * 1000

        if perf.min_ms or perf.max_ms or perf.avg_ms:
            return perf

        return None

    def _parse_benchmark_data(self, benchmark_data: dict[str, Any]) -> PerformanceMetrics:
        """Parse pytest-benchmark data."""
        stats = benchmark_data.get("stats", {})

        return PerformanceMetrics(
            min_ms=stats.get("min", 0) * 1000,
            max_ms=stats.get("max", 0) * 1000,
            avg_ms=stats.get("mean", 0) * 1000,
            median_ms=stats.get("median", 0) * 1000,
            std_ms=stats.get("stddev", 0) * 1000,
            custom_metrics={
                "rounds": stats.get("rounds", 0),
                "iterations": stats.get("iterations", 0),
                "outliers": stats.get("outliers", ""),
                "iqr": stats.get("iqr", 0),
            },
        )
