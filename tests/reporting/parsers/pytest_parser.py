"""
Parser for pytest test results.

Supports multiple pytest output formats:
- JUnit XML (--junit-xml flag)
- pytest JSON (via pytest-json-report plugin)
"""

import json
import xml.etree.ElementTree as ET
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Union
from uuid import uuid4

from .base import BaseParser
from ..models import (
    PerformanceMetrics,
    TestCase,
    TestResult,
    TestRun,
    TestSource,
    TestStatus,
    TestSuite,
    TestType,
)


class PytestParser(BaseParser):
    """Parser for pytest test results (JUnit XML or JSON)."""

    def parse(self, source: Union[str, Path, dict]) -> TestRun:
        """
        Parse pytest results into TestRun.

        Args:
            source: Path to JUnit XML file, JSON file, or dict with pytest data

        Returns:
            TestRun object with pytest results
        """
        # If dict, treat as JSON format
        if isinstance(source, dict):
            return self._parse_json(source)

        path = self._ensure_path(source)

        # Determine format from file extension
        if path.suffix == ".xml":
            return self._parse_junit_xml(path)
        elif path.suffix == ".json":
            with open(path, "r") as f:
                data = json.load(f)
            return self._parse_json(data)
        else:
            raise ValueError(f"Unsupported pytest result format: {path.suffix}")

    def _parse_junit_xml(self, path: Path) -> TestRun:
        """Parse JUnit XML format generated by pytest --junit-xml."""
        tree = ET.parse(path)
        root = tree.getroot()

        # Extract timestamp
        timestamp_str = root.get("timestamp")
        if timestamp_str:
            # JUnit timestamp format: 2024-09-24T19:28:46.123456
            timestamp = datetime.fromisoformat(timestamp_str)
        else:
            timestamp = datetime.now()

        # Create test run
        test_run = TestRun.create(
            source=TestSource.PYTEST,
            timestamp=timestamp,
            metadata={
                "format": "junit_xml",
                "name": root.get("name", ""),
                "tests": int(root.get("tests", 0)),
                "errors": int(root.get("errors", 0)),
                "failures": int(root.get("failures", 0)),
                "skipped": int(root.get("skipped", 0)),
                "time": float(root.get("time", 0)),
            },
        )

        # Parse test suites
        testsuites = root.findall("testsuite")
        if not testsuites:
            # Sometimes the root is the testsuite
            if root.tag == "testsuite":
                testsuites = [root]

        for testsuite_elem in testsuites:
            suite = self._parse_junit_testsuite(testsuite_elem)
            test_run.add_suite(suite)

        return test_run

    def _parse_junit_testsuite(self, elem: ET.Element) -> TestSuite:
        """Parse a JUnit testsuite element."""
        suite_name = elem.get("name", "unknown")

        # Try to determine test type from suite name
        test_type = self._infer_test_type(suite_name)

        suite = TestSuite(
            suite_id=str(uuid4()),
            name=suite_name,
            test_type=test_type,
            metadata={
                "tests": int(elem.get("tests", 0)),
                "errors": int(elem.get("errors", 0)),
                "failures": int(elem.get("failures", 0)),
                "skipped": int(elem.get("skipped", 0)),
                "time": float(elem.get("time", 0)),
            },
        )

        # Parse test cases
        for testcase_elem in elem.findall("testcase"):
            test_case = self._parse_junit_testcase(testcase_elem)
            suite.add_test_case(test_case)

        return suite

    def _parse_junit_testcase(self, elem: ET.Element) -> TestCase:
        """Parse a JUnit testcase element."""
        name = elem.get("name", "unknown")
        classname = elem.get("classname", "")
        file_path = elem.get("file", "")
        line_number = elem.get("line")

        # Try to determine test type from classname or name
        test_type = self._infer_test_type(classname or name)

        # Extract markers from properties
        markers = []
        properties = elem.find("properties")
        if properties is not None:
            for prop in properties.findall("property"):
                if prop.get("name") == "markers":
                    markers = prop.get("value", "").split()

        case = TestCase(
            case_id=str(uuid4()),
            name=name,
            file_path=file_path if file_path else None,
            line_number=int(line_number) if line_number else None,
            test_type=test_type,
            markers=markers,
            metadata={"classname": classname},
        )

        # Parse test result
        duration_ms = float(elem.get("time", 0)) * 1000  # Convert seconds to ms

        # Determine status
        failure = elem.find("failure")
        error = elem.find("error")
        skipped = elem.find("skipped")

        if failure is not None:
            status = TestStatus.FAILED
            error_message = failure.get("message", "")
            error_traceback = failure.text
        elif error is not None:
            status = TestStatus.ERROR
            error_message = error.get("message", "")
            error_traceback = error.text
        elif skipped is not None:
            status = TestStatus.SKIPPED
            error_message = skipped.get("message", "")
            error_traceback = None
        else:
            status = TestStatus.PASSED
            error_message = None
            error_traceback = None

        # Check for system-out (may contain benchmark data)
        system_out = elem.find("system-out")
        perf_metrics = None
        if system_out is not None and system_out.text:
            # Try to extract benchmark metrics from output
            perf_metrics = self._extract_benchmark_metrics(system_out.text)

        result = TestResult(
            test_id=str(uuid4()),
            name=name,
            status=status,
            duration_ms=duration_ms,
            timestamp=datetime.now(),  # JUnit XML doesn't have per-test timestamps
            error_message=error_message,
            error_traceback=error_traceback,
            performance=perf_metrics,
            metadata={"classname": classname},
        )

        case.add_result(result)
        return case

    def _parse_json(self, data: Dict[str, Any]) -> TestRun:
        """Parse pytest JSON format (from pytest-json-report plugin)."""
        # Extract timestamp
        created = data.get("created")
        if created:
            timestamp = datetime.fromtimestamp(created)
        else:
            timestamp = datetime.now()

        # Create test run
        summary = data.get("summary", {})
        test_run = TestRun.create(
            source=TestSource.PYTEST,
            timestamp=timestamp,
            metadata={
                "format": "pytest_json",
                "duration": data.get("duration", 0),
                "exitcode": data.get("exitcode", 0),
                "summary": summary,
                "environment": data.get("environment", {}),
            },
        )

        # Group tests by file/class into suites
        tests = data.get("tests", [])
        suite_map: Dict[str, TestSuite] = {}

        for test_data in tests:
            # Determine suite name (use file or class)
            nodeid = test_data.get("nodeid", "")
            file_path = test_data.get("location", [""])[0]

            # Use file as suite name
            suite_name = file_path or "unknown"

            # Get or create suite
            if suite_name not in suite_map:
                test_type = self._infer_test_type(suite_name)
                suite = TestSuite(
                    suite_id=str(uuid4()),
                    name=suite_name,
                    test_type=test_type,
                )
                suite_map[suite_name] = suite
                test_run.add_suite(suite)
            else:
                suite = suite_map[suite_name]

            # Create test case
            test_case = self._parse_json_testcase(test_data)
            suite.add_test_case(test_case)

        return test_run

    def _parse_json_testcase(self, data: Dict[str, Any]) -> TestCase:
        """Parse a test case from pytest JSON."""
        name = data.get("name", "unknown")
        nodeid = data.get("nodeid", "")

        # Extract location
        location = data.get("location", ["", 0, ""])
        file_path = location[0] if location else None
        line_number = location[1] if len(location) > 1 else None

        # Extract markers/keywords
        markers = data.get("keywords", [])
        if isinstance(markers, list):
            markers = [m for m in markers if not m.startswith("test_")]
        else:
            markers = []

        # Determine test type
        test_type = self._infer_test_type(nodeid)

        case = TestCase(
            case_id=str(uuid4()),
            name=name,
            file_path=file_path,
            line_number=line_number,
            test_type=test_type,
            markers=markers,
            metadata={"nodeid": nodeid, "keywords": data.get("keywords", [])},
        )

        # Parse result
        outcome = data.get("outcome", "unknown")
        status_map = {
            "passed": TestStatus.PASSED,
            "failed": TestStatus.FAILED,
            "skipped": TestStatus.SKIPPED,
            "error": TestStatus.ERROR,
            "xfailed": TestStatus.XFAILED,
            "xpassed": TestStatus.XPASSED,
        }
        status = status_map.get(outcome, TestStatus.ERROR)

        duration_ms = data.get("duration", 0) * 1000

        # Extract error info
        call = data.get("call", {})
        error_message = None
        error_traceback = None

        if status in [TestStatus.FAILED, TestStatus.ERROR]:
            longrepr = call.get("longrepr", "")
            if longrepr:
                # Split into message and traceback
                lines = longrepr.split("\n")
                error_message = lines[-1] if lines else longrepr
                error_traceback = longrepr

        # Check for benchmark data
        perf_metrics = None
        if "benchmark" in data:
            perf_metrics = self._parse_benchmark_data(data["benchmark"])

        result = TestResult(
            test_id=str(uuid4()),
            name=name,
            status=status,
            duration_ms=duration_ms,
            timestamp=datetime.now(),
            error_message=error_message,
            error_traceback=error_traceback,
            performance=perf_metrics,
            metadata={"outcome": outcome, "call": call},
        )

        case.add_result(result)
        return case

    def _infer_test_type(self, name: str) -> TestType:
        """Infer test type from name/path."""
        name_lower = name.lower()

        if "benchmark" in name_lower:
            return TestType.BENCHMARK
        elif "performance" in name_lower:
            return TestType.PERFORMANCE
        elif "unit" in name_lower:
            return TestType.UNIT
        elif "integration" in name_lower or "integr" in name_lower:
            return TestType.INTEGRATION
        elif "e2e" in name_lower or "end_to_end" in name_lower:
            return TestType.E2E
        elif "functional" in name_lower or "func" in name_lower:
            return TestType.FUNCTIONAL
        elif "stress" in name_lower:
            return TestType.STRESS
        elif "load" in name_lower:
            return TestType.LOAD
        elif "security" in name_lower or "sec" in name_lower:
            return TestType.SECURITY
        else:
            return TestType.UNIT  # Default

    def _extract_benchmark_metrics(self, text: str) -> Optional[PerformanceMetrics]:
        """Try to extract benchmark metrics from test output."""
        # Simple heuristic: look for common benchmark output patterns
        # This is basic - pytest-benchmark has structured output we could parse better
        if "benchmark" not in text.lower():
            return None

        # Try to find timing info
        # Format: "Min: 1.23ms, Max: 4.56ms, Mean: 2.34ms"
        import re

        perf = PerformanceMetrics()

        min_match = re.search(r"min[:\s]+([0-9.]+)\s*(ms|s)", text, re.IGNORECASE)
        if min_match:
            val = float(min_match.group(1))
            unit = min_match.group(2)
            perf.min_ms = val if unit == "ms" else val * 1000

        max_match = re.search(r"max[:\s]+([0-9.]+)\s*(ms|s)", text, re.IGNORECASE)
        if max_match:
            val = float(max_match.group(1))
            unit = max_match.group(2)
            perf.max_ms = val if unit == "ms" else val * 1000

        mean_match = re.search(r"mean[:\s]+([0-9.]+)\s*(ms|s)", text, re.IGNORECASE)
        if mean_match:
            val = float(mean_match.group(1))
            unit = mean_match.group(2)
            perf.avg_ms = val if unit == "ms" else val * 1000

        if perf.min_ms or perf.max_ms or perf.avg_ms:
            return perf

        return None

    def _parse_benchmark_data(self, benchmark_data: Dict[str, Any]) -> PerformanceMetrics:
        """Parse pytest-benchmark data."""
        stats = benchmark_data.get("stats", {})

        return PerformanceMetrics(
            min_ms=stats.get("min", 0) * 1000,
            max_ms=stats.get("max", 0) * 1000,
            avg_ms=stats.get("mean", 0) * 1000,
            median_ms=stats.get("median", 0) * 1000,
            std_ms=stats.get("stddev", 0) * 1000,
            custom_metrics={
                "rounds": stats.get("rounds", 0),
                "iterations": stats.get("iterations", 0),
                "outliers": stats.get("outliers", ""),
                "iqr": stats.get("iqr", 0),
            },
        )
