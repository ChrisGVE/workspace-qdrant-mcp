"""
End-to-End Conversational Rule Addition Workflow Tests (Task 337.2).

Tests the complete workflow from conversational rule addition through injection
to behavioral observation, validating that rules added through conversation
actually change LLM behavior in measurable ways.

Workflow:
1. User adds rule through conversational interface
2. Rule is extracted and parsed from conversation
3. Rule is stored in memory system
4. Rule is injected into LLM context
5. Behavioral changes are measured and validated

This ensures the full rule lifecycle works correctly from end to end.
"""

import asyncio
import pytest
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Optional
from unittest.mock import AsyncMock, Mock

from src.python.common.core.memory import (
    MemoryManager,
)
from src.python.common.memory.types import (
    AuthorityLevel,
    MemoryCategory,
    MemoryRule,
)

# Import conversational integration
try:
    from src.python.common.memory.claude_integration import ClaudeCodeIntegration
    from src.python.common.memory.types import ConversationalUpdate
    CONVERSATIONAL_AVAILABLE = True
except ImportError:
    CONVERSATIONAL_AVAILABLE = False

# Import our behavioral harness from 337.1
from tests.integration.test_llm_behavioral_harness import (
    LLMBehavioralHarness,
    MockLLMProvider,
    ClaudeProvider,
    ExecutionMode,
    BehavioralMetrics,
    LLMResponse,
    PromptTemplate,
    ANTHROPIC_AVAILABLE,
)


@pytest.fixture
def mock_token_counter():
    """Provide mock token counter."""
    counter = Mock()
    counter.count_rules_tokens = Mock(return_value=Mock(total_tokens=100))
    counter.optimize_rules_for_context = Mock(return_value=([], Mock(total_tokens=0)))
    return counter


@pytest.fixture
def claude_integration(mock_token_counter):
    """Provide ClaudeCodeIntegration for conversational rule extraction."""
    if not CONVERSATIONAL_AVAILABLE:
        pytest.skip("Conversational integration not available")

    return ClaudeCodeIntegration(
        token_counter=mock_token_counter,
        max_memory_tokens=5000,
    )


@pytest.fixture
async def memory_manager():
    """Provide mock MemoryManager for rule storage."""
    manager = AsyncMock(spec=MemoryManager)
    manager.add_rule = AsyncMock()
    manager.get_rules = AsyncMock(return_value=[])
    manager.delete_rule = AsyncMock()
    manager.initialize = AsyncMock()

    await manager.initialize()
    return manager


@pytest.fixture
def mock_llm_provider():
    """Provide mock LLM provider for testing."""
    return MockLLMProvider()


@pytest.fixture
def behavioral_harness(mock_llm_provider, memory_manager):
    """Provide behavioral harness for testing."""
    return LLMBehavioralHarness(
        provider=mock_llm_provider,
        memory_manager=memory_manager,
        mode=ExecutionMode.MOCK
    )


class ConversationalRuleWorkflow:
    """
    Orchestrates end-to-end conversational rule workflow.

    Handles the complete lifecycle:
    1. Extract rule from conversation
    2. Store in memory
    3. Inject into LLM context
    4. Test behavioral changes
    """

    def __init__(
        self,
        claude_integration: 'ClaudeCodeIntegration',
        memory_manager: MemoryManager,
        behavioral_harness: LLMBehavioralHarness
    ):
        """
        Initialize workflow orchestrator.

        Args:
            claude_integration: For rule extraction
            memory_manager: For rule storage
            behavioral_harness: For behavioral testing
        """
        self.claude_integration = claude_integration
        self.memory_manager = memory_manager
        self.behavioral_harness = behavioral_harness

    async def process_conversational_rule(
        self,
        conversation_text: str,
        test_prompt: str,
        expected_patterns: List[str] = None,
        forbidden_patterns: List[str] = None
    ) -> tuple[List[MemoryRule], BehavioralMetrics, LLMResponse, LLMResponse]:
        """
        Process a conversational rule end-to-end.

        Args:
            conversation_text: Conversational text containing rule
            test_prompt: Prompt to test behavioral changes
            expected_patterns: Patterns expected with rule
            forbidden_patterns: Patterns to avoid

        Returns:
            Tuple of (extracted_rules, metrics, response_with_rules, response_without_rules)
        """
        # Step 1: Extract rule from conversation
        updates = self.claude_integration.detect_conversational_updates(
            conversation_text
        )

        if not updates:
            raise ValueError(f"No rules extracted from: {conversation_text}")

        # Step 2: Convert updates to memory rules
        rules = []
        for update in updates:
            if update.extracted_rule:
                rule = MemoryRule(
                    rule=update.extracted_rule,
                    category=update.category,
                    authority=update.authority,
                    id=f"conv_rule_{len(rules)}",
                    scope=update.scope if update.scope else [],
                    source="conversation",
                    created_at=datetime.now(timezone.utc),
                    updated_at=datetime.now(timezone.utc),
                    metadata={
                        "confidence": update.confidence,
                        "original_text": conversation_text
                    }
                )
                rules.append(rule)

        # Step 3: Store rules in memory
        for rule in rules:
            await self.memory_manager.add_rule(rule)

        # Step 4: Test behavioral changes
        metrics, with_rules, without_rules = await self.behavioral_harness.run_behavioral_test(
            prompt=test_prompt,
            rules=rules,
            expected_patterns=expected_patterns,
            forbidden_patterns=forbidden_patterns
        )

        return rules, metrics, with_rules, without_rules


@pytest.mark.asyncio
class TestConversationalRuleWorkflow:
    """Test end-to-end conversational rule workflow."""

    async def test_simple_preference_rule_workflow(
        self,
        claude_integration,
        memory_manager,
        behavioral_harness
    ):
        """Test simple preference rule from conversation to behavior."""
        if not CONVERSATIONAL_AVAILABLE:
            pytest.skip("Conversational integration not available")

        workflow = ConversationalRuleWorkflow(
            claude_integration=claude_integration,
            memory_manager=memory_manager,
            behavioral_harness=behavioral_harness
        )

        # Conversational rule addition (matches "Remember: I prefer X" pattern)
        conversation = "Remember: I prefer using type hints in Python code"

        # Test prompt
        test_prompt = "Write a function to add two numbers"

        # Expected patterns (type hints should appear)
        expected_patterns = [r":\s*\w+", r"->\s*\w+"]

        # Execute workflow
        rules, metrics, with_rules, without_rules = await workflow.process_conversational_rule(
            conversation_text=conversation,
            test_prompt=test_prompt,
            expected_patterns=expected_patterns
        )

        # Verify rule extraction
        assert len(rules) > 0
        assert any("type hint" in r.rule.lower() for r in rules)

        # Verify rule storage
        assert memory_manager.add_rule.called

        # Verify behavioral change
        assert metrics.behavior_changed
        assert metrics.confidence_score > 0

    async def test_behavior_rule_workflow(
        self,
        claude_integration,
        memory_manager,
        behavioral_harness
    ):
        """Test behavior rule affecting code generation."""
        if not CONVERSATIONAL_AVAILABLE:
            pytest.skip("Conversational integration not available")

        workflow = ConversationalRuleWorkflow(
            claude_integration=claude_integration,
            memory_manager=memory_manager,
            behavioral_harness=behavioral_harness
        )

        # Strong behavioral rule (matches "Always use X for Y" pattern)
        conversation = "Always use docstrings with Args and Returns for all Python functions"

        test_prompt = "Create a function to process user data"

        expected_patterns = [
            r'""".*?"""',
            r"Args:",
            r"Returns:"
        ]

        rules, metrics, with_rules, without_rules = await workflow.process_conversational_rule(
            conversation_text=conversation,
            test_prompt=test_prompt,
            expected_patterns=expected_patterns
        )

        # Verify rule has high authority
        assert any(r.authority == AuthorityLevel.ABSOLUTE for r in rules)

        # Note: Mock provider may not show behavioral changes
        # This test primarily validates the workflow, not actual LLM behavior
        # For actual behavioral validation, use live_api tests
        if behavioral_harness.mode == ExecutionMode.MOCK:
            # With mock provider, we can't guarantee behavioral changes
            # But we can verify the workflow completed successfully
            assert len(rules) > 0  # Rules were extracted
        else:
            # With real LLM, expect behavioral changes
            assert metrics.behavior_changed
            assert metrics.pattern_matches > 0
            assert metrics.confidence_score >= 50.0

    async def test_multiple_conversational_rules(
        self,
        claude_integration,
        memory_manager,
        behavioral_harness
    ):
        """Test multiple rules from single conversation."""
        if not CONVERSATIONAL_AVAILABLE:
            pytest.skip("Conversational integration not available")

        workflow = ConversationalRuleWorkflow(
            claude_integration=claude_integration,
            memory_manager=memory_manager,
            behavioral_harness=behavioral_harness
        )

        # Multiple rules in conversation (using matching patterns)
        conversation = """
        Remember: I prefer type hints.
        Always use docstrings for all functions.
        Don't use single-letter variable names.
        """

        test_prompt = "Write a data processing function"

        expected_patterns = [
            r":\s*\w+",  # Type hints
            r'""".*?"""',  # Docstrings
        ]

        rules, metrics, with_rules, without_rules = await workflow.process_conversational_rule(
            conversation_text=conversation,
            test_prompt=test_prompt,
            expected_patterns=expected_patterns
        )

        # Verify multiple rules extracted
        # Note: The exact number depends on the extraction logic
        assert len(rules) >= 1

        # Verify combined behavioral effect
        assert metrics.behavior_changed
        assert metrics.confidence_score > 0

    async def test_rule_confidence_affects_behavior(
        self,
        claude_integration,
        memory_manager,
        behavioral_harness
    ):
        """Test that rule confidence level affects behavioral impact."""
        if not CONVERSATIONAL_AVAILABLE:
            pytest.skip("Conversational integration not available")

        workflow = ConversationalRuleWorkflow(
            claude_integration=claude_integration,
            memory_manager=memory_manager,
            behavioral_harness=behavioral_harness
        )

        # High confidence rule (matches "Always use X for Y" pattern)
        high_confidence = "Always use docstrings for every function"

        # Lower confidence rule (matches "Remember: I prefer X" pattern)
        low_confidence = "Remember: I prefer adding comments to code"

        # Test high confidence rule
        rules_high, metrics_high, _, _ = await workflow.process_conversational_rule(
            conversation_text=high_confidence,
            test_prompt="Write a helper function",
            expected_patterns=[r'""".*?"""']
        )

        # Test low confidence rule
        rules_low, metrics_low, _, _ = await workflow.process_conversational_rule(
            conversation_text=low_confidence,
            test_prompt="Write a helper function",
            expected_patterns=[r"#.*"]
        )

        # High confidence should have stronger impact
        # Note: This may not always hold with mock provider,
        # but validates the workflow
        assert len(rules_high) > 0
        assert len(rules_low) >= 0  # May or may not extract a rule

    async def test_authority_level_preservation(
        self,
        claude_integration,
        memory_manager,
        behavioral_harness
    ):
        """Test that authority levels are preserved through workflow."""
        if not CONVERSATIONAL_AVAILABLE:
            pytest.skip("Conversational integration not available")

        workflow = ConversationalRuleWorkflow(
            claude_integration=claude_integration,
            memory_manager=memory_manager,
            behavioral_harness=behavioral_harness
        )

        # Absolute authority pattern (matches "Always use X for Y")
        conversation_absolute = "Always use pytest for testing Python code"

        # Default authority pattern (matches "Remember: I prefer X")
        conversation_default = "Remember: I prefer using black for code formatting"

        # Test absolute authority
        rules_absolute, _, _, _ = await workflow.process_conversational_rule(
            conversation_text=conversation_absolute,
            test_prompt="Set up testing",
            expected_patterns=[r"pytest"]
        )

        # Test default authority
        rules_default, _, _, _ = await workflow.process_conversational_rule(
            conversation_text=conversation_default,
            test_prompt="Format code",
            expected_patterns=[r"black"]
        )

        # Verify authority levels
        if rules_absolute:
            assert any(r.authority == AuthorityLevel.ABSOLUTE for r in rules_absolute)

        if rules_default:
            assert any(r.authority == AuthorityLevel.DEFAULT for r in rules_default)

    async def test_workflow_with_real_memory_manager(
        self,
        claude_integration,
        tmp_path
    ):
        """Test workflow with actual memory manager (not mocked)."""
        if not CONVERSATIONAL_AVAILABLE:
            pytest.skip("Conversational integration not available")

        # This test would use a real memory manager
        # For now, we skip it in automated tests
        pytest.skip("Requires full memory manager setup")


@pytest.mark.asyncio
class TestConversationalRuleCategories:
    """Test different rule categories through conversational workflow."""

    async def test_behavior_category_rule(
        self,
        claude_integration,
        memory_manager,
        behavioral_harness
    ):
        """Test BEHAVIOR category rule extraction and effect."""
        if not CONVERSATIONAL_AVAILABLE:
            pytest.skip("Conversational integration not available")

        workflow = ConversationalRuleWorkflow(
            claude_integration=claude_integration,
            memory_manager=memory_manager,
            behavioral_harness=behavioral_harness
        )

        conversation = "Always use validation for user input processing"

        rules, metrics, _, _ = await workflow.process_conversational_rule(
            conversation_text=conversation,
            test_prompt="Create a user input handler",
            expected_patterns=[r"validate", r"check"]
        )

        assert any(r.category == MemoryCategory.BEHAVIOR for r in rules)

    async def test_preference_category_rule(
        self,
        claude_integration,
        memory_manager,
        behavioral_harness
    ):
        """Test PREFERENCE category rule extraction."""
        if not CONVERSATIONAL_AVAILABLE:
            pytest.skip("Conversational integration not available")

        workflow = ConversationalRuleWorkflow(
            claude_integration=claude_integration,
            memory_manager=memory_manager,
            behavioral_harness=behavioral_harness
        )

        conversation = "Remember: I prefer snake_case for function naming"

        rules, _, _, _ = await workflow.process_conversational_rule(
            conversation_text=conversation,
            test_prompt="Define a function",
            expected_patterns=[r"[a-z_]+"]
        )

        assert any(r.category == MemoryCategory.PREFERENCE for r in rules)


@pytest.mark.asyncio
class TestWorkflowEdgeCases:
    """Test edge cases in conversational workflow."""

    async def test_no_rule_extracted(
        self,
        claude_integration,
        memory_manager,
        behavioral_harness
    ):
        """Test handling when no rule is extracted."""
        if not CONVERSATIONAL_AVAILABLE:
            pytest.skip("Conversational integration not available")

        workflow = ConversationalRuleWorkflow(
            claude_integration=claude_integration,
            memory_manager=memory_manager,
            behavioral_harness=behavioral_harness
        )

        # Generic conversation without clear rule
        conversation = "The weather is nice today"

        with pytest.raises(ValueError, match="No rules extracted"):
            await workflow.process_conversational_rule(
                conversation_text=conversation,
                test_prompt="Write code",
                expected_patterns=[]
            )

    async def test_ambiguous_rule_handling(
        self,
        claude_integration,
        memory_manager,
        behavioral_harness
    ):
        """Test handling of ambiguous rule statements."""
        if not CONVERSATIONAL_AVAILABLE:
            pytest.skip("Conversational integration not available")

        workflow = ConversationalRuleWorkflow(
            claude_integration=claude_integration,
            memory_manager=memory_manager,
            behavioral_harness=behavioral_harness
        )

        # Ambiguous statement
        conversation = "Sometimes maybe use type hints or not"

        # Should still attempt to extract something
        # The extraction logic determines what happens
        try:
            rules, metrics, _, _ = await workflow.process_conversational_rule(
                conversation_text=conversation,
                test_prompt="Write function",
                expected_patterns=[]
            )

            # If a rule was extracted, verify it has lower confidence
            if rules:
                assert all(
                    r.metadata.get("confidence", 1.0) < 1.0
                    for r in rules
                    if "confidence" in r.metadata
                )
        except ValueError:
            # No rule extracted is also acceptable for ambiguous input
            pass


@pytest.mark.asyncio
@pytest.mark.live_api
class TestLiveConversationalWorkflow:
    """Test conversational workflow with live Claude API."""

    async def test_live_api_conversational_workflow(self, tmp_path):
        """Test full workflow with real Claude API."""
        if not ANTHROPIC_AVAILABLE or not CONVERSATIONAL_AVAILABLE:
            pytest.skip("Live API test requires anthropic and conversational integration")

        import os
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            pytest.skip("ANTHROPIC_API_KEY not available")

        # This would test with real API
        # For now, skip in automated tests
        pytest.skip("Live API test - run manually when needed")


class TestConversationalWorkflowOrchestrator:
    """Test the ConversationalRuleWorkflow orchestrator."""

    def test_workflow_initialization(
        self,
        claude_integration,
        memory_manager,
        behavioral_harness
    ):
        """Test workflow orchestrator initialization."""
        if not CONVERSATIONAL_AVAILABLE:
            pytest.skip("Conversational integration not available")

        workflow = ConversationalRuleWorkflow(
            claude_integration=claude_integration,
            memory_manager=memory_manager,
            behavioral_harness=behavioral_harness
        )

        assert workflow.claude_integration == claude_integration
        assert workflow.memory_manager == memory_manager
        assert workflow.behavioral_harness == behavioral_harness
