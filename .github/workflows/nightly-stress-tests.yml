name: Nightly Stress Tests

# Comprehensive stress testing scheduled nightly
on:
  schedule:
    # Run at 2 AM UTC every day
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      stress_level:
        description: 'Stress test intensity level'
        required: false
        default: 'normal'
        type: choice
        options:
          - light
          - normal
          - heavy
          - extreme
      test_duration:
        description: 'Test duration in minutes'
        required: false
        default: '60'
        type: string
      enable_memory_profiling:
        description: 'Enable detailed memory profiling'
        required: false
        default: 'true'
        type: boolean

env:
  PYTHON_VERSION: "3.11"
  STRESS_LEVEL: ${{ github.event.inputs.stress_level || 'normal' }}
  TEST_DURATION_MIN: ${{ github.event.inputs.test_duration || '60' }}

jobs:
  # High-volume ingestion stress test
  high-volume-ingestion:
    name: High-Volume Ingestion Stress Test
    runs-on: ubuntu-latest
    timeout-minutes: 90

    services:
      qdrant:
        image: qdrant/qdrant:v1.7.4
        ports:
          - 6333:6333
          - 6334:6334
        env:
          QDRANT__SERVICE__MAX_REQUEST_SIZE_MB: 100

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v1

    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/uv
        key: ${{ runner.os }}-stress-${{ hashFiles('**/pyproject.toml', '**/uv.lock') }}

    - name: Install dependencies
      run: |
        uv venv --python ${{ env.PYTHON_VERSION }}
        . .venv/bin/activate
        uv pip install -e ".[dev]"
        uv pip install memory-profiler pytest-monitor

    - name: Wait for Qdrant
      run: |
        timeout 60 bash -c 'until curl -f http://localhost:6333/healthz; do sleep 2; done'

    - name: Run high-volume ingestion stress tests
      env:
        QDRANT_URL: http://localhost:6333
        STRESS_LEVEL: ${{ env.STRESS_LEVEL }}
      run: |
        . .venv/bin/activate

        # Set ingestion volume based on stress level
        case "$STRESS_LEVEL" in
          light)
            DOCUMENT_COUNT=1000
            ;;
          normal)
            DOCUMENT_COUNT=5000
            ;;
          heavy)
            DOCUMENT_COUNT=10000
            ;;
          extreme)
            DOCUMENT_COUNT=50000
            ;;
        esac

        echo "Running high-volume ingestion with $DOCUMENT_COUNT documents..."

        pytest tests/integration/test_grpc_load_stress.py \
          -v \
          --tb=short \
          -m "stress or performance" \
          --durations=20 \
          --junitxml=high-volume-ingestion-results.xml \
          -o log_cli=true \
          -o log_cli_level=INFO
      timeout-minutes: 75

    - name: Generate ingestion performance report
      if: always()
      run: |
        cat > ingestion-report.md << 'EOF'
        # High-Volume Ingestion Stress Test Report

        **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
        **Stress Level:** ${{ env.STRESS_LEVEL }}
        **Duration:** ${{ env.TEST_DURATION_MIN }} minutes

        ## Test Configuration
        - Qdrant Version: v1.7.4
        - Python Version: ${{ env.PYTHON_VERSION }}
        - Max Request Size: 100 MB

        ## Results
        EOF

        if [ -f high-volume-ingestion-results.xml ]; then
          python3 << 'PYEOF' >> ingestion-report.md
import xml.etree.ElementTree as ET
try:
    tree = ET.parse('high-volume-ingestion-results.xml')
    root = tree.getroot()
    tests = root.attrib.get('tests', '0')
    failures = root.attrib.get('failures', '0')
    errors = root.attrib.get('errors', '0')
    time_val = root.attrib.get('time', '0')

    print(f"- **Total Tests:** {tests}")
    print(f"- **Failures:** {failures}")
    print(f"- **Errors:** {errors}")
    print(f"- **Duration:** {float(time_val):.2f}s")
except Exception as e:
    print(f"Could not parse results: {e}")
PYEOF
        fi

    - name: Upload ingestion test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: high-volume-ingestion-results
        path: |
          high-volume-ingestion-results.xml
          ingestion-report.md
        retention-days: 30

  # Concurrent operations stress test
  concurrent-operations:
    name: Concurrent Operations Stress Test
    runs-on: ubuntu-latest
    timeout-minutes: 90

    services:
      qdrant:
        image: qdrant/qdrant:v1.7.4
        ports:
          - 6333:6333
          - 6334:6334

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v1

    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/uv
        key: ${{ runner.os }}-concurrent-${{ hashFiles('**/pyproject.toml', '**/uv.lock') }}

    - name: Install dependencies
      run: |
        uv venv --python ${{ env.PYTHON_VERSION }}
        . .venv/bin/activate
        uv pip install -e ".[dev]"
        uv pip install pytest-xdist pytest-timeout

    - name: Wait for Qdrant
      run: |
        timeout 60 bash -c 'until curl -f http://localhost:6333/healthz; do sleep 2; done'

    - name: Run concurrent operations stress tests
      env:
        QDRANT_URL: http://localhost:6333
        STRESS_LEVEL: ${{ env.STRESS_LEVEL }}
      run: |
        . .venv/bin/activate

        # Set concurrency level based on stress level
        case "$STRESS_LEVEL" in
          light)
            WORKERS=4
            ;;
          normal)
            WORKERS=8
            ;;
          heavy)
            WORKERS=16
            ;;
          extreme)
            WORKERS=32
            ;;
        esac

        echo "Running concurrent operations with $WORKERS workers..."

        # Run E2E concurrent access tests
        pytest tests/e2e/test_multi_user_concurrent_access.py \
          -v \
          --tb=short \
          -n $WORKERS \
          --dist loadgroup \
          --durations=20 \
          --junitxml=concurrent-operations-results.xml \
          -o log_cli=true \
          -o log_cli_level=INFO
      timeout-minutes: 75

    - name: Generate concurrency report
      if: always()
      run: |
        cat > concurrency-report.md << 'EOF'
        # Concurrent Operations Stress Test Report

        **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
        **Stress Level:** ${{ env.STRESS_LEVEL }}

        ## Test Configuration
        - Concurrency Workers: Varies by stress level
        - Test Suite: Multi-user concurrent access

        ## Results
        EOF

        if [ -f concurrent-operations-results.xml ]; then
          python3 << 'PYEOF' >> concurrency-report.md
import xml.etree.ElementTree as ET
try:
    tree = ET.parse('concurrent-operations-results.xml')
    root = tree.getroot()
    tests = root.attrib.get('tests', '0')
    failures = root.attrib.get('failures', '0')
    errors = root.attrib.get('errors', '0')
    time_val = root.attrib.get('time', '0')

    print(f"- **Total Tests:** {tests}")
    print(f"- **Failures:** {failures}")
    print(f"- **Errors:** {errors}")
    print(f"- **Duration:** {float(time_val):.2f}s")
except Exception as e:
    print(f"Could not parse results: {e}")
PYEOF
        fi

    - name: Upload concurrent test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: concurrent-operations-results
        path: |
          concurrent-operations-results.xml
          concurrency-report.md
        retention-days: 30

  # Memory pressure stress test
  memory-pressure:
    name: Memory Pressure Stress Test
    runs-on: ubuntu-latest
    timeout-minutes: 90

    services:
      qdrant:
        image: qdrant/qdrant:v1.7.4
        ports:
          - 6333:6333
          - 6334:6334
        options: >-
          --memory=4g
          --memory-swap=4g

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v1

    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/uv
        key: ${{ runner.os }}-memory-${{ hashFiles('**/pyproject.toml', '**/uv.lock') }}

    - name: Install dependencies
      run: |
        uv venv --python ${{ env.PYTHON_VERSION }}
        . .venv/bin/activate
        uv pip install -e ".[dev]"
        uv pip install memory-profiler psutil

    - name: Wait for Qdrant
      run: |
        timeout 60 bash -c 'until curl -f http://localhost:6333/healthz; do sleep 2; done'

    - name: Run memory pressure stress tests
      env:
        QDRANT_URL: http://localhost:6333
        ENABLE_MEMORY_PROFILING: ${{ github.event.inputs.enable_memory_profiling || 'true' }}
      run: |
        . .venv/bin/activate

        echo "Running memory pressure stress tests..."

        # Run unit memory performance tests
        pytest tests/unit/memory/test_memory_performance.py \
          -v \
          --tb=short \
          -m "performance" \
          --durations=20 \
          --junitxml=memory-pressure-results.xml \
          -o log_cli=true \
          -o log_cli_level=INFO

        # Run E2E resource monitoring tests
        pytest tests/e2e/test_resource_usage_monitoring.py \
          -v \
          --tb=short \
          --durations=20 \
          --junitxml=memory-e2e-results.xml \
          -o log_cli=true \
          -o log_cli_level=INFO || echo "Some E2E memory tests may have issues"
      timeout-minutes: 75

    - name: Collect memory profiling data
      if: always() && github.event.inputs.enable_memory_profiling == 'true'
      run: |
        . .venv/bin/activate

        # Generate memory usage report
        python3 << 'EOF' > memory-profile.txt
import psutil
import os

print("=== System Memory Information ===")
vm = psutil.virtual_memory()
print(f"Total Memory: {vm.total / (1024**3):.2f} GB")
print(f"Available: {vm.available / (1024**3):.2f} GB")
print(f"Used: {vm.used / (1024**3):.2f} GB")
print(f"Percentage: {vm.percent}%")

print("\n=== Disk Information ===")
disk = psutil.disk_usage('/')
print(f"Total Disk: {disk.total / (1024**3):.2f} GB")
print(f"Used: {disk.used / (1024**3):.2f} GB")
print(f"Free: {disk.free / (1024**3):.2f} GB")
print(f"Percentage: {disk.percent}%")
EOF

    - name: Generate memory report
      if: always()
      run: |
        cat > memory-report.md << 'EOF'
        # Memory Pressure Stress Test Report

        **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
        **Memory Profiling:** ${{ github.event.inputs.enable_memory_profiling || 'true' }}

        ## Test Configuration
        - Memory Limit: 4 GB
        - Swap Limit: 4 GB

        ## Results
        EOF

        if [ -f memory-pressure-results.xml ]; then
          python3 << 'PYEOF' >> memory-report.md
import xml.etree.ElementTree as ET
try:
    tree = ET.parse('memory-pressure-results.xml')
    root = tree.getroot()
    tests = root.attrib.get('tests', '0')
    failures = root.attrib.get('failures', '0')
    errors = root.attrib.get('errors', '0')
    time_val = root.attrib.get('time', '0')

    print(f"- **Total Tests:** {tests}")
    print(f"- **Failures:** {failures}")
    print(f"- **Errors:** {errors}")
    print(f"- **Duration:** {float(time_val):.2f}s")
except Exception as e:
    print(f"Could not parse results: {e}")
PYEOF
        fi

        if [ -f memory-profile.txt ]; then
          echo "" >> memory-report.md
          echo "## Memory Profile" >> memory-report.md
          echo "\`\`\`" >> memory-report.md
          cat memory-profile.txt >> memory-report.md
          echo "\`\`\`" >> memory-report.md
        fi

    - name: Upload memory test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: memory-pressure-results
        path: |
          memory-pressure-results.xml
          memory-e2e-results.xml
          memory-report.md
          memory-profile.txt
        retention-days: 30

  # Stability stress test (long-running)
  stability-stress:
    name: Stability Stress Test
    runs-on: ubuntu-latest
    timeout-minutes: 120

    services:
      qdrant:
        image: qdrant/qdrant:v1.7.4
        ports:
          - 6333:6333
          - 6334:6334

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v1

    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/uv
        key: ${{ runner.os }}-stability-${{ hashFiles('**/pyproject.toml', '**/uv.lock') }}

    - name: Install dependencies
      run: |
        uv venv --python ${{ env.PYTHON_VERSION }}
        . .venv/bin/activate
        uv pip install -e ".[dev]"

    - name: Wait for Qdrant
      run: |
        timeout 60 bash -c 'until curl -f http://localhost:6333/healthz; do sleep 2; done'

    - name: Run stability stress tests
      env:
        QDRANT_URL: http://localhost:6333
      run: |
        . .venv/bin/activate

        echo "Running stability stress tests (1-hour duration)..."

        # Run 24-hour stability tests with 1-hour timeout for nightly runs
        pytest tests/e2e/test_24hour_stability.py::TestStabilityMonitoring::test_1hour_continuous_operation \
          -v \
          --tb=short \
          --durations=20 \
          --junitxml=stability-results.xml \
          -o log_cli=true \
          -o log_cli_level=INFO
      timeout-minutes: 105

    - name: Upload stability test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: stability-results
        path: stability-results.xml
        retention-days: 30

  # Performance regression detection
  performance-regression:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    timeout-minutes: 60

    services:
      qdrant:
        image: qdrant/qdrant:v1.7.4
        ports:
          - 6333:6333
          - 6334:6334

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v1

    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/uv
        key: ${{ runner.os }}-perf-${{ hashFiles('**/pyproject.toml', '**/uv.lock') }}

    - name: Install dependencies
      run: |
        uv venv --python ${{ env.PYTHON_VERSION }}
        . .venv/bin/activate
        uv pip install -e ".[dev]"
        uv pip install pytest-benchmark

    - name: Wait for Qdrant
      run: |
        timeout 60 bash -c 'until curl -f http://localhost:6333/healthz; do sleep 2; done'

    - name: Run performance regression tests
      env:
        QDRANT_URL: http://localhost:6333
      run: |
        . .venv/bin/activate

        pytest tests/e2e/test_performance_regression.py \
          -v \
          --tb=short \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          --junitxml=performance-regression-results.xml

    - name: Check for performance regressions
      if: always()
      run: |
        if [ -f benchmark-results.json ]; then
          python3 << 'EOF'
import json
import sys

with open('benchmark-results.json') as f:
    data = json.load(f)

regressions = []
for benchmark in data.get('benchmarks', []):
    name = benchmark['name']
    stats = benchmark['stats']
    mean = stats['mean']

    # Example thresholds (would compare against baseline in real scenario)
    if 'ingestion' in name.lower() and mean > 2.0:
        regressions.append(f"{name}: {mean:.3f}s (expected < 2.0s)")
    elif 'search' in name.lower() and mean > 0.5:
        regressions.append(f"{name}: {mean:.3f}s (expected < 0.5s)")

if regressions:
    print("⚠️ Performance regressions detected:")
    for reg in regressions:
        print(f"  - {reg}")
else:
    print("✅ No performance regressions detected")
EOF
        fi

    - name: Upload performance test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: performance-regression-results
        path: |
          performance-regression-results.xml
          benchmark-results.json
        retention-days: 30

  # Aggregate results and send notifications
  stress-test-summary:
    name: Stress Test Summary & Notifications
    runs-on: ubuntu-latest
    needs: [high-volume-ingestion, concurrent-operations, memory-pressure, stability-stress, performance-regression]
    if: always()
    timeout-minutes: 15

    steps:
    - name: Download all test results
      uses: actions/download-artifact@v4

    - name: Generate comprehensive summary
      run: |
        cat > stress-test-summary.md << 'EOF'
        # Nightly Stress Test Summary

        **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
        **Stress Level:** ${{ env.STRESS_LEVEL }}
        **Duration:** ${{ env.TEST_DURATION_MIN }} minutes

        ## Test Suite Results

        ### Job Status
        - **High-Volume Ingestion:** ${{ needs.high-volume-ingestion.result }}
        - **Concurrent Operations:** ${{ needs.concurrent-operations.result }}
        - **Memory Pressure:** ${{ needs.memory-pressure.result }}
        - **Stability Stress:** ${{ needs.stability-stress.result }}
        - **Performance Regression:** ${{ needs.performance-regression.result }}

        EOF

        # Parse each test result
        for dir in */; do
          if [ -d "$dir" ]; then
            echo "### ${dir%/}" >> stress-test-summary.md

            for xml_file in "$dir"*.xml; do
              if [ -f "$xml_file" ]; then
                python3 << PYEOF >> stress-test-summary.md
import xml.etree.ElementTree as ET
try:
    tree = ET.parse('$xml_file')
    root = tree.getroot()
    tests = root.attrib.get('tests', '0')
    failures = root.attrib.get('failures', '0')
    errors = root.attrib.get('errors', '0')
    time_val = root.attrib.get('time', '0')

    status = "✅" if int(failures) == 0 and int(errors) == 0 else "❌"
    print(f"\n{status} **$(basename '$xml_file')**")
    print(f"- Tests: {tests}, Failures: {failures}, Errors: {errors}, Duration: {float(time_val):.2f}s")
except Exception:
    pass
PYEOF
              fi
            done
          fi
        done

        # Add report files if they exist
        for report in *//*-report.md; do
          if [ -f "$report" ]; then
            echo "" >> stress-test-summary.md
            cat "$report" >> stress-test-summary.md
          fi
        done

    - name: Check for failures
      run: |
        FAILED=false

        if [[ "${{ needs.high-volume-ingestion.result }}" == "failure" ]]; then
          echo "❌ High-volume ingestion tests failed"
          FAILED=true
        fi

        if [[ "${{ needs.concurrent-operations.result }}" == "failure" ]]; then
          echo "❌ Concurrent operations tests failed"
          FAILED=true
        fi

        if [[ "${{ needs.memory-pressure.result }}" == "failure" ]]; then
          echo "❌ Memory pressure tests failed"
          FAILED=true
        fi

        if [[ "${{ needs.stability-stress.result }}" == "failure" ]]; then
          echo "❌ Stability stress tests failed"
          FAILED=true
        fi

        if [[ "${{ needs.performance-regression.result }}" == "failure" ]]; then
          echo "❌ Performance regression tests failed"
          FAILED=true
        fi

        if [ "$FAILED" = true ]; then
          echo "## ⚠️ STRESS TESTS FAILED" >> stress-test-summary.md
          echo "One or more stress test suites failed. Please review the results." >> stress-test-summary.md
          exit 1
        else
          echo "## ✅ All Stress Tests Passed" >> stress-test-summary.md
        fi

    - name: Upload comprehensive summary
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: stress-test-summary
        path: stress-test-summary.md
        retention-days: 90

    - name: Create GitHub issue on failure
      if: failure()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          let summary = '';

          if (fs.existsSync('stress-test-summary.md')) {
            summary = fs.readFileSync('stress-test-summary.md', 'utf8');
          }

          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `🚨 Nightly Stress Tests Failed - ${new Date().toISOString().split('T')[0]}`,
            body: `## Nightly Stress Test Failure Report\n\n${summary}\n\n**Workflow Run:** ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
            labels: ['test-failure', 'stress-test', 'automated']
          });
