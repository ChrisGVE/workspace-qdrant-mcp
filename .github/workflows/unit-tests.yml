name: Unit Tests (Fast Feedback)

# Run on every push to any branch for fast developer feedback
on:
  push:
    branches: ['**']  # All branches
  workflow_dispatch:
    inputs:
      parallelism:
        description: 'Test parallelism level'
        required: false
        default: 'auto'
        type: choice
        options:
          - auto
          - '2'
          - '4'
          - '8'

env:
  # Fast feedback configuration
  RUST_BACKTRACE: 1
  CARGO_TERM_COLOR: always
  CARGO_INCREMENTAL: 0  # Disable incremental for faster clean builds in CI

jobs:
  # Python unit tests with fast-fail and parallelization
  python-unit-tests:
    name: Python Unit Tests (${{ matrix.python-version }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 15  # Fast feedback timeout

    strategy:
      fail-fast: true  # Stop on first failure for fast feedback
      matrix:
        os: [ubuntu-latest]  # Single OS for speed, full matrix in ci.yml
        python-version: ["3.11"]  # Single version for speed

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 1  # Shallow clone for speed

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install uv
      uses: astral-sh/setup-uv@v1
      with:
        version: "latest"

    - name: Cache uv dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/uv
        key: ${{ runner.os }}-python-${{ matrix.python-version }}-uv-${{ hashFiles('**/pyproject.toml', '**/uv.lock') }}
        restore-keys: |
          ${{ runner.os }}-python-${{ matrix.python-version }}-uv-

    - name: Install dependencies
      run: |
        uv venv --python ${{ matrix.python-version }}
        . .venv/bin/activate
        uv pip install -e ".[dev]"
        uv pip install pytest-xdist  # For parallel execution

    - name: Run Python unit tests (parallel)
      run: |
        . .venv/bin/activate
        pytest tests/unit/ \
          -v \
          --tb=short \
          -n ${{ github.event.inputs.parallelism || 'auto' }} \
          --dist loadgroup \
          --maxfail=3 \
          --junitxml=python-unit-test-results.xml
      timeout-minutes: 10

    - name: Upload Python test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: python-unit-test-results
        path: python-unit-test-results.xml
        retention-days: 7

  # Rust unit tests with cargo test
  rust-unit-tests:
    name: Rust Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20  # Rust builds can take longer

    strategy:
      fail-fast: true  # Stop on first failure

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - name: Install Rust toolchain
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        profile: minimal
        override: true
        components: rustfmt, clippy

    - name: Cache Rust dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          rust-engine-legacy/target
          src/rust/daemon/target
          src/rust/daemon/core/target
        key: ${{ runner.os }}-rust-unit-${{ hashFiles('**/Cargo.lock', '**/Cargo.toml') }}
        restore-keys: |
          ${{ runner.os }}-rust-unit-
          ${{ runner.os }}-rust-

    - name: Check Rust formatting
      run: |
        cd rust-engine-legacy && cargo fmt --check || true
        cd ../src/rust/daemon && cargo fmt --check || true
      continue-on-error: true  # Don't fail on formatting in unit tests

    - name: Run Rust clippy
      run: |
        echo "Running clippy on rust-engine-legacy..."
        cd rust-engine-legacy && cargo clippy --all-targets --all-features -- -D warnings || true

        echo "Running clippy on daemon..."
        cd ../src/rust/daemon && cargo clippy --all-targets --all-features -- -D warnings || true
      continue-on-error: true  # Don't fail on clippy warnings in unit tests

    - name: Run rust-engine-legacy unit tests
      run: |
        cd rust-engine-legacy
        cargo test --lib --bins --tests --all-features -- --test-threads=4 --nocapture
      timeout-minutes: 10

    - name: Run daemon core unit tests
      run: |
        cd src/rust/daemon/core
        cargo test --lib --all-features -- --test-threads=4 --nocapture
      timeout-minutes: 8

    - name: Run daemon shared-test-utils tests
      run: |
        cd src/rust/daemon/shared-test-utils
        cargo test --lib --all-features -- --test-threads=4 --nocapture
      timeout-minutes: 5
      continue-on-error: true  # Shared test utils might not have tests

    - name: Upload Rust test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: rust-unit-test-results
        path: |
          rust-engine-legacy/target/debug/
          src/rust/daemon/target/debug/
        retention-days: 3
        if-no-files-found: ignore

  # Aggregate test results
  test-summary:
    name: Unit Test Summary
    runs-on: ubuntu-latest
    needs: [python-unit-tests, rust-unit-tests]
    if: always()
    timeout-minutes: 5

    steps:
    - name: Download Python test results
      uses: actions/download-artifact@v4
      with:
        name: python-unit-test-results
        path: test-results/
      continue-on-error: true

    - name: Check test status
      run: |
        echo "## Unit Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "### Job Status" >> $GITHUB_STEP_SUMMARY
        echo "- Python Unit Tests: ${{ needs.python-unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Rust Unit Tests: ${{ needs.rust-unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Check overall status
        if [[ "${{ needs.python-unit-tests.result }}" == "success" ]] && \
           [[ "${{ needs.rust-unit-tests.result }}" == "success" ]]; then
          echo "✅ **All unit tests passed!**" >> $GITHUB_STEP_SUMMARY
          exit 0
        elif [[ "${{ needs.python-unit-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.rust-unit-tests.result }}" == "failure" ]]; then
          echo "❌ **Some unit tests failed**" >> $GITHUB_STEP_SUMMARY
          exit 1
        else
          echo "⚠️ **Tests completed with warnings**" >> $GITHUB_STEP_SUMMARY
          exit 0
        fi

    - name: Parse Python test results
      if: always()
      run: |
        if [ -f test-results/python-unit-test-results.xml ]; then
          echo "### Python Test Results" >> $GITHUB_STEP_SUMMARY

          # Parse JUnit XML for test counts
          python3 << 'EOF'
import xml.etree.ElementTree as ET
import sys

try:
    tree = ET.parse('test-results/python-unit-test-results.xml')
    root = tree.getroot()

    # Get test statistics
    tests = root.attrib.get('tests', '0')
    failures = root.attrib.get('failures', '0')
    errors = root.attrib.get('errors', '0')
    skipped = root.attrib.get('skipped', '0')
    time = root.attrib.get('time', '0')

    print(f"- **Total tests:** {tests}")
    print(f"- **Passed:** {int(tests) - int(failures) - int(errors) - int(skipped)}")
    print(f"- **Failed:** {failures}")
    print(f"- **Errors:** {errors}")
    print(f"- **Skipped:** {skipped}")
    print(f"- **Duration:** {float(time):.2f}s")

except Exception as e:
    print(f"Could not parse test results: {e}")
    sys.exit(0)
EOF
        fi >> $GITHUB_STEP_SUMMARY || echo "Could not parse Python test results" >> $GITHUB_STEP_SUMMARY

  # Fast linting job (runs in parallel with tests)
  quick-lint:
    name: Quick Lint Check
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"
        cache: 'pip'

    - name: Install uv
      uses: astral-sh/setup-uv@v1

    - name: Install dependencies
      run: |
        uv venv --python 3.11
        . .venv/bin/activate
        uv pip install ruff

    - name: Run ruff check
      run: |
        . .venv/bin/activate
        ruff check src/ tests/ --output-format=github
      continue-on-error: true  # Don't block on lint errors

    - name: Run ruff format check
      run: |
        . .venv/bin/activate
        ruff format --check src/ tests/
      continue-on-error: true  # Don't block on format errors
