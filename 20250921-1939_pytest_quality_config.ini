# Enhanced pytest configuration for Quality Loop Framework
# This configuration optimizes pytest for iterative testing cycles
# with better reporting, parallel execution, and test isolation

[tool:pytest]
# Test discovery and execution
testpaths = tests
pythonpath = .
minversion = 7.0

# Parallel execution with pytest-xdist for faster iterations
addopts =
    -ra
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --cov=src/python
    --cov-report=term-missing:skip-covered
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-branch
    --cov-fail-under=0
    --benchmark-skip
    --durations=10
    --durations-min=1.0

# Enhanced markers for precise test categorization
markers =
    unit: Unit tests for individual components
    integration: Integration tests between components
    e2e: End-to-end tests for complete workflows
    slow: Tests that take more than 30 seconds
    performance: Performance and benchmark tests
    requires_qdrant: Tests requiring Qdrant server connection
    requires_git: Tests requiring Git repository
    requires_network: Tests requiring network connectivity
    requires_docker: Tests requiring Docker environment
    regression: Regression tests for bug fixes
    smoke: Smoke tests for basic functionality
    critical: Critical path tests that must always pass
    flaky: Tests known to be flaky (should be fixed)
    memory_intensive: Tests that use significant memory
    cpu_intensive: Tests that use significant CPU
    timeout_sensitive: Tests sensitive to timing issues
    platform_specific: Tests that only run on specific platforms

# Timeout settings to prevent hanging tests
timeout = 300
timeout_method = thread

# Filter warnings to reduce noise during iterations
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning:testcontainers
    ignore::pytest.PytestUnraisableExceptionWarning
    ignore::ResourceWarning
    error::workspace_qdrant_mcp.*:UserWarning
    error::wqm_cli.*:UserWarning
    error::common.*:UserWarning

# Asyncio configuration for async tests
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Test collection settings
collect_ignore = [
    "setup.py",
    "build",
    "dist",
    ".venv",
    "node_modules"
]

# Xfail configuration
xfail_strict = false

# Test log configuration
log_cli = false
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d %(funcName)s(): %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Coverage configuration for iterative improvement
[coverage:run]
source = src/python
omit =
    */tests/*
    */__init__.py
    */conftest.py
    */test_*.py
    */.venv/*
    */build/*
    */dist/*
    src/python/wqm_cli/cli/test_*.py
    src/python/workspace_qdrant_mcp/tools/test_*.py

branch = true
parallel = true
data_file = .coverage

# Dynamic contexts for better analysis
dynamic_contexts = test_function

[coverage:report]
show_missing = true
skip_covered = false
skip_empty = false
precision = 2
sort = Cover
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if False:
    if __name__ == .__main__.:
    pass
    @abstract
    @abstractmethod
    @pytest.fixture
    # pragma: no cover
    except ImportError:
    except ModuleNotFoundError:
    # TYPE_CHECKING
    if TYPE_CHECKING:

fail_under = 0  # Let quality loop handle failure thresholds

[coverage:html]
directory = htmlcov
skip_covered = false
skip_empty = false
show_contexts = true
title = Quality Loop Coverage Report

[coverage:xml]
output = coverage.xml

# JSON report for programmatic analysis
[coverage:json]
output = coverage.json
show_contexts = true