#!/usr/bin/env python3
"""
Comprehensive Testing Campaign Orchestrator
===========================================

Task #151 - Complete testing infrastructure integration and demonstration

This script orchestrates the complete testing infrastructure including:
1. Infrastructure setup and validation
2. Test data generation and management  
3. Multi-phase validation execution
4. Safety monitoring and emergency procedures
5. Performance monitoring and metrics collection
6. Comprehensive reporting and analysis

This serves as the foundation for Tasks 152-170 of the comprehensive testing campaign.

Usage:
    python 20250107-0902_run_comprehensive_tests.py --full-campaign
    python 20250107-0902_run_comprehensive_tests.py --phase lsp_integration
    python 20250107-0902_run_comprehensive_tests.py --validation-only
    python 20250107-0902_run_comprehensive_tests.py --report-only
"""

import asyncio
import json
import argparse
import sys
import time
import traceback
import importlib.util
from pathlib import Path
from typing import Dict, List, Any, Optional

# Dynamic imports for our testing infrastructure modules
def load_module_from_file(module_name: str, file_path: str):
    """Dynamically load a module from a file path."""
    spec = importlib.util.spec_from_file_location(module_name, file_path)
    if spec is None or spec.loader is None:
        raise ImportError(f"Could not load module {module_name} from {file_path}")
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module

# Load our testing infrastructure modules
try:
    current_dir = Path(__file__).parent
    
    # Load infrastructure module
    infra_module = load_module_from_file(
        "test_infrastructure",
        str(current_dir / "20250107-0900_comprehensive_testing_infrastructure.py")
    )
    
    # Load validation framework module
    validation_module = load_module_from_file(
        "test_validation_framework", 
        str(current_dir / "20250107-0901_test_validation_framework.py")
    )
    
    # Import required classes
    ComprehensiveTestingInfrastructure = infra_module.ComprehensiveTestingInfrastructure
    TestPhase = infra_module.TestPhase
    TestStatus = infra_module.TestStatus
    TestEnvironmentManager = infra_module.TestEnvironmentManager
    SafetyMonitor = infra_module.SafetyMonitor
    PerformanceMonitor = infra_module.PerformanceMonitor
    
    ValidationFramework = validation_module.ValidationFramework
    ValidationLevel = validation_module.ValidationLevel
    ValidationResult = validation_module.ValidationResult
    create_sample_test_queries = validation_module.create_sample_test_queries
    
    print("‚úÖ Testing infrastructure modules loaded successfully")
    
except Exception as e:
    print(f"‚ùå Import Error: {e}")
    print("Make sure the infrastructure files are in the same directory:")
    print("  - 20250107-0900_comprehensive_testing_infrastructure.py")
    print("  - 20250107-0901_test_validation_framework.py")
    sys.exit(1)

class ComprehensiveTester:
    """Main orchestrator for the comprehensive testing campaign."""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or self._get_default_campaign_config()
        self.infrastructure = None
        self.validator = None
        self.results = []
        self.campaign_start_time = time.time()
        
    def _get_default_campaign_config(self) -> Dict[str, Any]:
        """Get default configuration for comprehensive testing campaign."""
        return {
            "campaign": {
                "name": "workspace-qdrant-mcp Comprehensive Testing Campaign",
                "version": "1.0.0",
                "phases": [
                    TestPhase.INFRASTRUCTURE.value,
                    TestPhase.DATA_GENERATION.value,
                    TestPhase.LSP_INTEGRATION.value,
                    TestPhase.INGESTION_CAPABILITIES.value,
                    TestPhase.RETRIEVAL_ACCURACY.value,
                    TestPhase.AUTOMATION.value,
                    TestPhase.PERFORMANCE.value
                ]
            },
            "infrastructure": {
                "safety_monitoring": True,
                "performance_monitoring": True,
                "emergency_procedures": True,
                "data_profile": "comprehensive"
            },
            "validation": {
                "level": "standard",
                "accuracy_threshold": 0.8,
                "performance_threshold_ms": 1000.0,
                "memory_threshold_mb": 200.0,
                "continue_on_failure": True
            },
            "reporting": {
                "detailed_results": True,
                "performance_metrics": True,
                "recommendations": True,
                "export_json": True,
                "export_html": False
            }
        }
        
    async def initialize_campaign(self) -> bool:
        """Initialize the comprehensive testing campaign."""
        print("üöÄ Initializing Comprehensive Testing Campaign")
        print("=" * 60)
        
        try:
            # Initialize testing infrastructure
            print("üìã Setting up testing infrastructure...")
            self.infrastructure = ComprehensiveTestingInfrastructure(
                self.config.get("infrastructure", {})
            )
            
            setup_success = await self.infrastructure.setup_infrastructure()
            if not setup_success:
                print("‚ùå Infrastructure setup failed")
                return False
                
            print("‚úÖ Testing infrastructure ready")
            
            # Initialize validation framework
            print("üîç Setting up validation framework...")
            validation_level = ValidationLevel(self.config["validation"]["level"])
            self.validator = ValidationFramework(
                env_manager=self.infrastructure.env_manager,
                validation_level=validation_level
            )
            
            print("‚úÖ Validation framework ready")
            
            # Display campaign overview
            self._display_campaign_overview()
            
            return True
            
        except Exception as e:
            print(f"‚ùå Campaign initialization failed: {e}")
            print(traceback.format_exc())
            return False
            
    def _display_campaign_overview(self):
        """Display campaign configuration overview."""
        print("\nüìä Campaign Overview")
        print("-" * 40)
        print(f"Campaign: {self.config['campaign']['name']}")
        print(f"Version: {self.config['campaign']['version']}")
        print(f"Phases: {len(self.config['campaign']['phases'])}")
        print(f"Validation Level: {self.config['validation']['level']}")
        print(f"Data Profile: {self.config['infrastructure']['data_profile']}")
        print(f"Safety Monitoring: {'Enabled' if self.config['infrastructure']['safety_monitoring'] else 'Disabled'}")
        print(f"Test Directory: {self.infrastructure.env_manager.base_dir}")
        
    async def run_full_campaign(self) -> Dict[str, Any]:
        """Run the complete comprehensive testing campaign."""
        print("\nüéØ Starting Full Testing Campaign")
        print("=" * 50)
        
        campaign_results = {
            "campaign_info": self.config["campaign"],
            "start_time": self.campaign_start_time,
            "phases_completed": [],
            "phases_failed": [],
            "total_tests": 0,
            "tests_passed": 0,
            "overall_success": False
        }
        
        try:
            # Run each phase sequentially
            for phase_name in self.config["campaign"]["phases"]:
                phase = TestPhase(phase_name)
                print(f"\nüîÑ Executing Phase: {phase.value.upper()}")
                print("-" * 30)
                
                phase_results = await self._run_phase(phase)
                
                if phase_results["success"]:
                    campaign_results["phases_completed"].append(phase_name)
                    print(f"‚úÖ Phase {phase.value} completed successfully")
                else:
                    campaign_results["phases_failed"].append(phase_name)
                    print(f"‚ùå Phase {phase.value} failed")
                    
                    if not self.config["validation"]["continue_on_failure"]:
                        print("üõë Stopping campaign due to phase failure")
                        break
                        
                campaign_results["total_tests"] += phase_results["total_tests"]
                campaign_results["tests_passed"] += phase_results["tests_passed"]
                
            # Calculate overall success
            campaign_results["overall_success"] = (
                len(campaign_results["phases_failed"]) == 0 or
                (len(campaign_results["phases_completed"]) > len(campaign_results["phases_failed"]))
            )
            
            campaign_results["end_time"] = time.time()
            campaign_results["duration"] = campaign_results["end_time"] - campaign_results["start_time"]
            
            # Generate final campaign report
            await self._generate_campaign_report(campaign_results)
            
            return campaign_results
            
        except Exception as e:
            print(f"üí• Campaign execution failed: {e}")
            campaign_results["error"] = str(e)
            campaign_results["overall_success"] = False
            return campaign_results
            
    async def _run_phase(self, phase: TestPhase) -> Dict[str, Any]:
        """Run a specific testing phase."""
        phase_start = time.time()
        
        try:
            if phase == TestPhase.INFRASTRUCTURE:
                return await self._run_infrastructure_phase()
            elif phase == TestPhase.DATA_GENERATION:
                return await self._run_data_generation_phase()
            elif phase == TestPhase.LSP_INTEGRATION:
                return await self._run_lsp_integration_phase()
            elif phase == TestPhase.INGESTION_CAPABILITIES:
                return await self._run_ingestion_phase()
            elif phase == TestPhase.RETRIEVAL_ACCURACY:
                return await self._run_retrieval_phase()
            elif phase == TestPhase.AUTOMATION:
                return await self._run_automation_phase()
            elif phase == TestPhase.PERFORMANCE:
                return await self._run_performance_phase()
            else:
                print(f"‚ö†Ô∏è  Phase {phase.value} not implemented yet")
                return {
                    "success": False,
                    "total_tests": 0,
                    "tests_passed": 0,
                    "error": f"Phase {phase.value} not implemented"
                }
                
        except Exception as e:
            print(f"üí• Phase {phase.value} execution failed: {e}")
            return {
                "success": False,
                "total_tests": 0,
                "tests_passed": 0,
                "error": str(e),
                "duration": time.time() - phase_start
            }
            
    async def _run_infrastructure_phase(self) -> Dict[str, Any]:
        """Run infrastructure validation phase."""
        print("   üîß Running infrastructure tests...")
        
        # Run infrastructure validation tests
        infra_results = await self.infrastructure.run_basic_infrastructure_tests()
        
        passed = sum(1 for r in infra_results if r.status == TestStatus.PASSED)
        total = len(infra_results)
        
        print(f"   üìä Infrastructure: {passed}/{total} tests passed")
        
        return {
            "success": passed == total,
            "total_tests": total,
            "tests_passed": passed,
            "results": infra_results
        }
        
    async def _run_data_generation_phase(self) -> Dict[str, Any]:
        """Run test data generation phase."""
        print("   üìÅ Generating comprehensive test data...")
        
        try:
            # Generate test data based on profile
            data_profile = self.config["infrastructure"]["data_profile"]
            generated_files = self.infrastructure.env_manager.generate_test_data(data_profile)
            
            total_files = sum(len(files) for files in generated_files.values())
            
            # Validate generated data
            validation_passed = True
            for category, files in generated_files.items():
                if not files:
                    validation_passed = False
                    print(f"   ‚ùå No files generated for category: {category}")
                else:
                    print(f"   ‚úÖ {category}: {len(files)} files generated")
                    
            print(f"   üìä Data Generation: {total_files} total files, validation {'passed' if validation_passed else 'failed'}")
            
            return {
                "success": validation_passed and total_files > 0,
                "total_tests": 1,
                "tests_passed": 1 if validation_passed else 0,
                "generated_files": generated_files,
                "total_files": total_files
            }
            
        except Exception as e:
            print(f"   ‚ùå Data generation failed: {e}")
            return {
                "success": False,
                "total_tests": 1,
                "tests_passed": 0,
                "error": str(e)
            }
            
    async def _run_lsp_integration_phase(self) -> Dict[str, Any]:
        """Run LSP integration testing phase."""
        print("   üîå Running LSP integration tests...")
        
        # Run LSP integration validation
        lsp_results = await self.validator.validate_phase(TestPhase.LSP_INTEGRATION)
        
        passed = sum(1 for r in lsp_results if r.passed)
        total = len(lsp_results)
        
        for result in lsp_results:
            status_emoji = "‚úÖ" if result.passed else "‚ùå"
            print(f"   {status_emoji} {result.test_id}: {result.accuracy_score:.2f} accuracy, {result.performance_ms:.1f}ms")
            
        print(f"   üìä LSP Integration: {passed}/{total} validations passed")
        
        return {
            "success": passed == total,
            "total_tests": total,
            "tests_passed": passed,
            "validation_results": lsp_results
        }
        
    async def _run_ingestion_phase(self) -> Dict[str, Any]:
        """Run ingestion capabilities testing phase."""
        print("   üì§ Running ingestion capability tests...")
        
        # Prepare test data
        test_data = {"generated_files": self.infrastructure.env_manager.generate_test_data("minimal")}
        
        # Run ingestion validation
        ingestion_results = await self.validator.validate_phase(
            TestPhase.INGESTION_CAPABILITIES, 
            test_data
        )
        
        passed = sum(1 for r in ingestion_results if r.passed)
        total = len(ingestion_results)
        
        for result in ingestion_results:
            status_emoji = "‚úÖ" if result.passed else "‚ùå"
            print(f"   {status_emoji} {result.test_id}: {result.accuracy_score:.2f} accuracy, {result.performance_ms:.1f}ms")
            
        print(f"   üìä Ingestion: {passed}/{total} validations passed")
        
        return {
            "success": passed == total,
            "total_tests": total,
            "tests_passed": passed,
            "validation_results": ingestion_results
        }
        
    async def _run_retrieval_phase(self) -> Dict[str, Any]:
        """Run retrieval accuracy testing phase."""
        print("   üîç Running retrieval accuracy tests...")
        
        # Prepare test queries
        test_data = {"test_queries": create_sample_test_queries()}
        
        # Run retrieval validation
        retrieval_results = await self.validator.validate_phase(
            TestPhase.RETRIEVAL_ACCURACY,
            test_data
        )
        
        passed = sum(1 for r in retrieval_results if r.passed)
        total = len(retrieval_results)
        
        for result in retrieval_results:
            status_emoji = "‚úÖ" if result.passed else "‚ùå"
            print(f"   {status_emoji} {result.test_id}: {result.accuracy_score:.2f} accuracy, {result.performance_ms:.1f}ms")
            
        print(f"   üìä Retrieval: {passed}/{total} validations passed")
        
        return {
            "success": passed == total,
            "total_tests": total,
            "tests_passed": passed,
            "validation_results": retrieval_results
        }
        
    async def _run_automation_phase(self) -> Dict[str, Any]:
        """Run automation testing phase."""
        print("   ü§ñ Running automation tests...")
        
        # Run automation validation
        automation_results = await self.validator.validate_phase(TestPhase.AUTOMATION)
        
        passed = sum(1 for r in automation_results if r.passed)
        total = len(automation_results)
        
        for result in automation_results:
            status_emoji = "‚úÖ" if result.passed else "‚ùå"
            print(f"   {status_emoji} {result.test_id}: {result.accuracy_score:.2f} accuracy, {result.performance_ms:.1f}ms")
            
        print(f"   üìä Automation: {passed}/{total} validations passed")
        
        return {
            "success": passed == total,
            "total_tests": total,
            "tests_passed": passed,
            "validation_results": automation_results
        }
        
    async def _run_performance_phase(self) -> Dict[str, Any]:
        """Run performance testing phase."""
        print("   ‚ö° Running performance tests...")
        
        try:
            # Start performance monitoring
            perf_monitor = PerformanceMonitor(collection_interval=0.1)
            perf_monitor.start_monitoring()
            
            # Simulate performance testing workload
            performance_tests = [
                {"name": "concurrent_search", "duration": 2.0},
                {"name": "bulk_ingestion", "duration": 3.0},
                {"name": "memory_stress", "duration": 1.5},
                {"name": "cpu_intensive", "duration": 2.5}
            ]
            
            test_results = []
            for test in performance_tests:
                test_start = time.time()
                
                # Simulate test execution
                await asyncio.sleep(test["duration"] / 10)  # Scaled down for demo
                
                test_duration = time.time() - test_start
                success = test_duration < test["duration"]  # Should complete in expected time
                
                test_results.append({
                    "name": test["name"],
                    "duration": test_duration,
                    "expected_duration": test["duration"],
                    "success": success
                })
                
                status_emoji = "‚úÖ" if success else "‚ùå"
                print(f"   {status_emoji} {test['name']}: {test_duration:.3f}s (expected: {test['duration']}s)")
                
            # Stop monitoring and get metrics
            metrics = perf_monitor.stop_monitoring()
            
            passed = sum(1 for r in test_results if r["success"])
            total = len(test_results)
            
            print(f"   üìä Performance: {passed}/{total} tests passed")
            print(f"   üî¨ Avg CPU: {metrics.avg_cpu_usage:.1f}%, Peak Memory: {metrics.peak_memory_usage:.1f}%")
            
            return {
                "success": passed == total,
                "total_tests": total,
                "tests_passed": passed,
                "performance_metrics": {
                    "duration": metrics.duration,
                    "avg_cpu_usage": metrics.avg_cpu_usage,
                    "peak_memory_usage": metrics.peak_memory_usage
                },
                "test_results": test_results
            }
            
        except Exception as e:
            print(f"   ‚ùå Performance testing failed: {e}")
            return {
                "success": False,
                "total_tests": len(performance_tests) if 'performance_tests' in locals() else 0,
                "tests_passed": 0,
                "error": str(e)
            }
            
    async def _generate_campaign_report(self, results: Dict[str, Any]):
        """Generate comprehensive campaign report."""
        print("\nüìã Generating Campaign Report")
        print("-" * 40)
        
        try:
            # Generate validation report if validator is available
            validation_report = {}
            if self.validator and self.validator.validation_results:
                validation_report = self.validator.generate_validation_report()
                
            # Combine results
            final_report = {
                "campaign_results": results,
                "validation_report": validation_report,
                "infrastructure_info": {
                    "test_directory": str(self.infrastructure.env_manager.base_dir),
                    "safety_monitoring": self.config["infrastructure"]["safety_monitoring"],
                    "performance_monitoring": self.config["infrastructure"]["performance_monitoring"]
                },
                "recommendations": self._generate_campaign_recommendations(results)
            }
            
            # Save report
            report_file = (self.infrastructure.env_manager.results_dir / 
                          f"comprehensive_campaign_report_{int(time.time())}.json")
            
            with open(report_file, 'w') as f:
                json.dump(final_report, f, indent=2)
                
            print(f"‚úÖ Campaign report saved to: {report_file}")
            
            # Display summary
            self._display_campaign_summary(results, validation_report)
            
        except Exception as e:
            print(f"‚ùå Report generation failed: {e}")
            
    def _generate_campaign_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on campaign results."""
        recommendations = []
        
        success_rate = (results["tests_passed"] / results["total_tests"] * 100) if results["total_tests"] > 0 else 0
        
        if success_rate >= 95:
            recommendations.append("Excellent performance! System is ready for production deployment.")
        elif success_rate >= 80:
            recommendations.append("Good performance. Consider addressing failed tests before production.")
            recommendations.append("Focus on improving accuracy in underperforming areas.")
        else:
            recommendations.append("Performance needs improvement. Review failed tests and optimization opportunities.")
            recommendations.append("Consider additional testing and validation before production deployment.")
            
        if results["phases_failed"]:
            recommendations.append(f"Failed phases need attention: {', '.join(results['phases_failed'])}")
            
        if results.get("duration", 0) > 300:  # 5 minutes
            recommendations.append("Consider optimizing test execution time for faster feedback.")
            
        return recommendations
        
    def _display_campaign_summary(self, results: Dict[str, Any], validation_report: Dict[str, Any]):
        """Display campaign summary."""
        print("\nüéØ CAMPAIGN SUMMARY")
        print("=" * 50)
        
        success_emoji = "üéâ" if results["overall_success"] else "‚ö†Ô∏è"
        print(f"{success_emoji} Campaign Status: {'SUCCESS' if results['overall_success'] else 'PARTIAL SUCCESS'}")
        print(f"üìä Overall Results: {results['tests_passed']}/{results['total_tests']} tests passed")
        print(f"‚è±Ô∏è  Duration: {results.get('duration', 0):.1f} seconds")
        print(f"‚úÖ Phases Completed: {len(results['phases_completed'])}")
        
        if results["phases_failed"]:
            print(f"‚ùå Phases Failed: {len(results['phases_failed'])}")
            
        success_rate = (results["tests_passed"] / results["total_tests"] * 100) if results["total_tests"] > 0 else 0
        print(f"üìà Success Rate: {success_rate:.1f}%")
        
        if validation_report.get("validation_summary"):
            val_summary = validation_report["validation_summary"]
            print(f"üîç Validation Score: {val_summary.get('overall_score', 0):.1f}/100")
            print(f"üéØ Accuracy: {val_summary.get('overall_accuracy', 0):.2f}")
            print(f"‚ö° Performance: {val_summary.get('overall_performance_ms', 0):.1f}ms avg")
            
    async def shutdown(self):
        """Graceful shutdown of the testing campaign."""
        print("\nüîÑ Shutting down testing campaign...")
        
        if self.infrastructure:
            await self.infrastructure.shutdown()
            
        print("‚úÖ Campaign shutdown complete")

async def main():
    """Main entry point for comprehensive testing campaign."""
    parser = argparse.ArgumentParser(
        description="Comprehensive Testing Campaign for workspace-qdrant-mcp"
    )
    
    parser.add_argument("--full-campaign", action="store_true", 
                       help="Run the complete comprehensive testing campaign")
    parser.add_argument("--phase", choices=[p.value for p in TestPhase],
                       help="Run a specific testing phase")
    parser.add_argument("--validation-only", action="store_true",
                       help="Run only validation tests")
    parser.add_argument("--report-only", action="store_true",
                       help="Generate report from existing results")
    parser.add_argument("--config", type=str,
                       help="Path to custom configuration file")
    parser.add_argument("--data-profile", choices=["minimal", "comprehensive", "stress"],
                       default="comprehensive", help="Test data generation profile")
    parser.add_argument("--validation-level", choices=[v.value for v in ValidationLevel],
                       default="standard", help="Validation rigor level")
    
    args = parser.parse_args()
    
    # Load configuration
    config = None
    if args.config and Path(args.config).exists():
        try:
            with open(args.config, 'r') as f:
                config = json.load(f)
        except Exception as e:
            print(f"‚ùå Error loading config: {e}")
            return 1
            
    # Initialize tester
    tester = ComprehensiveTester(config)
    
    # Update config with command line arguments
    if not config:
        tester.config["infrastructure"]["data_profile"] = args.data_profile
        tester.config["validation"]["level"] = args.validation_level
        
    try:
        # Initialize campaign
        if not await tester.initialize_campaign():
            print("‚ùå Campaign initialization failed")
            return 1
            
        # Execute based on arguments
        if args.full_campaign:
            print("üöÄ Running Full Comprehensive Testing Campaign")
            results = await tester.run_full_campaign()
            
            if results["overall_success"]:
                print("\nüéâ COMPREHENSIVE TESTING CAMPAIGN COMPLETED SUCCESSFULLY!")
                return 0
            else:
                print("\n‚ö†Ô∏è  COMPREHENSIVE TESTING CAMPAIGN COMPLETED WITH ISSUES")
                return 1
                
        elif args.phase:
            phase = TestPhase(args.phase)
            print(f"üîÑ Running Phase: {phase.value}")
            results = await tester._run_phase(phase)
            
            if results["success"]:
                print(f"‚úÖ Phase {phase.value} completed successfully")
                return 0
            else:
                print(f"‚ùå Phase {phase.value} failed")
                return 1
                
        elif args.validation_only:
            print("üîç Running Validation Tests Only")
            # Run minimal infrastructure setup
            validation_results = []
            
            for phase in [TestPhase.LSP_INTEGRATION, TestPhase.INGESTION_CAPABILITIES, 
                         TestPhase.RETRIEVAL_ACCURACY, TestPhase.AUTOMATION]:
                results = await tester.validator.validate_phase(phase)
                validation_results.extend(results)
                
            passed = sum(1 for r in validation_results if r.passed)
            total = len(validation_results)
            
            print(f"üìä Validation Results: {passed}/{total} tests passed")
            
            if passed == total:
                print("‚úÖ All validation tests passed!")
                return 0
            else:
                print("‚ö†Ô∏è  Some validation tests failed")
                return 1
                
        elif args.report_only:
            print("üìã Generating Report from Existing Results")
            # This would load existing results and generate a report
            # For now, just show that the infrastructure is working
            print("‚úÖ Report generation infrastructure ready")
            print(f"üìÅ Test directory: {tester.infrastructure.env_manager.base_dir}")
            return 0
            
        else:
            parser.print_help()
            return 0
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Campaign interrupted by user")
        await tester.shutdown()
        return 1
        
    except Exception as e:
        print(f"\nüí• Fatal error: {e}")
        print(traceback.format_exc())
        await tester.shutdown()
        return 1
        
    finally:
        await tester.shutdown()

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)