//! Property-based testing for edge cases and data validation using proptest
//!
//! This module uses proptest to generate random test data and validate robustness
//! of file processing, data serialization, memory bounds, and error handling
//! across all possible input scenarios.

use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::time::Duration;

use proptest::prelude::*;
use proptest::test_runner::TestCaseError;
use tempfile::{NamedTempFile, TempDir};
use tokio::runtime::Runtime;
use uuid::Uuid;

// Import core components
use workspace_qdrant_core::{
    DocumentProcessor, DocumentType, DocumentContent, TextChunk, ChunkingConfig,
    ProcessingError, ProcessingEngine, TaskPriority,
    EmbeddingGenerator, EmbeddingConfig, EmbeddingResult, EmbeddingError,
    DenseEmbedding, SparseEmbedding,
    config::Config,
    error::{WorkspaceError, ErrorSeverity, ErrorRecoveryStrategy},
    logging::{PerformanceMetrics, track_async_operation},
};

// Import shared test utilities
use shared_test_utils::{
    test_helpers::init_test_tracing,
    TestResult,
};

// ============================================================================
// CUSTOM PROPTEST GENERATORS
// ============================================================================

/// Generate random file content with various encodings and formats
prop_compose! {
    fn arb_file_content()(
        content_type in prop_oneof![
            "text", "binary", "mixed", "unicode", "empty", "large"
        ],
        size in 0..100000usize,
    ) -> String {
        match content_type.as_str() {
            "text" => {
                let chars: Vec<char> = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 \n\t.,!?;:-_()[]{}\"'@#$%^&*+=|\\/<>~`"
                    .chars().collect();
                (0..size).map(|_| chars[fastrand::usize(..chars.len())]).collect()
            },
            "binary" => {
                (0..size).map(|_| fastrand::u8(..) as char).collect()
            },
            "mixed" => {
                let mut content = String::new();
                for _ in 0..size {
                    match fastrand::u8(..4) {
                        0 => content.push_str("Hello, World! "),
                        1 => content.push(fastrand::u8(..) as char),
                        2 => content.push_str("ðŸš€ðŸ”¥ðŸ’¯\n"),
                        _ => content.push_str(&format!("Number: {}\t", fastrand::u64(..))),
                    }
                }
                content
            },
            "unicode" => {
                let unicode_ranges = [
                    0x0000..0x007F,  // Basic Latin
                    0x0080..0x00FF,  // Latin-1 Supplement
                    0x0100..0x017F,  // Latin Extended-A
                    0x1F600..0x1F64F, // Emoticons
                    0x4E00..0x9FFF,  // CJK Unified Ideographs
                ];
                (0..size).map(|_| {
                    let range = &unicode_ranges[fastrand::usize(..unicode_ranges.len())];
                    let code_point = fastrand::u32(range.clone());
                    std::char::from_u32(code_point).unwrap_or('?')
                }).collect()
            },
            "empty" => String::new(),
            "large" => {
                let base_text = "This is a large document with repeated content. ";
                base_text.repeat(size / base_text.len() + 1)
            },
            _ => "default content".to_string()
        }
    }
}

/// Generate random document types
fn arb_document_type() -> impl Strategy<Value = DocumentType> {
    prop_oneof![
        Just(DocumentType::Pdf),
        Just(DocumentType::Epub),
        Just(DocumentType::Docx),
        Just(DocumentType::Text),
        Just(DocumentType::Markdown),
        any::<String>().prop_map(DocumentType::Code),
        Just(DocumentType::Unknown),
    ]
}

/// Generate random chunking configurations
prop_compose! {
    fn arb_chunking_config()(
        chunk_size in 1..10000usize,
        overlap_size in 0..1000usize,
        preserve_paragraphs in any::<bool>(),
    ) -> ChunkingConfig {
        ChunkingConfig {
            chunk_size,
            overlap_size: std::cmp::min(overlap_size, chunk_size / 2),
            preserve_paragraphs,
        }
    }
}

/// Generate random file extensions
fn arb_file_extension() -> impl Strategy<Value = String> {
    prop_oneof![
        "txt", "md", "pdf", "docx", "epub", "rs", "py", "js", "json", "yaml", "xml",
        "html", "css", "cpp", "java", "go", "rb", "php", "sh", "sql", "toml", "unknown"
    ].prop_map(|s| s.to_string())
}

/// Generate malformed or edge case file content
prop_compose! {
    fn arb_malformed_content()(
        corruption_type in prop_oneof![
            "truncated", "oversized", "null_bytes", "invalid_utf8",
            "control_chars", "bom", "mixed_encodings"
        ],
        base_content in arb_file_content(),
    ) -> Vec<u8> {
        match corruption_type.as_str() {
            "truncated" => base_content.as_bytes()[..base_content.len() / 2].to_vec(),
            "oversized" => {
                let mut content = base_content.into_bytes();
                content.extend(vec![b'X'; 10_000_000]); // 10MB of X's
                content
            },
            "null_bytes" => {
                let mut content = base_content.into_bytes();
                for i in (0..content.len()).step_by(10) {
                    if i < content.len() {
                        content[i] = 0;
                    }
                }
                content
            },
            "invalid_utf8" => vec![0xFF, 0xFE, 0xFD, 0xFC, 0x80, 0x81, 0x82],
            "control_chars" => (0..256).map(|i| (i % 32) as u8).collect(),
            "bom" => {
                let mut content = vec![0xEF, 0xBB, 0xBF]; // UTF-8 BOM
                content.extend(base_content.into_bytes());
                content
            },
            "mixed_encodings" => {
                let mut content = base_content.into_bytes();
                // Add some Latin-1 bytes that aren't valid UTF-8
                content.extend(&[0xC0, 0xC1, 0xF5, 0xF6, 0xF7, 0xF8, 0xF9, 0xFA, 0xFB, 0xFC, 0xFD, 0xFE, 0xFF]);
                content
            },
            _ => base_content.into_bytes()
        }
    }
}

/// Generate random vector dimensions for embedding tests
prop_compose! {
    fn arb_embedding_dimensions()(
        dims in prop_oneof![
            1..10usize,      // Very small
            384..385usize,   // Common size
            768..769usize,   // Common size
            1536..1537usize, // Common size
            10000..50000usize, // Very large
        ]
    ) -> usize {
        dims
    }
}

/// Generate random error scenarios
#[derive(Debug, Clone)]
pub enum ErrorScenario {
    NetworkTimeout,
    DiskFull,
    PermissionDenied,
    CorruptedData,
    OutOfMemory,
    InvalidFormat,
    ResourceExhausted,
    ConcurrencyLimit,
}

impl Arbitrary for ErrorScenario {
    type Parameters = ();
    type Strategy = BoxedStrategy<Self>;

    fn arbitrary_with(_args: ()) -> Self::Strategy {
        prop_oneof![
            Just(ErrorScenario::NetworkTimeout),
            Just(ErrorScenario::DiskFull),
            Just(ErrorScenario::PermissionDenied),
            Just(ErrorScenario::CorruptedData),
            Just(ErrorScenario::OutOfMemory),
            Just(ErrorScenario::InvalidFormat),
            Just(ErrorScenario::ResourceExhausted),
            Just(ErrorScenario::ConcurrencyLimit),
        ].boxed()
    }
}

// ============================================================================
// FILE PROCESSING PROPERTY TESTS
// ============================================================================

/// Property test: Document processing should handle any valid file content
proptest! {
    #[test]
    fn prop_document_processing_robustness(
        content in arb_file_content(),
        extension in arb_file_extension(),
        chunking_config in arb_chunking_config(),
    ) {
        let rt = Runtime::new().unwrap();
        rt.block_on(async {
            init_test_tracing();

            // Create processor with random chunking config
            let processor = DocumentProcessor::with_chunking_config(chunking_config);

            // Create temporary file with random content
            let temp_file = NamedTempFile::with_suffix(&format!(".{}", extension)).unwrap();
            tokio::fs::write(temp_file.path(), &content).await.unwrap();

            // Process should either succeed or fail gracefully
            match processor.process_file(temp_file.path(), "test_collection").await {
                Ok(result) => {
                    // Verify result properties
                    assert!(!result.document_id.is_empty());
                    assert_eq!(result.collection, "test_collection");
                    if let Some(chunks) = result.chunks_created {
                        assert!(chunks > 0 || content.is_empty());
                    }
                    assert!(result.processing_time_ms >= 0);
                },
                Err(e) => {
                    // Errors should be well-formed
                    let error_str = e.to_string();
                    assert!(!error_str.is_empty());
                    assert!(!error_str.contains("panicked"));

                    // Log for analysis but don't fail test
                    tracing::debug!("Expected processing error: {}", error_str);
                }
            }
        });
    }
}

/// Property test: Text processing should preserve content integrity
proptest! {
    #[test]
    fn prop_text_processing_integrity(
        content in arb_file_content(),
        extension in arb_file_extension(),
        chunking_config in arb_chunking_config(),
    ) {
        let rt = Runtime::new().unwrap();
        rt.block_on(async {
            init_test_tracing();

            let processor = DocumentProcessor::with_chunking_config(chunking_config.clone());

            // Create temporary file with random content
            let temp_file = NamedTempFile::with_suffix(&format!(".{}", extension)).unwrap();
            if tokio::fs::write(temp_file.path(), &content).await.is_ok() {
                // Test document processing behavior
                match processor.process_file(temp_file.path(), "test_collection").await {
                    Ok(result) => {
                        // Verify result properties
                        assert!(!result.document_id.is_empty());
                        assert_eq!(result.collection, "test_collection");
                        assert!(result.processing_time_ms >= 0);

                        if let Some(chunks) = result.chunks_created {
                            if content.is_empty() {
                                assert!(chunks == 0 || chunks == 1); // Empty content might create one empty chunk
                            } else {
                                assert!(chunks > 0, "Non-empty content should create chunks");
                                // Reasonable chunk count for content size
                                let expected_chunks = (content.len() / chunking_config.chunk_size).max(1);
                                assert!(chunks <= expected_chunks * 3, "Too many chunks: {} vs expected max {}", chunks, expected_chunks * 3);
                            }
                        }
                    },
                    Err(e) => {
                        // Processing errors should be graceful
                        let error_str = e.to_string();
                        assert!(!error_str.contains("panicked"));
                        tracing::debug!("Processing error for content length {}: {}", content.len(), error_str);
                    }
                }
            }
        });
    }
}

/// Property test: Document type detection should be consistent
proptest! {
    #[test]
    fn prop_document_type_detection(
        extension in arb_file_extension(),
        filename_prefix in "[a-zA-Z0-9_-]{1,20}",
    ) {
        let processor = DocumentProcessor::new();
        let filename = format!("{}.{}", filename_prefix, extension);
        let path = Path::new(&filename);

        // Detection should be deterministic
        let detected1 = processor.detect_document_type(path);
        let detected2 = processor.detect_document_type(path);

        match (detected1, detected2) {
            (Ok(type1), Ok(type2)) => {
                assert_eq!(type1, type2, "Document type detection should be deterministic");

                // Verify expected mappings for known extensions
                match extension.as_str() {
                    "pdf" => assert_eq!(type1, DocumentType::Pdf),
                    "epub" => assert_eq!(type1, DocumentType::Epub),
                    "docx" => assert_eq!(type1, DocumentType::Docx),
                    "txt" => assert_eq!(type1, DocumentType::Text),
                    "md" | "markdown" => assert_eq!(type1, DocumentType::Markdown),
                    "rs" => assert_eq!(type1, DocumentType::Code("rust".to_string())),
                    "py" => assert_eq!(type1, DocumentType::Code("python".to_string())),
                    "js" => assert_eq!(type1, DocumentType::Code("javascript".to_string())),
                    "json" => assert_eq!(type1, DocumentType::Code("json".to_string())),
                    _ => {
                        // Unknown extensions should map to Unknown or specific code types
                        match type1 {
                            DocumentType::Unknown | DocumentType::Code(_) => {},
                            _ => panic!("Unexpected document type for extension {}: {:?}", extension, type1),
                        }
                    }
                }
            },
            (Err(_), Err(_)) => {
                // Both should fail consistently (shouldn't happen for simple paths)
            },
            _ => panic!("Inconsistent document type detection results"),
        }
    }
}

// ============================================================================
// DATA SERIALIZATION PROPERTY TESTS
// ============================================================================

/// Property test: Document content structure validation
proptest! {
    #[test]
    fn prop_document_content_structure(
        raw_text in arb_file_content(),
        document_type in arb_document_type(),
        chunk_count in 0..100usize,
    ) {
        // Create a DocumentContent instance
        let mut metadata = HashMap::new();
        metadata.insert("test_key".to_string(), "test_value".to_string());
        metadata.insert("char_count".to_string(), raw_text.len().to_string());

        // Create chunks
        let chunks: Vec<TextChunk> = (0..chunk_count).map(|i| {
            let mut chunk_metadata = HashMap::new();
            chunk_metadata.insert("chunk_index".to_string(), i.to_string());

            TextChunk {
                content: format!("Chunk {} content", i),
                chunk_index: i,
                start_char: i * 100,
                end_char: (i + 1) * 100,
                metadata: chunk_metadata,
            }
        }).collect();

        let document_content = DocumentContent {
            raw_text: raw_text.clone(),
            metadata: metadata.clone(),
            document_type: document_type.clone(),
            chunks: chunks.clone(),
        };

        // Verify structure properties
        assert_eq!(document_content.raw_text, raw_text);
        assert_eq!(document_content.metadata.len(), 2);
        assert_eq!(document_content.chunks.len(), chunk_count);

        // Verify chunk integrity
        for (i, chunk) in document_content.chunks.iter().enumerate() {
            assert_eq!(chunk.chunk_index, i);
            assert!(chunk.end_char >= chunk.start_char);
            assert!(chunk.metadata.contains_key("chunk_index"));
        }

        // Test that structure is logically consistent
        if !document_content.chunks.is_empty() {
            let first_chunk = &document_content.chunks[0];
            let last_chunk = &document_content.chunks[document_content.chunks.len() - 1];
            assert!(first_chunk.chunk_index < last_chunk.chunk_index || document_content.chunks.len() == 1);
        }
    }
}

/// Property test: Configuration serialization should be robust
proptest! {
    #[test]
    fn prop_config_serialization(
        max_concurrent in prop::option::of(1..1000usize),
        default_timeout in prop::option::of(100..300000u64),
        chunk_size in 1..100000usize,
        overlap_size in 0..10000usize,
        preserve_paragraphs in any::<bool>(),
    ) {
        let config = Config {
            max_concurrent_tasks: max_concurrent,
            default_timeout_ms: default_timeout,
            ..Default::default()
        };

        let chunking_config = ChunkingConfig {
            chunk_size,
            overlap_size: std::cmp::min(overlap_size, chunk_size / 2),
            preserve_paragraphs,
        };

        // Test YAML serialization for config
        match serde_yaml::to_string(&config) {
            Ok(yaml_str) => {
                match serde_yaml::from_str::<Config>(&yaml_str) {
                    Ok(restored) => {
                        assert_eq!(config.max_concurrent_tasks, restored.max_concurrent_tasks);
                        assert_eq!(config.default_timeout_ms, restored.default_timeout_ms);
                    },
                    Err(e) => {
                        tracing::debug!("YAML config deserialization error: {}", e);
                    }
                }
            },
            Err(e) => {
                tracing::debug!("YAML config serialization error: {}", e);
            }
        }

        // Test manual serialization for chunking config (since it doesn't derive Serialize)
        let chunking_json = format!(
            r#"{{"chunk_size":{},"overlap_size":{},"preserve_paragraphs":{}}}"#,
            chunking_config.chunk_size,
            chunking_config.overlap_size,
            chunking_config.preserve_paragraphs
        );

        // Verify the JSON is well-formed
        match serde_json::from_str::<serde_json::Value>(&chunking_json) {
            Ok(parsed) => {
                assert!(parsed.is_object());
                assert!(parsed.get("chunk_size").is_some());
                assert!(parsed.get("overlap_size").is_some());
                assert!(parsed.get("preserve_paragraphs").is_some());
            },
            Err(e) => {
                tracing::debug!("JSON chunking config validation error: {}", e);
            }
        }
    }
}

// ============================================================================
// MEMORY BOUNDS AND OVERFLOW PROPERTY TESTS
// ============================================================================

/// Property test: Large file processing should handle memory constraints
proptest! {
    #[test]
    fn prop_memory_bounds_large_files(
        file_size_kb in 1..10000usize, // Up to 10MB files
        chunk_size in 1..10000usize,
    ) {
        let rt = Runtime::new().unwrap();
        rt.block_on(async {
            init_test_tracing();

            // Create large content
            let content = "X".repeat(file_size_kb * 1024);

            let chunking_config = ChunkingConfig {
                chunk_size,
                overlap_size: chunk_size / 10,
                preserve_paragraphs: false,
            };

            let processor = DocumentProcessor::with_chunking_config(chunking_config);

            // Create temporary file
            let temp_file = NamedTempFile::with_suffix(".txt").unwrap();
            tokio::fs::write(temp_file.path(), &content).await.unwrap();

            // Process with timeout to prevent hanging
            let process_future = processor.process_file(temp_file.path(), "test_collection");
            let timeout_duration = Duration::from_secs(30);

            match tokio::time::timeout(timeout_duration, process_future).await {
                Ok(Ok(result)) => {
                    // Processing succeeded
                    assert!(!result.document_id.is_empty());
                    assert!(result.processing_time_ms > 0);

                    // Verify reasonable chunk count
                    if let Some(chunks) = result.chunks_created {
                        let expected_chunks = (content.len() / chunk_size).max(1);
                        assert!(chunks <= expected_chunks * 2, "Too many chunks created: {} vs expected ~{}", chunks, expected_chunks);
                    }
                },
                Ok(Err(e)) => {
                    // Processing failed - should be graceful
                    let error_str = e.to_string();
                    assert!(!error_str.contains("panicked"));
                    tracing::debug!("Large file processing error: {}", error_str);
                },
                Err(_) => {
                    // Timeout - may be acceptable for very large files
                    tracing::debug!("Large file processing timed out for size: {} KB", file_size_kb);
                }
            }
        });
    }
}

/// Property test: Vector dimension overflow and underflow handling
proptest! {
    #[test]
    fn prop_embedding_dimension_bounds(
        dimensions in arb_embedding_dimensions(),
        _content in arb_file_content(),
    ) {
        let rt = Runtime::new().unwrap();
        rt.block_on(async {
            init_test_tracing();

            // Test with extreme embedding dimensions
            let config = EmbeddingConfig {
                model_cache_dir: std::env::temp_dir().join("test_models"),
                max_cache_size: 100,
                batch_size: 1,
                max_sequence_length: 512,
                enable_preprocessing: true,
                bm25_k1: 1.2,
                bm25_b: 0.75,
            };

            // This should either work or fail gracefully
            match EmbeddingGenerator::new(config) {
                Ok(_generator) => {
                    // If generator creation succeeds, configuration is valid
                    tracing::debug!("Embedding generator created successfully with dimensions: {}", dimensions);
                    // Skip actual embedding generation for property test to avoid async complexity
                },
                Err(e) => {
                    // Generator creation errors should be graceful
                    let error_str = e.to_string();
                    assert!(!error_str.contains("panicked"));
                    tracing::debug!("Embedding generator creation error: {}", error_str);
                }
            }
        });
    }
}

/// Property test: String length and buffer overflow prevention
proptest! {
    #[test]
    fn prop_string_buffer_overflow(
        string_length in 0..1000000usize,
        pattern in prop_oneof!["A", "AB", "ABC", "ðŸš€", "æµ‹è¯•", "\n\t\r"],
    ) {
        // Create very long strings that might cause buffer issues
        let content = pattern.repeat(string_length / pattern.len() + 1);

        let processor = DocumentProcessor::new();

        // Test document type detection with very long filenames
        let long_filename = format!("{}.txt", "a".repeat(string_length.min(255)));
        let path = Path::new(&long_filename);

        match processor.detect_document_type(path) {
            Ok(doc_type) => {
                // Should detect as text or unknown
                assert!(matches!(doc_type, DocumentType::Text | DocumentType::Unknown));
            },
            Err(e) => {
                // Errors should be well-formed
                let error_str = e.to_string();
                assert!(!error_str.contains("panicked"));
                assert!(!error_str.is_empty());
            }
        }

        // Test processing with very long content using actual processor methods
        let temp_file = NamedTempFile::with_suffix(".txt").unwrap();
        if std::fs::write(temp_file.path(), &content).is_ok() {
            let rt = Runtime::new().unwrap();
            rt.block_on(async {
                match processor.process_file(temp_file.path(), "test_collection").await {
                    Ok(result) => {
                        // Verify processing completed without issues
                        assert!(!result.document_id.is_empty());
                        assert!(result.processing_time_ms >= 0);
                        if let Some(chunks) = result.chunks_created {
                            assert!(chunks <= content.len() / 10 + 100, "Too many chunks for content size");
                        }
                    },
                    Err(e) => {
                        // Processing errors should be graceful
                        tracing::debug!("Processing error for long content: {}", e);
                    }
                }
            });
        }
    }
}

// ============================================================================
// ERROR HANDLING PROPERTY TESTS
// ============================================================================

/// Property test: Invalid file formats should be handled gracefully
proptest! {
    #[test]
    fn prop_invalid_file_format_handling(
        corrupted_content in arb_malformed_content(),
        extension in arb_file_extension(),
    ) {
        let rt = Runtime::new().unwrap();
        rt.block_on(async {
            init_test_tracing();

            let processor = DocumentProcessor::new();

            // Create file with corrupted content
            let temp_file = NamedTempFile::with_suffix(&format!(".{}", extension)).unwrap();
            tokio::fs::write(temp_file.path(), &corrupted_content).await.unwrap();

            // Processing should handle corruption gracefully
            match processor.process_file(temp_file.path(), "test_collection").await {
                Ok(result) => {
                    // If processing succeeds, result should be valid
                    assert!(!result.document_id.is_empty());
                    assert_eq!(result.collection, "test_collection");

                    // Processing time should be reasonable
                    assert!(result.processing_time_ms < 60000, "Processing took too long");
                },
                Err(e) => {
                    // Errors should be specific and actionable
                    let error_str = e.to_string();
                    assert!(!error_str.is_empty());
                    assert!(!error_str.contains("panicked"));
                    assert!(!error_str.contains("unwrap"));

                    // Error should indicate the type of problem
                    assert!(
                        error_str.contains("IO error") ||
                        error_str.contains("Parsing error") ||
                        error_str.contains("Processing error") ||
                        error_str.contains("encoding") ||
                        error_str.contains("format"),
                        "Error message should be descriptive: {}", error_str
                    );
                }
            }
        });
    }
}

/// Property test: Network timeout and connection failures
proptest! {
    #[test]
    fn prop_network_error_handling(
        _error_scenario in any::<ErrorScenario>(),
        timeout_ms in 100..10000u64,
    ) {
        let rt = Runtime::new().unwrap();
        rt.block_on(async {
            init_test_tracing();

            // Test timeout behavior with processing engine
            let config = Config {
                max_concurrent_tasks: Some(1),
                default_timeout_ms: Some(timeout_ms),
                ..Default::default()
            };

            let mut engine = ProcessingEngine::with_config(config);

            // Start engine with timeout
            let start_future = engine.start();
            let timeout_duration = Duration::from_millis(timeout_ms * 2);

            match tokio::time::timeout(timeout_duration, start_future).await {
                Ok(Ok(())) => {
                    // Engine started successfully
                    tracing::debug!("Engine started within timeout");

                    // Test graceful shutdown
                    match tokio::time::timeout(timeout_duration, engine.shutdown()).await {
                        Ok(Ok(())) => {
                            tracing::debug!("Engine shutdown gracefully");
                        },
                        Ok(Err(e)) => {
                            let error_str = e.to_string();
                            assert!(!error_str.contains("panicked"));
                        },
                        Err(_) => {
                            tracing::debug!("Engine shutdown timed out");
                        }
                    }
                },
                Ok(Err(e)) => {
                    // Engine start failed - should be graceful
                    let error_str = e.to_string();
                    assert!(!error_str.contains("panicked"));
                    tracing::debug!("Engine start error: {}", error_str);
                },
                Err(_) => {
                    // Timeout occurred - may be acceptable
                    tracing::debug!("Engine start timed out after {}ms", timeout_ms);
                }
            }
        });
    }
}

/// Property test: Resource exhaustion scenarios
proptest! {
    #[test]
    fn prop_resource_exhaustion_handling(
        concurrent_tasks in 1..100usize,
        task_count in 1..1000usize,
    ) {
        let rt = Runtime::new().unwrap();
        rt.block_on(async {
            init_test_tracing();

            // Create engine with limited concurrency
            let config = Config {
                max_concurrent_tasks: Some(concurrent_tasks),
                default_timeout_ms: Some(1000),
                ..Default::default()
            };

            let mut engine = ProcessingEngine::with_config(config);

            // Try to start the engine
            if let Ok(()) = engine.start().await {
                // Test resource limits with simplified approach
                let mut task_results = Vec::new();

                // Submit limited number of tasks for resource testing
                for i in 0..task_count.min(5) { // Very limited for testing
                    let temp_file = NamedTempFile::with_suffix(".txt").unwrap();
                    tokio::fs::write(temp_file.path(), format!("Test content {}", i)).await.unwrap();

                    // Process immediately to avoid borrowing issues
                    let result = tokio::time::timeout(
                        Duration::from_millis(3000),
                        engine.process_document(
                            temp_file.path(),
                            "test_collection",
                            TaskPriority::CliCommands
                        )
                    ).await;

                    task_results.push(result);
                }

                // Analyze task results
                let mut completed = 0;
                let mut errors = 0;
                let mut timeouts = 0;

                for result in task_results {
                    match result {
                        Ok(Ok(_)) => completed += 1,
                        Ok(Err(e)) => {
                            errors += 1;
                            let error_str = e.to_string();
                            assert!(!error_str.contains("panicked"));
                        },
                        Err(_) => {
                            timeouts += 1;
                            // Timeout - may be acceptable under load
                        }
                    }
                }

                tracing::debug!("Resource exhaustion test: {} completed, {} errors", completed, errors);

                // At least some tasks should complete or fail gracefully
                assert!(completed + errors > 0, "No tasks completed or failed gracefully");

                // Shutdown
                let _ = engine.shutdown().await;
            }
        });
    }
}

// ============================================================================
// INTEGRATION PROPERTY TESTS
// ============================================================================

/// Property test: End-to-end processing pipeline integrity
proptest! {
    #[test]
    fn prop_pipeline_integrity(
        documents in prop::collection::vec(
            (arb_file_content(), arb_file_extension()),
            1..10
        ),
        chunking_config in arb_chunking_config(),
    ) {
        let rt = Runtime::new().unwrap();
        rt.block_on(async {
            init_test_tracing();

            let processor = DocumentProcessor::with_chunking_config(chunking_config);
            let temp_dir = TempDir::new().unwrap();

            let mut successful_processes = 0;
            let mut total_chunks = 0;

            // Process multiple documents
            for (i, (content, extension)) in documents.iter().enumerate() {
                let file_path = temp_dir.path().join(format!("test_{}.{}", i, extension));

                if let Ok(()) = tokio::fs::write(&file_path, content).await {
                    match processor.process_file(&file_path, "test_collection").await {
                        Ok(result) => {
                            successful_processes += 1;
                            if let Some(chunks) = result.chunks_created {
                                total_chunks += chunks;
                            }

                            // Verify result integrity
                            assert!(!result.document_id.is_empty());
                            assert_eq!(result.collection, "test_collection");
                            assert!(result.processing_time_ms >= 0);
                        },
                        Err(e) => {
                            // Log errors but don't fail test - some inputs may legitimately fail
                            tracing::debug!("Document processing error: {}", e);
                        }
                    }
                }
            }

            // At least some documents should process successfully if content is reasonable
            if documents.iter().any(|(content, _)| !content.trim().is_empty()) {
                // Don't require success for all documents due to potential edge cases
                tracing::debug!(
                    "Pipeline integrity test: {}/{} documents processed successfully, {} total chunks",
                    successful_processes, documents.len(), total_chunks
                );
            }
        });
    }
}

/// Property test: Concurrent processing should be safe and consistent
proptest! {
    #[test]
    fn prop_concurrent_processing_safety(
        content in arb_file_content(),
        concurrent_count in 1..10usize,
    ) {
        let rt = Runtime::new().unwrap();
        rt.block_on(async {
            init_test_tracing();

            let processor = Arc::new(DocumentProcessor::new());
            let temp_dir = TempDir::new().unwrap();

            // Create multiple identical files
            let mut file_paths = Vec::new();
            for i in 0..concurrent_count {
                let file_path = temp_dir.path().join(format!("concurrent_test_{}.txt", i));
                if tokio::fs::write(&file_path, &content).await.is_ok() {
                    file_paths.push(file_path);
                }
            }

            // Process all files concurrently
            let mut handles = Vec::new();
            for file_path in file_paths {
                let processor_clone = processor.clone();
                let handle = tokio::spawn(async move {
                    processor_clone.process_file(&file_path, "concurrent_test").await
                });
                handles.push(handle);
            }

            // Wait for all tasks to complete
            let mut results = Vec::new();
            for handle in handles {
                if let Ok(result) = handle.await {
                    results.push(result);
                }
            }

            // Analyze results for consistency
            let successful_results: Vec<_> = results.into_iter()
                .filter_map(|r| r.ok())
                .collect();

            if successful_results.len() > 1 {
                // Verify consistency across concurrent processing
                let first_result = &successful_results[0];

                for result in &successful_results[1..] {
                    // All should have same collection
                    assert_eq!(result.collection, first_result.collection);

                    // Chunk counts should be identical for identical content
                    if let (Some(chunks1), Some(chunks2)) = (first_result.chunks_created, result.chunks_created) {
                        assert_eq!(chunks1, chunks2, "Concurrent processing should produce consistent chunk counts");
                    }

                    // Processing times should be reasonable
                    assert!(result.processing_time_ms > 0);
                    assert!(result.processing_time_ms < 60000); // Less than 1 minute
                }
            }

            tracing::debug!("Concurrent processing test: {}/{} succeeded",
                successful_results.len(), concurrent_count);
        });
    }
}