//! Rust daemon with service discovery integration
//!
//! This binary demonstrates how to integrate the service discovery system
//! with the Rust daemon, enabling automatic communication with the Python
//! MCP server.

use std::time::Duration;
use tokio::signal;
use tracing::{error, info, warn};

use workspace_qdrant_core::{
    service_discovery::{
        DiscoveryManager, ServiceInfo, ServiceStatus, DiscoveryConfig
    },
    config::DaemonConfig,
    logging::initialize_logging,
};

// Service name constants
const RUST_DAEMON: &str = "rust-daemon";
const PYTHON_MCP: &str = "python-mcp";

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logging
    initialize_logging().await?;
    
    info!("Starting Rust daemon with service discovery");
    
    // Load daemon configuration
    let daemon_config = DaemonConfig::default();
    
    // Configure service discovery
    let discovery_config = DiscoveryConfig {
        registry_path: None, // Use default path
        multicast_address: "239.255.42.42".to_string(),
        multicast_port: 9999,
        discovery_timeout: Duration::from_secs(10),
        health_check_interval: Duration::from_secs(30),
        cleanup_interval: Duration::from_secs(60),
        enable_network_discovery: true,
        enable_authentication: true,
    };
    
    // Initialize service discovery manager
    let discovery_manager = DiscoveryManager::new(discovery_config).await?;
    
    // Start discovery system
    discovery_manager.start().await?;
    
    // Subscribe to discovery events
    let mut event_receiver = discovery_manager.subscribe_events();
    
    // Handle discovery events in background
    tokio::spawn(async move {
        while let Ok(event) = event_receiver.recv().await {
            match &event {
                workspace_qdrant_core::service_discovery::ServiceDiscoveryEvent::ServiceDiscovered { 
                    service_name, service_info, strategy 
                } => {
                    info!("Discovered service {} via {:?} at {}:{}", 
                          service_name, strategy, service_info.host, service_info.port);
                }
                workspace_qdrant_core::service_discovery::ServiceDiscoveryEvent::ServiceLost { 
                    service_name, reason 
                } => {
                    warn!("Lost service {}: {}", service_name, reason);
                }
                workspace_qdrant_core::service_discovery::ServiceDiscoveryEvent::HealthChanged { 
                    service_name, old_status, new_status 
                } => {
                    info!("Health changed for {}: {:?} -> {:?}", 
                          service_name, old_status, new_status);
                }
                _ => {}
            }
        }
    });
    
    // Create and register our service info
    let our_service = ServiceInfo::new("127.0.0.1".to_string(), daemon_config.server_port)
        .with_auth_token(ServiceInfo::generate_auth_token())
        .with_health_endpoint("/health".to_string())
        .with_additional_port("grpc".to_string(), daemon_config.grpc_port)
        .with_metadata("version".to_string(), "0.2.1".to_string())
        .with_metadata("component".to_string(), "rust-daemon".to_string());
    
    // Register ourselves in the discovery system
    discovery_manager.register_service(RUST_DAEMON, our_service).await?;
    info!("Rust daemon registered for service discovery");
    
    // Try to discover Python MCP server
    match discovery_manager.discover_service(PYTHON_MCP).await? {
        Some(python_service) => {
            info!("Found Python MCP server at {}:{}", python_service.host, python_service.port);
            
            // TODO: Establish communication with Python MCP server
            // This would involve setting up IPC channels, gRPC connections, etc.
        }
        None => {
            warn!("Python MCP server not found. Will continue checking...");
        }
    }
    
    // Note: In a real implementation, you would use Arc<DiscoveryManager> for sharing
    // across tasks. For this demo, we'll skip the periodic health checking task.
    
    // Wait for shutdown signal
    info!("Rust daemon running. Press Ctrl+C to shutdown.");
    
    match signal::ctrl_c().await {
        Ok(()) => {
            info!("Received shutdown signal");
        }
        Err(err) => {
            error!("Unable to listen for shutdown signal: {}", err);
        }
    }
    
    // Graceful shutdown
    info!("Shutting down Rust daemon");
    
    // Deregister from discovery system
    discovery_manager.deregister_service(RUST_DAEMON).await?;
    
    // Stop discovery manager
    discovery_manager.stop().await?;
    
    info!("Rust daemon shutdown complete");
    Ok(())
}

/// Example of how to implement health check endpoint
#[allow(dead_code)]
async fn setup_health_endpoint() -> Result<(), Box<dyn std::error::Error>> {
    use tokio::net::TcpListener;
    use tokio::io::{AsyncReadExt, AsyncWriteExt};
    
    let listener = TcpListener::bind("127.0.0.1:8080").await?;
    info!("Health check endpoint listening on http://127.0.0.1:8080/health");
    
    while let Ok((mut stream, _)) = listener.accept().await {
        tokio::spawn(async move {
            let mut buffer = [0; 1024];
            
            if let Ok(n) = stream.read(&mut buffer).await {
                let request = String::from_utf8_lossy(&buffer[..n]);
                
                if request.contains("GET /health") {
                    let response = "HTTP/1.1 200 OK\r\nContent-Type: application/json\r\nContent-Length: 27\r\n\r\n{\"status\":\"healthy\",\"ok\":true}";
                    let _ = stream.write_all(response.as_bytes()).await;
                }
            }
        });
    }
    
    Ok(())
}