#!/usr/bin/env python3
"""
Task #149: Resilience and Recovery Testing Suite
Comprehensive testing system for daemon restart behavior, data consistency, 
network failure recovery, and production deployment reliability validation.
"""

import json
import requests
import psutil
import subprocess
import signal
import time
import os
import random
import threading
import socket
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import hashlib

@dataclass
class ResilienceTestResult:
    """Structured result for resilience test scenarios"""
    test_name: str
    success: bool
    duration_seconds: float
    recovery_time_seconds: Optional[float]
    data_integrity_verified: bool
    error_message: Optional[str]
    metrics: Dict[str, Any]

class SystemSafetyMonitor:
    """Enhanced safety monitoring with emergency recovery capabilities"""
    
    def __init__(self, emergency_threshold_cpu=95, emergency_threshold_memory=95):
        self.emergency_threshold_cpu = emergency_threshold_cpu
        self.emergency_threshold_memory = emergency_threshold_memory
        self.monitoring_active = False
        self.emergency_triggered = False
        
    def check_system_safety(self) -> Tuple[bool, Dict[str, float]]:
        """Check if system is within safe operational parameters"""
        try:
            memory = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=0.1)
            
            metrics = {
                'memory_percent': memory.percent,
                'cpu_percent': cpu_percent,
                'memory_available_gb': memory.available / 1024**3,
                'load_average': os.getloadavg()[0] if hasattr(os, 'getloadavg') else 0
            }
            
            safe = (memory.percent < self.emergency_threshold_memory and 
                   cpu_percent < self.emergency_threshold_cpu)
            
            if not safe and not self.emergency_triggered:
                self.emergency_triggered = True
                print(f"üö® SYSTEM SAFETY EMERGENCY: Memory {memory.percent:.1f}%, CPU {cpu_percent:.1f}%")
                
            return safe, metrics
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Safety check error: {e}")
            return False, {}

class ProcessManager:
    """Manages workspace-qdrant-mcp daemon processes for resilience testing"""
    
    def __init__(self):
        self.daemon_pids = []
        self.original_processes = self._discover_daemon_processes()
        
    def _discover_daemon_processes(self) -> List[psutil.Process]:
        """Discover running workspace-qdrant-mcp processes"""
        processes = []
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                cmdline = proc.info['cmdline'] or []
                cmdline_str = ' '.join(cmdline).lower()
                
                # Look for workspace-qdrant-mcp related processes
                if any(keyword in cmdline_str for keyword in [
                    'workspace-qdrant-mcp',
                    'qdrant-mcp',
                    'workspace_qdrant',
                    'mcp-server'
                ]):
                    processes.append(proc)
                    print(f"üìã Discovered process: PID {proc.pid}, CMD: {' '.join(cmdline[:3])}")
                    
            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):
                continue
                
        return processes
        
    def graceful_shutdown_daemon(self, timeout=30) -> bool:
        """Attempt graceful shutdown of daemon processes"""
        if not self.original_processes:
            print("‚ö†Ô∏è  No daemon processes found to shutdown")
            return True
            
        print(f"üîÑ Attempting graceful shutdown of {len(self.original_processes)} processes")
        
        for proc in self.original_processes:
            try:
                if proc.is_running():
                    proc.terminate()
                    print(f"üì§ Sent SIGTERM to PID {proc.pid}")
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
                
        # Wait for graceful shutdown
        start_time = time.time()
        while time.time() - start_time < timeout:
            all_stopped = True
            for proc in self.original_processes:
                try:
                    if proc.is_running():
                        all_stopped = False
                        break
                except psutil.NoSuchProcess:
                    continue
                    
            if all_stopped:
                print(f"‚úÖ Graceful shutdown completed in {time.time() - start_time:.1f}s")
                return True
                
            time.sleep(0.5)
            
        print(f"‚ö†Ô∏è  Graceful shutdown timeout after {timeout}s")
        return False
        
    def force_kill_daemon(self) -> bool:
        """Force kill daemon processes"""
        if not self.original_processes:
            print("‚ö†Ô∏è  No daemon processes found to kill")
            return True
            
        print(f"üí• Force killing {len(self.original_processes)} processes")
        
        for proc in self.original_processes:
            try:
                if proc.is_running():
                    proc.kill()
                    print(f"üî´ Sent SIGKILL to PID {proc.pid}")
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
                
        time.sleep(2)  # Allow time for cleanup
        
        # Verify all processes are terminated
        for proc in self.original_processes:
            try:
                if proc.is_running():
                    print(f"‚ùå Process PID {proc.pid} still running after SIGKILL")
                    return False
            except psutil.NoSuchProcess:
                continue
                
        print("‚úÖ All daemon processes terminated")
        return True
        
    def restart_daemon(self, startup_command: Optional[str] = None) -> bool:
        """Restart daemon processes (placeholder for actual startup logic)"""
        print("üîÑ Daemon restart would be initiated here")
        print("   In production: execute startup command or systemd service restart")
        
        if startup_command:
            print(f"   Command: {startup_command}")
            # In actual implementation:
            # subprocess.Popen(startup_command.split())
            
        # Simulate restart time
        time.sleep(5)
        print("‚úÖ Daemon restart simulation completed")
        return True

class DataConsistencyValidator:
    """Validates data consistency and integrity after recovery scenarios"""
    
    def __init__(self, qdrant_url="http://localhost:6333"):
        self.qdrant_url = qdrant_url
        self.test_collections = {}
        
    def create_test_data(self, collection_name: str, document_count: int = 100) -> Dict[str, str]:
        """Create test data with checksums for integrity validation"""
        print(f"üìã Creating test data: {document_count} documents in {collection_name}")
        
        # Create collection
        config = {
            "vectors": {"size": 384, "distance": "Cosine"},
            "optimizers_config": {"default_segment_number": 2},
            "replication_factor": 1
        }
        
        try:
            response = requests.put(f"{self.qdrant_url}/collections/{collection_name}", json=config)
            if response.status_code not in [200, 201]:
                raise Exception(f"Failed to create collection: {response.status_code}")
                
        except Exception as e:
            raise Exception(f"Collection creation error: {e}")
            
        # Generate test documents with checksums
        documents = []
        checksums = {}
        
        for i in range(document_count):
            content = f"resilience_test_document_{i}_{random.randint(1000, 9999)}"
            vector = [random.random() for _ in range(384)]
            
            # Calculate content checksum
            content_hash = hashlib.sha256(content.encode()).hexdigest()
            checksums[str(i)] = content_hash
            
            documents.append({
                "id": i,
                "vector": vector,
                "payload": {
                    "content": content,
                    "checksum": content_hash,
                    "created_timestamp": datetime.now().isoformat(),
                    "test_marker": "resilience_validation"
                }
            })
            
        # Insert documents
        try:
            response = requests.put(
                f"{self.qdrant_url}/collections/{collection_name}/points",
                json={"points": documents}
            )
            
            if response.status_code not in [200, 201]:
                raise Exception(f"Failed to insert documents: {response.status_code}")
                
        except Exception as e:
            raise Exception(f"Document insertion error: {e}")
            
        # Store checksums for validation
        self.test_collections[collection_name] = checksums
        
        print(f"‚úÖ Created {document_count} test documents with integrity checksums")
        return checksums
        
    def validate_data_integrity(self, collection_name: str) -> Tuple[bool, Dict[str, Any]]:
        """Validate data integrity after recovery scenarios"""
        if collection_name not in self.test_collections:
            return False, {"error": "No test data found for collection"}
            
        original_checksums = self.test_collections[collection_name]
        validation_results = {
            "total_expected": len(original_checksums),
            "total_found": 0,
            "checksum_matches": 0,
            "checksum_mismatches": 0,
            "missing_documents": 0,
            "corrupted_documents": []
        }
        
        print(f"üîç Validating data integrity for {collection_name}")
        print(f"   Expected documents: {len(original_checksums)}")
        
        try:
            # Retrieve all documents
            response = requests.post(
                f"{self.qdrant_url}/collections/{collection_name}/points/scroll",
                json={"limit": len(original_checksums), "with_payload": True}
            )
            
            if response.status_code != 200:
                return False, {"error": f"Failed to retrieve documents: {response.status_code}"}
                
            result = response.json()
            points = result.get("result", {}).get("points", [])
            validation_results["total_found"] = len(points)
            
            # Validate each document
            found_ids = set()
            for point in points:
                doc_id = str(point["id"])
                found_ids.add(doc_id)
                
                payload = point.get("payload", {})
                stored_checksum = payload.get("checksum")
                content = payload.get("content")
                
                if doc_id in original_checksums:
                    expected_checksum = original_checksums[doc_id]
                    
                    if content:
                        # Recalculate checksum
                        actual_checksum = hashlib.sha256(content.encode()).hexdigest()
                        
                        if actual_checksum == expected_checksum == stored_checksum:
                            validation_results["checksum_matches"] += 1
                        else:
                            validation_results["checksum_mismatches"] += 1
                            validation_results["corrupted_documents"].append({
                                "id": doc_id,
                                "expected": expected_checksum,
                                "stored": stored_checksum,
                                "calculated": actual_checksum
                            })
                    else:
                        validation_results["corrupted_documents"].append({
                            "id": doc_id,
                            "error": "missing_content"
                        })
                        
            # Check for missing documents
            missing_ids = set(original_checksums.keys()) - found_ids
            validation_results["missing_documents"] = len(missing_ids)
            
            # Determine overall integrity status
            integrity_valid = (
                validation_results["checksum_matches"] == validation_results["total_expected"] and
                validation_results["checksum_mismatches"] == 0 and
                validation_results["missing_documents"] == 0
            )
            
            print(f"   Found documents: {validation_results['total_found']}")
            print(f"   Checksum matches: {validation_results['checksum_matches']}")
            print(f"   Checksum mismatches: {validation_results['checksum_mismatches']}")
            print(f"   Missing documents: {validation_results['missing_documents']}")
            
            if integrity_valid:
                print("‚úÖ Data integrity validation PASSED")
            else:
                print("‚ùå Data integrity validation FAILED")
                
            return integrity_valid, validation_results
            
        except Exception as e:
            return False, {"error": f"Validation error: {e}"}
            
    def cleanup_test_data(self, collection_name: str) -> bool:
        """Clean up test collections"""
        try:
            response = requests.delete(f"{self.qdrant_url}/collections/{collection_name}")
            if collection_name in self.test_collections:
                del self.test_collections[collection_name]
            return response.status_code in [200, 204]
        except Exception:
            return False

class NetworkFailureSimulator:
    """Simulates network and connection failures for resilience testing"""
    
    def __init__(self, qdrant_url="http://localhost:6333"):
        self.qdrant_url = qdrant_url
        self.blocked_ports = []
        
    def test_qdrant_connectivity(self) -> bool:
        """Test basic Qdrant connectivity"""
        try:
            response = requests.get(f"{self.qdrant_url}/cluster", timeout=5)
            return response.status_code == 200
        except Exception:
            return False
            
    def simulate_connection_timeout(self, duration_seconds: int) -> bool:
        """Simulate connection timeout scenarios"""
        print(f"üåê Simulating connection timeout for {duration_seconds}s")
        
        # In production, this would use iptables or similar to block connections
        # For testing, we'll simulate by making requests with very short timeouts
        
        start_time = time.time()
        timeout_errors = 0
        
        while time.time() - start_time < duration_seconds:
            try:
                response = requests.get(f"{self.qdrant_url}/cluster", timeout=0.1)
            except requests.RequestException:
                timeout_errors += 1
            time.sleep(0.5)
            
        print(f"‚úÖ Connection timeout simulation completed ({timeout_errors} timeouts)")
        return True
        
    def test_connection_recovery(self, recovery_attempts: int = 10) -> Tuple[bool, float]:
        """Test automatic connection recovery capabilities"""
        print(f"üîÑ Testing connection recovery with {recovery_attempts} attempts")
        
        start_time = time.time()
        
        for attempt in range(recovery_attempts):
            try:
                response = requests.get(f"{self.qdrant_url}/cluster", timeout=5)
                if response.status_code == 200:
                    recovery_time = time.time() - start_time
                    print(f"‚úÖ Connection recovered in {recovery_time:.1f}s (attempt {attempt + 1})")
                    return True, recovery_time
                    
            except Exception as e:
                print(f"   Attempt {attempt + 1}/{recovery_attempts}: {e}")
                
            time.sleep(2)  # Wait between recovery attempts
            
        print(f"‚ùå Connection recovery failed after {recovery_attempts} attempts")
        return False, time.time() - start_time

class ResilienceTestingSuite:
    """Main resilience and recovery testing suite for Task #149"""
    
    def __init__(self, qdrant_url="http://localhost:6333"):
        self.qdrant_url = qdrant_url
        self.safety_monitor = SystemSafetyMonitor()
        self.process_manager = ProcessManager()
        self.data_validator = DataConsistencyValidator(qdrant_url)
        self.network_simulator = NetworkFailureSimulator(qdrant_url)
        self.test_results = []
        self.test_session_id = f"resilience_test_{int(time.time())}"
        
        # Create test sandbox
        self.test_collection = f"resilience_test_{int(time.time())}"
        
    def run_daemon_restart_tests(self) -> List[ResilienceTestResult]:
        """Execute comprehensive daemon restart and recovery tests"""
        print("\n" + "="*60)
        print("üîÑ DAEMON RESTART AND RECOVERY TESTING")
        print("="*60)
        
        restart_tests = []
        
        # Test 1: Graceful Shutdown and Restart
        print("\nüìã TEST 1: Graceful Shutdown and Restart")
        test_start = time.time()
        
        try:
            # Create test data first
            checksums = self.data_validator.create_test_data(f"{self.test_collection}_graceful", 50)
            
            # Record baseline connectivity
            baseline_connected = self.network_simulator.test_qdrant_connectivity()
            print(f"   Baseline connectivity: {'‚úÖ' if baseline_connected else '‚ùå'}")
            
            # Perform graceful shutdown
            shutdown_start = time.time()
            graceful_success = self.process_manager.graceful_shutdown_daemon()
            shutdown_time = time.time() - shutdown_start
            
            if graceful_success:
                print(f"‚úÖ Graceful shutdown completed in {shutdown_time:.1f}s")
                
                # Wait brief period to simulate downtime
                time.sleep(3)
                
                # Restart daemon
                restart_start = time.time()
                restart_success = self.process_manager.restart_daemon()
                restart_time = time.time() - restart_start
                
                if restart_success:
                    # Test connectivity recovery
                    connectivity_recovered, recovery_time = self.network_simulator.test_connection_recovery()
                    
                    # Validate data integrity
                    integrity_valid, integrity_results = self.data_validator.validate_data_integrity(
                        f"{self.test_collection}_graceful"
                    )
                    
                    total_test_time = time.time() - test_start
                    
                    restart_tests.append(ResilienceTestResult(
                        test_name="graceful_shutdown_restart",
                        success=restart_success and connectivity_recovered and integrity_valid,
                        duration_seconds=total_test_time,
                        recovery_time_seconds=recovery_time,
                        data_integrity_verified=integrity_valid,
                        error_message=None,
                        metrics={
                            "shutdown_time_seconds": shutdown_time,
                            "restart_time_seconds": restart_time,
                            "connectivity_recovery_time": recovery_time,
                            "data_integrity": integrity_results
                        }
                    ))
                    
                else:
                    restart_tests.append(ResilienceTestResult(
                        test_name="graceful_shutdown_restart",
                        success=False,
                        duration_seconds=time.time() - test_start,
                        recovery_time_seconds=None,
                        data_integrity_verified=False,
                        error_message="Daemon restart failed",
                        metrics={"shutdown_time_seconds": shutdown_time}
                    ))
                    
            else:
                restart_tests.append(ResilienceTestResult(
                    test_name="graceful_shutdown_restart",
                    success=False,
                    duration_seconds=time.time() - test_start,
                    recovery_time_seconds=None,
                    data_integrity_verified=False,
                    error_message="Graceful shutdown failed",
                    metrics={}
                ))
                
        except Exception as e:
            restart_tests.append(ResilienceTestResult(
                test_name="graceful_shutdown_restart",
                success=False,
                duration_seconds=time.time() - test_start,
                recovery_time_seconds=None,
                data_integrity_verified=False,
                error_message=str(e),
                metrics={}
            ))
            
        # Test 2: Force Kill and Recovery
        print("\nüìã TEST 2: Force Kill and Recovery")
        test_start = time.time()
        
        try:
            # Create test data
            checksums = self.data_validator.create_test_data(f"{self.test_collection}_force_kill", 50)
            
            # Force kill daemon
            kill_start = time.time()
            kill_success = self.process_manager.force_kill_daemon()
            kill_time = time.time() - kill_start
            
            if kill_success:
                print(f"‚úÖ Force kill completed in {kill_time:.1f}s")
                
                # Simulate crash recovery time
                time.sleep(5)
                
                # Restart daemon
                restart_start = time.time()
                restart_success = self.process_manager.restart_daemon()
                restart_time = time.time() - restart_start
                
                # Test recovery capabilities
                connectivity_recovered, recovery_time = self.network_simulator.test_connection_recovery()
                integrity_valid, integrity_results = self.data_validator.validate_data_integrity(
                    f"{self.test_collection}_force_kill"
                )
                
                total_test_time = time.time() - test_start
                
                restart_tests.append(ResilienceTestResult(
                    test_name="force_kill_recovery",
                    success=restart_success and connectivity_recovered and integrity_valid,
                    duration_seconds=total_test_time,
                    recovery_time_seconds=recovery_time,
                    data_integrity_verified=integrity_valid,
                    error_message=None,
                    metrics={
                        "kill_time_seconds": kill_time,
                        "restart_time_seconds": restart_time,
                        "connectivity_recovery_time": recovery_time,
                        "data_integrity": integrity_results
                    }
                ))
                
            else:
                restart_tests.append(ResilienceTestResult(
                    test_name="force_kill_recovery",
                    success=False,
                    duration_seconds=time.time() - test_start,
                    recovery_time_seconds=None,
                    data_integrity_verified=False,
                    error_message="Force kill failed",
                    metrics={}
                ))
                
        except Exception as e:
            restart_tests.append(ResilienceTestResult(
                test_name="force_kill_recovery",
                success=False,
                duration_seconds=time.time() - test_start,
                recovery_time_seconds=None,
                data_integrity_verified=False,
                error_message=str(e),
                metrics={}
            ))
            
        return restart_tests
        
    def run_resource_exhaustion_recovery_tests(self) -> List[ResilienceTestResult]:
        """Test recovery from resource exhaustion scenarios"""
        print("\n" + "="*60)
        print("üíæ RESOURCE EXHAUSTION RECOVERY TESTING")
        print("="*60)
        
        resource_tests = []
        
        # Test 1: Memory Pressure Recovery
        print("\nüìã TEST 1: Memory Pressure Recovery")
        test_start = time.time()
        
        try:
            # Get baseline metrics
            safe, baseline_metrics = self.safety_monitor.check_system_safety()
            print(f"   Baseline: Memory {baseline_metrics['memory_percent']:.1f}%, CPU {baseline_metrics['cpu_percent']:.1f}%")
            
            # Create test data
            checksums = self.data_validator.create_test_data(f"{self.test_collection}_memory_pressure", 100)
            
            # Simulate memory pressure by creating large documents
            print("   Applying memory pressure...")
            memory_pressure_docs = []
            
            for i in range(20):  # Create large documents
                large_content = "x" * (1024 * 1024)  # 1MB per document
                vector = [random.random() for _ in range(384)]
                
                memory_pressure_docs.append({
                    "id": f"pressure_{i}",
                    "vector": vector,
                    "payload": {
                        "content": large_content,
                        "size_mb": 1,
                        "pressure_test": True
                    }
                })
                
            # Insert pressure documents
            pressure_collection = f"{self.test_collection}_memory_pressure_load"
            config = {"vectors": {"size": 384, "distance": "Cosine"}}
            requests.put(f"{self.qdrant_url}/collections/{pressure_collection}", json=config)
            
            pressure_start = time.time()
            response = requests.put(
                f"{self.qdrant_url}/collections/{pressure_collection}/points",
                json={"points": memory_pressure_docs}
            )
            
            # Check system state under pressure
            pressure_safe, pressure_metrics = self.safety_monitor.check_system_safety()
            pressure_time = time.time() - pressure_start
            
            # Allow system to recover
            print("   Allowing system recovery...")
            time.sleep(10)
            
            # Validate recovery
            recovery_safe, recovery_metrics = self.safety_monitor.check_system_safety()
            integrity_valid, integrity_results = self.data_validator.validate_data_integrity(
                f"{self.test_collection}_memory_pressure"
            )
            
            # Cleanup pressure data
            requests.delete(f"{self.qdrant_url}/collections/{pressure_collection}")
            
            success = recovery_safe and integrity_valid
            total_test_time = time.time() - test_start
            
            resource_tests.append(ResilienceTestResult(
                test_name="memory_pressure_recovery",
                success=success,
                duration_seconds=total_test_time,
                recovery_time_seconds=10.0,  # Recovery monitoring period
                data_integrity_verified=integrity_valid,
                error_message=None if success else "Recovery or integrity validation failed",
                metrics={
                    "baseline_memory_percent": baseline_metrics.get('memory_percent', 0),
                    "pressure_memory_percent": pressure_metrics.get('memory_percent', 0),
                    "recovery_memory_percent": recovery_metrics.get('memory_percent', 0),
                    "pressure_duration_seconds": pressure_time,
                    "data_integrity": integrity_results,
                    "pressure_safe": pressure_safe,
                    "recovery_safe": recovery_safe
                }
            ))
            
            print(f"   {'‚úÖ' if success else '‚ùå'} Memory pressure recovery test completed")
            print(f"   Memory: {baseline_metrics.get('memory_percent', 0):.1f}% ‚Üí {pressure_metrics.get('memory_percent', 0):.1f}% ‚Üí {recovery_metrics.get('memory_percent', 0):.1f}%")
            
        except Exception as e:
            resource_tests.append(ResilienceTestResult(
                test_name="memory_pressure_recovery",
                success=False,
                duration_seconds=time.time() - test_start,
                recovery_time_seconds=None,
                data_integrity_verified=False,
                error_message=str(e),
                metrics={}
            ))
            
        return resource_tests
        
    def run_network_failure_recovery_tests(self) -> List[ResilienceTestResult]:
        """Test network failure and recovery scenarios"""
        print("\n" + "="*60)
        print("üåê NETWORK FAILURE RECOVERY TESTING")
        print("="*60)
        
        network_tests = []
        
        # Test 1: Connection Timeout Recovery
        print("\nüìã TEST 1: Connection Timeout Recovery")
        test_start = time.time()
        
        try:
            # Create test data
            checksums = self.data_validator.create_test_data(f"{self.test_collection}_network", 30)
            
            # Test baseline connectivity
            baseline_connected = self.network_simulator.test_qdrant_connectivity()
            print(f"   Baseline connectivity: {'‚úÖ' if baseline_connected else '‚ùå'}")
            
            # Simulate connection timeouts
            timeout_success = self.network_simulator.simulate_connection_timeout(10)
            
            # Test connection recovery
            recovery_success, recovery_time = self.network_simulator.test_connection_recovery()
            
            # Validate data integrity after network issues
            integrity_valid, integrity_results = self.data_validator.validate_data_integrity(
                f"{self.test_collection}_network"
            )
            
            success = baseline_connected and timeout_success and recovery_success and integrity_valid
            total_test_time = time.time() - test_start
            
            network_tests.append(ResilienceTestResult(
                test_name="connection_timeout_recovery",
                success=success,
                duration_seconds=total_test_time,
                recovery_time_seconds=recovery_time,
                data_integrity_verified=integrity_valid,
                error_message=None if success else "Network recovery or integrity validation failed",
                metrics={
                    "baseline_connected": baseline_connected,
                    "timeout_simulation_success": timeout_success,
                    "recovery_success": recovery_success,
                    "recovery_time_seconds": recovery_time,
                    "data_integrity": integrity_results
                }
            ))
            
            print(f"   {'‚úÖ' if success else '‚ùå'} Network failure recovery test completed")
            
        except Exception as e:
            network_tests.append(ResilienceTestResult(
                test_name="connection_timeout_recovery",
                success=False,
                duration_seconds=time.time() - test_start,
                recovery_time_seconds=None,
                data_integrity_verified=False,
                error_message=str(e),
                metrics={}
            ))
            
        return network_tests
        
    def run_data_consistency_tests(self) -> List[ResilienceTestResult]:
        """Comprehensive data consistency and integrity testing"""
        print("\n" + "="*60)
        print("üîç DATA CONSISTENCY AND INTEGRITY TESTING")
        print("="*60)
        
        consistency_tests = []
        
        # Test 1: Large Scale Data Integrity
        print("\nüìã TEST 1: Large Scale Data Integrity")
        test_start = time.time()
        
        try:
            # Create large dataset
            large_collection = f"{self.test_collection}_large_consistency"
            checksums = self.data_validator.create_test_data(large_collection, 500)
            
            # Simulate various operations that could affect consistency
            print("   Performing consistency stress operations...")
            
            # Multiple concurrent searches
            search_threads = []
            search_results = []
            
            def concurrent_search(thread_id):
                try:
                    for i in range(10):
                        vector = [random.random() for _ in range(384)]
                        response = requests.post(
                            f"{self.qdrant_url}/collections/{large_collection}/points/search",
                            json={"vector": vector, "limit": 5}
                        )
                        search_results.append((thread_id, response.status_code == 200))
                        time.sleep(0.1)
                except Exception as e:
                    search_results.append((thread_id, False))
                    
            # Launch concurrent searches
            for i in range(5):
                thread = threading.Thread(target=concurrent_search, args=(i,))
                search_threads.append(thread)
                thread.start()
                
            # Wait for searches to complete
            for thread in search_threads:
                thread.join()
                
            # Validate data integrity after concurrent operations
            integrity_valid, integrity_results = self.data_validator.validate_data_integrity(large_collection)
            
            # Analyze search results
            successful_searches = sum(1 for _, success in search_results if success)
            total_searches = len(search_results)
            search_success_rate = successful_searches / total_searches if total_searches > 0 else 0
            
            success = integrity_valid and search_success_rate > 0.95
            total_test_time = time.time() - test_start
            
            consistency_tests.append(ResilienceTestResult(
                test_name="large_scale_data_integrity",
                success=success,
                duration_seconds=total_test_time,
                recovery_time_seconds=None,
                data_integrity_verified=integrity_valid,
                error_message=None if success else f"Integrity failed or low search success rate: {search_success_rate:.1%}",
                metrics={
                    "document_count": 500,
                    "concurrent_searches": total_searches,
                    "search_success_rate": search_success_rate,
                    "data_integrity": integrity_results
                }
            ))
            
            print(f"   {'‚úÖ' if success else '‚ùå'} Large scale integrity test completed")
            print(f"   Search success rate: {search_success_rate:.1%}")
            print(f"   Data integrity: {'‚úÖ' if integrity_valid else '‚ùå'}")
            
        except Exception as e:
            consistency_tests.append(ResilienceTestResult(
                test_name="large_scale_data_integrity",
                success=False,
                duration_seconds=time.time() - test_start,
                recovery_time_seconds=None,
                data_integrity_verified=False,
                error_message=str(e),
                metrics={}
            ))
            
        return consistency_tests
        
    def run_comprehensive_resilience_test(self) -> Dict[str, Any]:
        """Execute complete resilience testing suite"""
        print("\n" + "üî•" * 20 + " TASK #149: RESILIENCE & RECOVERY TESTING SUITE " + "üî•" * 20)
        print(f"Test Session ID: {self.test_session_id}")
        print(f"Start Time: {datetime.now().isoformat()}")
        print(f"Qdrant URL: {self.qdrant_url}")
        
        # System safety check
        safe, baseline_metrics = self.safety_monitor.check_system_safety()
        if not safe:
            print("üö® SYSTEM NOT SAFE FOR RESILIENCE TESTING - ABORTING")
            return {"error": "System safety check failed", "baseline_metrics": baseline_metrics}
            
        print(f"‚úÖ System safety verified - proceeding with resilience testing")
        print(f"   Baseline: Memory {baseline_metrics['memory_percent']:.1f}%, CPU {baseline_metrics['cpu_percent']:.1f}%")
        
        test_start_time = time.time()
        all_results = []
        
        try:
            # Execute test suites
            print("\nüéØ Executing resilience test battery...")
            
            # 1. Daemon restart tests
            daemon_results = self.run_daemon_restart_tests()
            all_results.extend(daemon_results)
            
            # 2. Resource exhaustion recovery tests
            resource_results = self.run_resource_exhaustion_recovery_tests()
            all_results.extend(resource_results)
            
            # 3. Network failure recovery tests
            network_results = self.run_network_failure_recovery_tests()
            all_results.extend(network_results)
            
            # 4. Data consistency tests
            consistency_results = self.run_data_consistency_tests()
            all_results.extend(consistency_results)
            
        except Exception as e:
            print(f"üö® CRITICAL ERROR during resilience testing: {e}")
            return {"error": str(e), "partial_results": all_results}
            
        finally:
            # Cleanup test data
            print("\nüßπ Cleaning up test data...")
            for collection_suffix in ["graceful", "force_kill", "memory_pressure", "network", "large_consistency"]:
                self.data_validator.cleanup_test_data(f"{self.test_collection}_{collection_suffix}")
                
        # Analyze results
        total_test_time = time.time() - test_start_time
        successful_tests = sum(1 for result in all_results if result.success)
        total_tests = len(all_results)
        overall_success_rate = successful_tests / total_tests if total_tests > 0 else 0
        
        # Calculate recovery time statistics
        recovery_times = [r.recovery_time_seconds for r in all_results if r.recovery_time_seconds is not None]
        avg_recovery_time = sum(recovery_times) / len(recovery_times) if recovery_times else None
        max_recovery_time = max(recovery_times) if recovery_times else None
        
        # Data integrity statistics
        integrity_verified_count = sum(1 for result in all_results if result.data_integrity_verified)
        integrity_success_rate = integrity_verified_count / total_tests if total_tests > 0 else 0
        
        # Final system safety check
        final_safe, final_metrics = self.safety_monitor.check_system_safety()
        
        # Compile comprehensive results
        comprehensive_results = {
            "test_session_id": self.test_session_id,
            "timestamp": datetime.now().isoformat(),
            "total_duration_seconds": total_test_time,
            
            "overall_results": {
                "total_tests_executed": total_tests,
                "successful_tests": successful_tests,
                "failed_tests": total_tests - successful_tests,
                "overall_success_rate": overall_success_rate,
                "integrity_success_rate": integrity_success_rate
            },
            
            "recovery_analysis": {
                "recovery_times_available": len(recovery_times),
                "average_recovery_time_seconds": avg_recovery_time,
                "maximum_recovery_time_seconds": max_recovery_time,
                "recovery_times": recovery_times
            },
            
            "system_health": {
                "baseline_metrics": baseline_metrics,
                "final_metrics": final_metrics,
                "final_system_safe": final_safe,
                "system_stability_maintained": final_safe
            },
            
            "detailed_test_results": [
                {
                    "test_name": result.test_name,
                    "success": result.success,
                    "duration_seconds": result.duration_seconds,
                    "recovery_time_seconds": result.recovery_time_seconds,
                    "data_integrity_verified": result.data_integrity_verified,
                    "error_message": result.error_message,
                    "metrics": result.metrics
                }
                for result in all_results
            ],
            
            "production_readiness_assessment": {
                "daemon_restart_capability": any(r.success for r in daemon_results),
                "resource_recovery_capability": any(r.success for r in resource_results), 
                "network_recovery_capability": any(r.success for r in network_results),
                "data_consistency_capability": any(r.success for r in consistency_results),
                "overall_resilience_score": overall_success_rate,
                "production_deployment_recommended": overall_success_rate >= 0.8 and final_safe
            }
        }
        
        return comprehensive_results

def main():
    """Main execution function for resilience testing suite"""
    print("üöÄ TASK #149: RESILIENCE AND RECOVERY TESTING SUITE")
    print("="*80)
    
    # Initialize testing suite
    suite = ResilienceTestingSuite()
    
    # Execute comprehensive testing
    results = suite.run_comprehensive_resilience_test()
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    results_file = f"resilience_recovery_results_{timestamp}.json"
    
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
        
    print(f"\nüìä RESILIENCE TESTING RESULTS SUMMARY")
    print("="*50)
    
    if "error" in results:
        print(f"‚ùå TESTING FAILED: {results['error']}")
        return
        
    overall = results["overall_results"]
    recovery = results["recovery_analysis"] 
    production = results["production_readiness_assessment"]
    
    print(f"‚úÖ Total Tests: {overall['total_tests_executed']}")
    print(f"‚úÖ Successful: {overall['successful_tests']}")
    print(f"‚ùå Failed: {overall['failed_tests']}")
    print(f"üìà Success Rate: {overall['overall_success_rate']:.1%}")
    print(f"üîí Data Integrity Rate: {overall['integrity_success_rate']:.1%}")
    
    if recovery["average_recovery_time_seconds"]:
        print(f"‚è±Ô∏è  Avg Recovery Time: {recovery['average_recovery_time_seconds']:.1f}s")
        print(f"‚è±Ô∏è  Max Recovery Time: {recovery['maximum_recovery_time_seconds']:.1f}s")
        
    print(f"\nüè≠ PRODUCTION READINESS ASSESSMENT")
    print(f"   Daemon Restart: {'‚úÖ' if production['daemon_restart_capability'] else '‚ùå'}")
    print(f"   Resource Recovery: {'‚úÖ' if production['resource_recovery_capability'] else '‚ùå'}")
    print(f"   Network Recovery: {'‚úÖ' if production['network_recovery_capability'] else '‚ùå'}")
    print(f"   Data Consistency: {'‚úÖ' if production['data_consistency_capability'] else '‚ùå'}")
    print(f"   Overall Score: {production['overall_resilience_score']:.1%}")
    print(f"   Production Ready: {'üöÄ YES' if production['production_deployment_recommended'] else '‚ö†Ô∏è  NEEDS WORK'}")
    
    print(f"\nüíæ Results saved to: {results_file}")
    print("\nüéØ RESILIENCE TESTING COMPLETED!")

if __name__ == "__main__":
    main()